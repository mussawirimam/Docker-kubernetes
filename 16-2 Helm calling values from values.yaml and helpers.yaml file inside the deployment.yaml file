Helm create mychat

### we are deleting these charts because:

root@master:~/app/helm# tree mychat/
mychat/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── serviceaccount.yaml
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml


### we are removing some of the files, because we will be creating our own chart.
4 directories, 10 files
root@master:~/app/helm# rm mychat/templates/_helpers.tpl
root@master:~/app/helm# rm mychat/templates/service.yaml
root@master:~/app/helm# rm mychat/templates/serviceaccount.yaml
root@master:~/app/helm# rm mychat/templates/hpa.yaml
root@master:~/app/helm# rm mychat/templates/tests/
rm: cannot remove 'mychat/templates/tests/': Is a directory
root@master:~/app/helm# ls 

### Copy the content of the deployment file and put it inside the deployment.yaml file within the mychart file
remove everything from the file and paste below inside it.

root@master:~/app/helm# vi mychat/templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
spec:
  replicas: 3
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80






### we are left with these files, we are doing this so that we can create our own chart 
root@master:~/app/helm# tree mychat/
mychat/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── ingress.yaml
│   └── NOTES.txt
└── values.yaml

3 directories, 5 files
root@master:~/app/helm#



### moved ingress to the /tmp since I was having issues at the deployment
root@master:~/app/helm/mychat/templates# mv ingress.yaml /tmp/

root@master:~/app/helm/mychat# tree
.
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   └── NOTES.txt
└── values.yaml

3 directories, 4 files

### we modified the NOTES.text to:
root@master:~/app/helm/mychat/templates# cat NOTES.txt
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm/mychat/templates#
### we want to keep the ingress, because we want to keep some of the things from the ingress.yaml file.


### if you want to do the quick deployment using helm and you dont have enough time to convert it into the template. 
# Create a skeleton, place all those yaml files into template directory and you can install it. 
# dont do the this in your project, if you are not converting yaml to the helm it can cause escalations. 

root@master:~/app/helm# helm install myapp mychat/
NAME: myapp
LAST DEPLOYED: Wed Jan 22 22:18:18 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods


root@master:~/app/helm# helm ls
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
myapp   default         1               2025-01-22 22:18:18.749870971 -0500 EST deployed        mychat-0.1.0    1.16.0
root@master:~/app/helm#
root@master:~/app/helm#

### you cannnot relate name that your helm is having and the deployment is having a different name and similarly your pod which helm deployment is having is named diffrently
root@master:~/app/helm# kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mydep   3/3     3            3           60s
root@master:~/app/helm#


### Built-in Objects: https://helm.sh/docs/chart_template_guide/builtin_objects/
If you open the deployment.yaml, file you will see that the the names are mentioned 
Why this is happening is because we mentioned our plain deployment file. and in this in yaml file, we have mentioned the name of the deployment mydep
We have something in helm called builtin objects. These are the objects that can call the Release.Names. To use this Release.Name builtin object, what will happen
is whatever name you are defining during the helm install that will be picked up there.

root@master:~# helm uninstall myapp
release "myapp" uninstalled
root@master:~#

### before Release.Name builtin object added.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
spec:
  replicas: 3
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80



### After Release.Name builtin object added.
root@master:~/app/helm# vi mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ Release.Name }}    <----------------- this is coming from the builtin objects mentioned in the docs. https://helm.sh/docs/chart_template_guide/builtin_objects/
spec:
  replicas: 3
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80

root@master:~/app/helm# helm install myapp mychat/
Error: INSTALLATION FAILED: parse error at (mychat/templates/deployment.yaml:4): function "Release" not defined
root@master:~/app/helm#

# i forgot to add the . infront of the release
kind: Deployment
metadata:
  name: {{ [Release.Name }}  <----------- add the . before the Release.Name in the deployment.yaml file and then re-deploy the helm 
root@master:~/app/helm# vi mychat/templates/deployment.yaml
root@master:~/app/helm# helm install myapp mychat/
NAME: myapp
LAST DEPLOYED: Thu Jan 23 07:42:49 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm#

root@master:~/app/helm# helm ls
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
myapp   default         1               2025-01-23 07:42:49.112027018 -0500 EST deployed        mychat-0.1.0    1.16.0
root@master:~/app/helm#

### Now if you see the deploymemt is having the similar name as the helm 
root@master:~/app/helm# kubectl get deployments.apps
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
myapp   3/3     3            3           32s
root@master:~/app/helm#

### Look the pod is also having the similar naming convention as mentioned in our helm chart
root@master:~/app/helm# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
myapp-76449b474c-6ngfj   1/1     Running   0          3m7s
myapp-76449b474c-gtfd6   1/1     Running   0          3m7s
myapp-76449b474c-zthj9   1/1     Running   0          3m7s
newpod                   1/1     Running   0          31h
root@master:~/app/helm#


======================= defining values in values.yaml file and calling it from the deployment.yaml file ==========================
###
root@master:~/app/helm#  helm uninstall myapp
release "myapp" uninstalled
root@master:~/app/helm#

### Lets figure out the values.yaml file.
root@master:~/app/helm# cat mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: 3  <----------- can be defined in the values.yaml 
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:    <------------- containers settings can be defined in the values.yaml file.
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
root@master:~/app/helm#

### when it comes to label or some brutanic logic that you want to implement if else loop or something... that is something that you will define in the _helper.pel. That is a lesson for another day, 
right now we are going to dive into the takes values from the deployment.yaml and put it inside the values.yaml file. 
### I want o move the replicas from the deployment.yaml and put it inside the yaml file and call it the values from the values.yaml file into the deployment.yaml file.

once the values have been defined in the values.yaml, then you should point the values inside the deployment.yaml file. 
example:
deployment.yaml file 
replicas: {{ .Values.replica }}

values.yaml
replica: 4

### you can also do the debug using the helm and dry run it if you dont want the chart to be installed before hand 
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-23 08:06:25.290955215 -0500 EST m=+0.150428752 [debug] Original chart version: ""
install.go:242: 2025-01-23 08:06:25.293558103 -0500 EST m=+0.153031593 [debug] CHART PATH: /root/app/helm/mychat

NAME: myapp
LAST DEPLOYED: Thu Jan 23 08:06:25 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
replicas: 4

HOOKS:
MANIFEST:
---
# Source: mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas:
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80

NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm#

### I want to define the container name from the values.yaml file inside the deployment.yaml file 

# before changes to the deployment.yaml file and values.yaml file
root@master:~/app/helm# cat mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
root@master:~/app/helm#

root@master:~/app/helm# cat mychat/values.yaml
replicas: 4

root@master:~/app/helm#

###after the changes to the deployment.yaml file and the values.yaml file 
root@master:~/app/helm# vi mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}
~               

root@master:~/app/helm# vi mychat/values.yaml
replicas: 4

app:
  con1: blacon
  image: httpd:latest
  port: 80
~            

### lets do the debug with dry run
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-23 08:16:47.963153973 -0500 EST m=+0.132803897 [debug] Original chart version: ""
install.go:242: 2025-01-23 08:16:47.963753456 -0500 EST m=+0.133403368 [debug] CHART PATH: /root/app/helm/mychat

NAME: myapp
LAST DEPLOYED: Thu Jan 23 08:16:48 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
app:
  con1: blacon
  image: httpd:latest
  port: 80
replicas: 4

HOOKS:
MANIFEST:
---
# Source: mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas:
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: blacon
        image: httpd:latest
        ports:
        - containerPort: 80

NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm#


=================== Calling the label with from _workers.tpl file inside the deployment.yaml file. ===========================
### in deployment the labels should be added inside the _helper.tpl and should be added inside the deployment.yaml file as the call, label should be calling from the _helpers.tpl to deployment.yaml file.
### we will take the file deployment file and call the label from the _helpers.tpl file
# before calling the label from the _helpers.tpl 
root@master:~/app/helm# vi mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp <--------------- we will be adding this in the _helpers.tpl file and calling it from there   
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}

### Adding the content of the _helpers.tpl file to call it from the deployment.yaml file
root@master:~/app/helm# vi mychat/templates/_helpers.tpl
{{- define "appdep.labels" -}}
app: myapp
{{- end }}     

### calling the label inside the deployment file 
root@master:~/app/helm# vi mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels: 
      {{- include "appdep.labels" . | nindent 8 }} ## indent is 8, because if you count the white spaces before the brackets it will be 8 whitescpaces   <---------------- this is how we call the labels from the _helpers.tpl file 
      app: myapp <--------------- we will be adding this in the _helpers.tpl file and calling it from there   
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}

root@master:~/app/helm# helm install --debug --dry-run
Error: "helm install" requires at least 1 argument

Usage:  helm install [NAME] [CHART] [flags]
helm.go:86: 2025-01-23 18:51:54.961906117 -0500 EST m=+0.160951397 [debug] "helm install" requires at least 1 argument

Usage:  helm install [NAME] [CHART] [flags]
main.newInstallCmd.MinimumNArgs.func3
        helm.sh/helm/v3/cmd/helm/require/args.go:71
github.com/spf13/cobra.(*Command).ValidateArgs
        github.com/spf13/cobra@v1.8.1/command.go:1145
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:938
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
        helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
root@master:~/app/helm#

### we are missing an argument in deployment.yaml file... the (.) means the present working directory location
root@master:~/app/helm# vi mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels: 
      {{- include "appdep.labels" . | nindent 8 }} ## indent is 8, because if you count the white spaces before the brackets it will be 8 whitescpaces    <---------------- this is how we call the labels from the _helpers.tpl file # the dot before the end bracket is to specify in whcih location 
      app: myapp <--------------- we will be adding this in the _helpers.tpl file and calling it from there   
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}


root@master:~/app/helm# helm install --debug --dry-run
Error: "helm install" requires at least 1 argument

Usage:  helm install [NAME] [CHART] [flags]
helm.go:86: 2025-01-23 18:59:15.940880449 -0500 EST m=+0.148088852 [debug] "helm install" requires at least 1 argument

Usage:  helm install [NAME] [CHART] [flags]
main.newInstallCmd.MinimumNArgs.func3
        helm.sh/helm/v3/cmd/helm/require/args.go:71
github.com/spf13/cobra.(*Command).ValidateArgs
        github.com/spf13/cobra@v1.8.1/command.go:1145
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:938
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
        helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700

### So the issue is since I am calling the label from the _helpers.tpl file, I removed the labels from the deployment file check the above files for the comparison
root@master:~/app/helm# cat mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels:
      {{- include "appdep.labels" . | nindent 8 }} ## indent is 8, because if you count the white spaces before the brackets it will be 8 whitescpaces
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}
root@master:~/app/helm#


### Now this worked because the syntax is properly mentioned and calling is being done properly from the two files'
### which is your values.yaml file and _helpers.tpl file
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-23 19:12:58.06806059 -0500 EST m=+0.122057084 [debug] Original chart version: ""
install.go:242: 2025-01-23 19:12:58.06854299 -0500 EST m=+0.122539451 [debug] CHART PATH: /root/app/helm/mychat


================ we are going to turn the service into the helm take an example of the if else ================ 
# what I will do is create a service.
### you can covert this file below from service to the template 
root@master:~/app/helm# vi mychat/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
    app: myapp  ### <---------------- we will change this so that values.yaml so that,  _helpers.tpl can provide the parameter and service.yaml file can call it from _helpers.tpl
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30123

### you can parse the name, type, and selector from service.yaml into deployment.yaml and _helpers.tpl
root@master:~/app/helm# vi mychat/templates/deployment.yaml  ### this file has nothing to do at this point with the service file, we are just copying the syntax from here.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replica }}
  selector:
    matchLabels:
      {{- include "appdep.labels" . | nindent 8 }} ### <------ we will copy this and put it inside the service file 
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: {{ .Values.app.con1 }}
        image: {{ .Values.app.image }}
        ports:
        - containerPort: {{ .Values.app.port }}


### after the changes the service file should look like this and now we will plug the appdep.labels inside the _helpers.tpl
root@master:~/app/helm# vi mychat/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
    {{- include "appdep.labels" . | nindent 5 }} ### 5 indentations, because we have 5 white spaces
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30123


### if this service is enabled from the values.yaml, only then it should be deployed
### we are going to give the if statement inside the service.yaml file. And I am getting this from the ingress.yaml file which I moved it to the /tmp folder
### for now we are going to copy the syntax from the ingress file 
root@master:~/app/helm# ls /tmp
dbus-TQv2N5FoXn
ingress.yaml
nvim.root
snap-private-tmp
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-bluetooth.service-0nSDTA
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-colord.service-pCQOtu
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-fwupd.service-Ify0vW
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-ModemManager.service-wb1P0B
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-polkit.service-IvSvay
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-power-profiles-daemon.service-rE3dBo
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-switcheroo-control.service-aDNWb8
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-systemd-logind.service-bOiiV0
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-systemd-oomd.service-iOGPi9
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-systemd-resolved.service-07y2o7
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-systemd-timesyncd.service-n8S1Te
systemd-private-929cfcd4beff4fff808b8b6b256e5b53-upower.service-aQmq1x
root@master:~/app/helm# cat /tmp/
cat: /tmp/: Is a directory
root@master:~/app/helm# cat /tmp/ingress.yaml
{{- if .Values.ingress.enabled -}}      ### <----------------- we are going to copy this if statement syntax and plug it inside the service.yaml file 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "mychat.fullname" . }}
  labels:
    {{- include "mychat.labels" . | nindent 4 }}
  {{- with .Values.ingress.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}                          ### and we are going to copy this statement to end, put it inside the service.yaml file. 
spec:
  {{- with .Values.ingress.className }}
  ingressClassName: {{ . }}
  {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- range .Values.ingress.tls }}
    - hosts:
        {{- range .hosts }}
        - {{ . | quote }}
        {{- end }}
      secretName: {{ .secretName }}
    {{- end }}
  {{- end }}
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
          {{- range .paths }}
          - path: {{ .path }}
            {{- with .pathType }}
            pathType: {{ . }}
            {{- end }}
            backend:
              service:
                name: {{ include "mychat.fullname" $ }}
                port:
                  number: {{ $.Values.service.port }}
          {{- end }}
    {{- end }}
{{- end }}
root@master:~/app/helm#

### {{- if .Values.app.service.enabled -}} ### after copying the syntax from ingress.yaml, we changed it to app.service. We are going to copy this and save it in the value.yaml file.
root@master:~/app/helm# vi mychat/templates/service.yaml
{{- if .Values.app.service.enabled -}} ### after copying the syntax from ingress.yaml, we changed it to app.service. We are going to copy this and save it in the value.yaml file.
apiVersion: v1
kind: Service
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
    {{- include "appdep.labels" . | nindent 5 }} ### 5 indentations, because we have 5 white spaces
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30123
{{-end }}

root@master:~/app/helm# vi mychat/templates/service.yaml
root@master:~/app/helm# vi mychat/
charts/      Chart.yaml   .helmignore  templates/   values.yaml
### we added the if statement inside the service.yaml file which is stating that if the service is enabled then only deploy the file; else there will be an error. 
root@master:~/app/helm# vi mychat/values.yaml
replicas: 4

app:
  con1: blacon
  image: httpd:latest
  port: 80
  service: enabled ### this is the part which is service.yaml is calling 

### lets do the dry run and see what is the output we see.
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-25 21:13:00.675892572 -0500 EST m=+0.321716553 [debug] Original chart version: ""
install.go:242: 2025-01-25 21:13:00.676679357 -0500 EST m=+0.322503291 [debug] CHART PATH: /root/app/helm/mychat

Error: INSTALLATION FAILED: parse error at (mychat/templates/service.yaml:15): "-en"
helm.go:86: 2025-01-25 21:13:00.8006113 -0500 EST m=+0.446435274 [debug] parse error at (mychat/templates/service.yaml:15): "-en"
INSTALLATION FAILED
main.newInstallCmd.func2
        helm.sh/helm/v3/cmd/helm/install.go:158
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
        helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
root@master:~/app/helm#

### fixing the file 
root@master:~/app/helm# vi mychat/templates/service.yaml
:set number
  1 {{- if .Values.app.service.enabled -}} ### after copying the syntax from ingress.yaml, we changed it to app.service and we are going to copy this and sa    ve it in the value.yaml file.
  2 apiVersion: v1
  3 kind: Service
  4 metadata:
  5   name: newpod-svc
  6 spec:
  7   type: NodePort
  8   selector:
  9     {{- include "appdep.labels" . | nindent 5 }} ### 5 indentations, because we have 5 white spaces
 10   ports:
 11   - port: 80
 12     protocol: TCP
 13     targetPort: 80
 14     nodePort: 30123
 15 {{- end }} # <--------------- this was the issue

### dry run again, we got another issue 
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-25 21:16:12.310900905 -0500 EST m=+0.264693237 [debug] Original chart version: ""
install.go:242: 2025-01-25 21:16:12.311666044 -0500 EST m=+0.265458379 [debug] CHART PATH: /root/app/helm/mychat

Error: INSTALLATION FAILED: template: mychat/templates/service.yaml:1:14: executing "mychat/templates/service.yaml" at <.Values.app.service.enabled>: can't evaluate field enabled in type interface {}
helm.go:86: 2025-01-25 21:16:12.443621835 -0500 EST m=+0.397414140 [debug] template: mychat/templates/service.yaml:1:14: executing "mychat/templates/service.yaml" at <.Values.app.service.enabled>: can't evaluate field enabled in type interface {}
INSTALLATION FAILED
main.newInstallCmd.func2
        helm.sh/helm/v3/cmd/helm/install.go:158
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
        helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
root@master:~/app/helm#

### fixing the values.yaml file 
root@master:~/app/helm# cat mychat/values.yaml
replicas: 4

app:
  con1: blacon
  image: httpd:latest
  port: 80
  service:
    enabled ### I had syntax error, so I have put this in a new line ### this is the part which is service.yaml is calling

### lets do the dry run again, got an error
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-25 21:19:14.534585722 -0500 EST m=+0.181918804 [debug] Original chart version: ""
install.go:242: 2025-01-25 21:19:14.535478552 -0500 EST m=+0.182811625 [debug] CHART PATH: /root/app/helm/mychat

Error: INSTALLATION FAILED: template: mychat/templates/service.yaml:1:14: executing "mychat/templates/service.yaml" at <.Values.app.service.enabled>: can't evaluate field enabled in type interface {}
helm.go:86: 2025-01-25 21:19:14.637731769 -0500 EST m=+0.285064840 [debug] template: mychat/templates/service.yaml:1:14: executing "mychat/templates/service.yaml" at <.Values.app.service.enabled>: can't evaluate field enabled in type interface {}
INSTALLATION FAILED
main.newInstallCmd.func2
        helm.sh/helm/v3/cmd/helm/install.go:158
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
        helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
root@master:~/app/helm#

### fixing step: going to create another helm charter to cross verify and find out quickly what is the issue with our files. 
### I found the cross verification file of the values from the test2 chart that the enabled should have boolean values; meaning true or false. 
root@master:~/app# helm create test2
Creating test2
root@master:~/app# cd test2/
root@master:~/app/test2# cd ..
root@master:~/app# vi test2/values.yaml
...
ingress:
  enabled: false
...

### So I have to define boolean in my values.yaml file instead of enabled 
root@master:~/app/test2# cd ..
root@master:~/app# cd helm/
root@master:~/app/helm# vi mychat/values.yaml
replicas: 4

app:
  con1: blacon
  image: httpd:latest
  port: 80
  service:
    enabled: true ### Boolean statement  ### I had syntax error, so I have put this in a new line ### this is the part which is service.yaml is calling

### finally it worked and file is fixed and dry run shows it is deployable
IMPORTANT:
### Since we have defined in our values.yaml file service enabled as true... The file will be show the output in the dry run
    ### scroll below and you will the service.yaml below and if you turn the statement into false in the values.yaml file; next time when you will do the dry run, it will not show. 

root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-25 21:36:41.964875021 -0500 EST m=+0.190144029 [debug] Original chart version: ""
install.go:242: 2025-01-25 21:36:41.965196646 -0500 EST m=+0.190465661 [debug] CHART PATH: /root/app/helm/mychat

NAME: myapp
LAST DEPLOYED: Sat Jan 25 21:36:42 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
app:
  con1: blacon
  image: httpd:latest
  port: 80
  service:
    enabled: true
replicas: 4

HOOKS:
MANIFEST:
---
# Source: mychat/templates/service.yaml
### after copying the syntax from ingress.yaml, we changed it to app.service and we are going to copy this and save it in the value.yaml file.
apiVersion: v1
kind: Service      <-------------------------------------------------------- this is showing right now, it will vanish from here if you turn it false
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
     app: myapp ### 5 indentations, because we have 5 white spaces
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30123
---
# Source: mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas:
  selector:
    matchLabels:
        app: myapp ## indent is 8, because if you count the white spaces before the brackets it will be 8 whitescpaces
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: blacon
        image: httpd:latest
        ports:
        - containerPort: 80

NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm#


### dry run again to demonstrate if you turn the service enabled false from the values.yaml file, it will not be showing during deployment which means service will not be deployed
### adding false statement to demonstrate how it will how the chart will not deploy the service if the service enabled is false in the values.yaml file 
### service will not be picked up during the deployment of the chart. 
root@master:~/app/helm# vi mychat/values.yaml
replicas: 4

app:
  con1: blacon
  image: httpd:latest
  port: 80
  service:
    enabled: false ### Boolean statement  ### I had syntax error, so I have put this in a new line ### this is the part which is service.yaml is calling


### you can see now that If we do the dry run the service is not showing up in the dry run, please scroll below to see the difference. 
root@master:~/app/helm# helm install myapp mychat/ --debug --dry-run
install.go:225: 2025-01-25 21:47:33.913386499 -0500 EST m=+0.139370226 [debug] Original chart version: ""
install.go:242: 2025-01-25 21:47:33.913485687 -0500 EST m=+0.139469368 [debug] CHART PATH: /root/app/helm/mychat

NAME: myapp
LAST DEPLOYED: Sat Jan 25 21:47:34 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
app:
  con1: blacon
  image: httpd:latest
  port: 80
  service:
    enabled: false
replicas: 4

HOOKS:
MANIFEST:
---
# Source: mychat/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas:
  selector:
    matchLabels:
        app: myapp ## indent is 8, because if you count the white spaces before the brackets it will be 8 whitescpaces
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: blacon
        image: httpd:latest
        ports:
        - containerPort: 80

NOTES:
Thank you for using our application
for technical support visist http://www.google.com/blog

To list application pods run following

kubectl get pods

kubectl get pods
root@master:~/app/helm#

### you can go over other concepts through the helm and helm chart and helm programming through the helm docs 
https://helm.sh/docs/chart_template_guide/getting_started/
### this is the research you have to do on your own!!! 
### work on the projects to get to know more.

### remember these skeleton that you create with the helm create command will help you a lot 
helm create <chartname>

### how I enabled the helm auto-completion STEPS:
root@master:~/app/helm# vi ~/.bashrc
# autocompletion kubernetes
source <(kubectl completion bash)
# this will add the autocompletion to the letter k and once you press tab it will write kubectl
alias k=kubectl
complete -o default -F __start_kubectl k
# autocompletion helm     ### <------------ add this line in your .bashrc file
source <(helm completion bash)


### then re-load with source command 
root@master:~/app/helm# source ~/.bashrc
root@master:~/app/helm#


### I want to create a package out of this..
### how to package our chart?
root@master:~/app/helm# helm package mychat/
Successfully packaged chart and saved it to: /root/app/helm/mychat-0.1.0.tgz
root@master:~/app/helm# ls
mychat  mychat-0.1.0.tgz
root@master:~/app/helm# mkdir package
root@master:~/app/helm# mv mychat-0.1.0.tgz package/
root@master:~/app/helm#


### whenever we are keeping the package mayebe in our github repository as our helmchart repository.
### we should keep our index file in that, wihtout the index file it will not work be working at all. 
### generate the index file 
root@master:~/app/helm/package# helm repo index .
root@master:~/app/helm/package# ls
index.yaml  mychat-0.1.0.tgz
root@master:~/app/helm/package#

### this index file keeps the information/metadata of your chart
### if you have hundreds of charts, it will be mentioned in this file
### whenever you are updating anything on chart or package, you need to regenerate this repo index. (YOU ALWAYS HAVE TO DO THAT)
root@master:~/app/helm/package# cat index.yaml
apiVersion: v1
entries:
  mychat:
  - apiVersion: v2
    appVersion: 1.16.0
    created: "2025-01-26T15:22:37.170234356-05:00"
    description: A Helm chart for Kubernetes
    digest: bc401d38bb2c94999e8c4bc46bfb4114a37d26a22010ed8b8df043cc138193a7
    name: mychat
    type: application
    urls:
    - mychat-0.1.0.tgz
    version: 0.1.0
generated: "2025-01-26T15:22:37.164825491-05:00"
root@master:~/app/helm/package#

### whenever you are updating or recreating the chart or package, you need to regenerate the index to keep 
### post it to github

root@master:~/app/helm# ls
mychat  package
root@master:~/app/helm# cd package/
root@master:~/app/helm/package#


root@master:~/app/helm/package# git config user.name
mussawirimam
root@master:~/app/helm/package#

root@master:~/app/helm/package# git init
hint: Using 'master' as the name for the initial branch. This default branch name
hint: is subject to change. To configure the initial branch name to use in all
hint: of your new repositories, which will suppress this warning, call:
hint:
hint:   git config --global init.defaultBranch <name>
hint:
hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
hint: 'development'. The just-created branch can be renamed via this command:
hint:
hint:   git branch -m <name>
Initialized empty Git repository in /root/app/helm/package/.git/
root@master:~/app/helm/package# git branch -l
root@master:~/app/helm/package#



root@master:~/app/helm/package# git branch --list
* master
root@master:~/app/helm/package# git push origin main
error: src refspec main does not match any
error: failed to push some refs to 'github.com:mussawirimam/gittestdelete.git'
root@master:~/app/helm/package# git push origin master
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (4/4), done.
Writing objects: 100% (4/4), 1.78 KiB | 911.00 KiB/s, done.
Total 4 (delta 0), reused 0 (delta 0), pack-reused 0
remote:
remote: Create a pull request for 'master' on GitHub by visiting:
remote:      https://github.com/mussawirimam/gittestdelete/pull/new/master
remote:
To github.com:mussawirimam/gittestdelete.git
 * [new branch]      master -> master
root@master:~/app/helm/package#


### you need to get the url of the chart you have pushed to the Github
### go to the file that you need from github repo > click on the raw > copy the url:
e.g (it might be different in your case
https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml

### do helm repo l
root@master:~/app/helm/package# helm repo ls
NAME                    URL
bitnami                 https://charts.bitnami.com/bitnami
inseefrlab              https://inseefrlab.github.io/helm-charts-datascience
shubhamtatvamasi        https://shubhamtatvamasi.github.io/helm
root@master:~/app/helm/package# helm repo
add     (add a chart repository)
index   (generate an index file given a directory containing packaged charts)
list    (list chart repositories)
remove  (remove one or more chart repositories)
update  (update information of available charts locally from chart repositories)
root@master:~/app/helm/package# helm repo remove inseefrlab
"inseefrlab" has been removed from your repositories
root@master:~/app/helm/package# helm repo remove shubhamtatvamasi
"shubhamtatvamasi" has been removed from your repositories
root@master:~/app/helm/package#
root@master:~/app/helm/package#
root@master:~/app/helm/package# helm repo ls
NAME    URL
bitnami https://charts.bitnami.com/bitnami
root@master:~/app/helm/package#
root@master:~/app/helm/package#
root@master:~/app/helm/package# helm repo add hrepo https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml
Error: looks like "https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml/index.yaml : 404 Not Found
root@master:~/app/helm/package# helm repo add hrepo 'https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml'
Error: looks like "https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml/index.yaml : 404 Not Found
root@master:~/app/helm/package# helm repo add hrepo 'https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml'
Error: looks like "https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/mussawirimam/gittestdelete/refs/heads/master/index.yaml/index.yaml : 404 Not Found
root@master:~/app/helm/package#

### trouble shoot
so the issue was that I was adding the index.yaml raw file link and that is not how we do it.
we need to add the repo link only

