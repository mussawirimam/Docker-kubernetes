LEGEND 
ğŸ–¥ï¸ Server
ğŸ’» containers/replicas
## if one container is crashed and replica container needs to be spun up by itself, docker itself cant do it alone. You will need a mechanism such as Docker Swarm or Kubernetes. To make your container highly available. 
## you have to define if the load is increasing on your server (container) how many maximum or minumum containers should spin up based on your CPU utilization (Horizontal Pod Autoscaling (HPA)) in your cluster. 

Manager/Lead Node 
We can have multiple manager nodes 
If we have multiple manager nodes, we will have a leader of them who will be taking the decisions.
max 7 manager but no limit but one has to be leader node

Worker Nodes
We can have multiple worker nodes
Job of a worker node is to get deployed your containers in it, your worker nodes. 
In docker swarm allows the containers to be deployed on the managers or leaders as well. 

If multiple node is down
Suggestion: always create even number of nodes that can be divided in groups 4,6,8
Assume that your manager is getting down. Always create even number of nodes that can be in groups of 4,6,8

If you have 3 containers with 2 managers and 1 leader, and both managers are down, the leader will not be able to make a decision if you request the creation of a deployment with 3 replicas (of containers), because the managers are responsible 
for managing the worker nodes and it will not know in which one to place the worker nodes. If you have 4 worker nodes for example. If you want it to be working, you will need to reset the manager 

## if your managers are down, you can create your leader a manager as well
## your worker nodes can be promoted as the managers 

#########################################
# docker swarm init --force-new-cluster #
#########################################

In this case where multiple manager node are down one eader will not be able to take decision in this situation we need
To create new cluster
#docker swarm init --force-new-cluster
Here your node will be having already information of worker node, your swarm services will continue to run.
Promote existing worker to manager node #docker node promote {node-name}
All worker node will be having your deployment in it. U can disable deployment in manager node #docker node update - -availability drain {node-name}

Mastering the Raft Consensus Algorithm: A Comprehensive Tutorial in Distributed Systems
https://www.youtube.com/watch?app=desktop&v=ZyqAbQkpeUo
https://www.youtube.com/watch?v=IujMVjKvWP4
## RAFT ALGORITHM are used to distribute the workload

Distributed consensus using raft algorithm
what is Raft consensus?
Consensus is a fundamental problem in fault-tolerant distributed systems. Consensus involves multiple servers agreeing on values. Once they reach a decision on a value, that decision is final.
Typical consensus algorithms make progress when any majority of their servers is available; for example, a cluster of 5 servers can continue to operate even if 2 servers fail. If more servers fail, they stop making progress
Every decision has to agreed upon by majority of manager


docker service create - -replicas=3 httpd:latest
â¬‡ï¸
Orchestrator   ğŸ’» manager/leader Node
â¬‡ï¸                Task will be updating manager node, so that manager can track status of containers, if container fails task also fail and it will inform manager.
Scheduler
â¬‡ï¸
----------------------------
ğŸ’»``````ğŸ’»``````ğŸ’»``````ğŸ’»
â¬‡ï¸      â¬‡ï¸      â¬‡ï¸      â¬‡ï¸
task    task     task    task

the reason for that is to spreading the replicas on multiple nodes, so there is no single point of failure (in this case single node) 
e.g if the worker node is failed, then scheduler will create another container on different node that is available. 
if all nodes are failed, in this case manager node have space, it will create a container on that node. 

DOCKER SWARM NETWORK OVERLAY
============================
#docker network create - -driver overlayimynet
#docker service create - -name mydep - -network mvol - -replicas=3 -p 8080:80 httpd:latest

----------------------------
    OVERLAY NETWORK
----------------------------
â¬‡ï¸      â¬‡ï¸      â¬‡ï¸      â¬‡ï¸
ğŸ’»``````ğŸ’»``````ğŸ’»``````ğŸ’»

overlay will not displayed generally listing all the network docker network ls, untill unless the swarm mode is enabled only then the driver will work. You have to do swarm initialiazation.
What overlay does:
it will be creating the layer of the network, where all the the nodes are connected to single adapter, which is overlay network where all nodes will be able to connect with the containers in those nodes.
do not confuse kubernetes with this

the default name for network is Ingress which will be using overlay driver on docker swarm.

----------------------------
          INGRESS (OVERLAY)
----------------------------
            â¬‡ï¸
      |HOSTPORT 8080|
      |             |            two container in same node with same port is not possible 
      â†“             â†“
PORT 80              PORT 80



#docker network create --driver overlay myol
#docker service create --name mydep --network mvol --replicas=3 -p 8080:80 httpd:latest
----------------------------
          INGRESS (OVERLAY)
----------------------------
              â†“
â•­ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•®
â”Š       Host-port:8080          â”Š  Swarm Ingress controller will have inbuilt load balancer it will be forwarding traffic to mapped containers ports
â”Š   Swarm ingress/overlay       â”Š
â”Š     |              |          â”Š
â”Š PORT 80          PORT80       â”Š
â•° â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•¯

#docker service create - -name mydep - -replicas=3 -p 8080:80 httpd:latest

port 8080 on host can be mapped to multiple port 80 on containers but the condition is that all the containers should belong to particular deployment.
for example my deployment name is mydep. 

Once you create a deployment with port 8080, you will not be able to use port 8080 for another deployment. 


                                (DOCKER SWARM NETWORK INGRESS)            (Here load balancing will be done by ingress)
                                             ğŸ‘¨â€ğŸ’»
---------------------------------------------------------------------------------------------------------
            â¬‡ï¸                               â¬‡ï¸                                   â¬‡ï¸
            Node-1                          Node-2                                Node-3
            192.168.1.20                    192.168.1.25                          192.168.1.30  
â•­ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•®  â•­ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•®   â•­ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•® 
â”Š     Host port 8080                Host port 8080                      Host port 8080                 â”Š
â”Š  --------------------------------------------------------------------------------------------------  â”Š
                                    Swarm ingress/overlay                                              â”Š 
â”Š  ------------/----------------/\----------------/-----------------/--------------------------------- â”Š
â”Š   PORT 80   /                /  \      PORT80  /                 /                                   â”Š 
â•° â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•¯  â•° â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•¯   â•° â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•¯


==============ARCHITECTURE OF PRACTICAL LAB:==============
3 nodes Cluseter on ec2:
             === Manager ===
            |               |
        worker 1      worker 2 

LAB ON AWS:
step1: delete the garbage security groups (if you have any)
step2: click on the create security groups
step3: name your security group (name it docker-swarm)
step4: name description (docker-swarm)
step5: dont change the vpc, leave it as default vpc
    On your fresly Security group there will be no inbound configuration and outbound would be allowed for all trafic. 
step6: click on Create security group
step7: Edit Inboud Rule
Step8: add Rule
step9: enable all traffic. 
step10: select the docker swarm from security group drop down menu 

## the reason we did the above steps is to keep the 3 node cluster in one security group (docker-swarm security group)
## the reason for this is so that the servers within the security group (docker-swarm security group) are able to communicate with each other. 
## you wont have to enable any additional rule with this architecture above ğŸ‘†

Step11: click on ec2 service
step12: click on launch instance
        a. name it node 
        b. select Ubuntu 
        c. select t2 mediume <---------------<< important <<<<<<<<
        d. choose the keypair
        e. click on select existing security existing security group
        f. click on drop down and select docker-swarm 
step13: ENTER 3 NUMBER OF INSTANCES 
        g. click on Advance details
        h. find user data - optional box write a shell script below ğŸ‘‡
â•­ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€  â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€  â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•®         
#!/bin/bash
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh 
â•° â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€  â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€  â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â”€â”€ â•¯
## sudo needs to be copied without sudo above â¬† you will not be able to install docker on 3 instances
## ubuntu user cannot install anything by itself, that is why you need sudo.
step14: Launch the instance

## you should have three instances
## PRO-TIP: these are paid ec2 instances you will have to pay for it. You can write a CronJob or Lambda for safe side but 
## SHUTDOWN EVERYTIME YOU USE IT

Step14: Adding ssh in inbound rules
    1. go to security group (docker-swarm security group)
    2. click on edit inbound rules
    3. from drop down menu under Type column, select ssh
    4. from drop down menu under Source column, select anywhere.
    5. for cidr/block add 0.0.0.0/0 from anywhere


----------------------------------------------------------------------------------------------------------------------------------------------------------------
## to check if DOCKER was installed on the system
root@ip-172-31-30-94:~# systemctl status docker
â— docker.service - Docker Application Container Engine
     Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; preset: enabled)
     Active: active (running) since Mon 2024-08-19 21:53:52 UTC; 7min ago
TriggeredBy: â— docker.socket
       Docs: https://docs.docker.com
   Main PID: 773 (dockerd)

=======CHANGING THE NAME OF THE HOSTS=======
root@ip-172-31-30-94:~# hostnamectl
 Static hostname: ip-172-31-30-94
       Icon name: computer-vm
         Chassis: vm ğŸ–´
      Machine ID: ec260cef56df604a88985cd50783f18c
         Boot ID: 159221f5ac6b4479ba91b36f84057be8
  Virtualization: xen
Operating System: Ubuntu 24.04 LTS
          Kernel: Linux 6.8.0-1009-aws
    Architecture: x86-64
 Hardware Vendor: Xen
  Hardware Model: HVM domU
Firmware Version: 4.11.amazon
   Firmware Date: Thu 2006-08-24
    Firmware Age: 17y 11month 3w 5d
root@ip-172-31-30-94:~#

root@ip-172-31-30-94:~# vi /etc/hostname
manager
#ip-172-31-30-94
~
~
root@ip-172-31-30-94:~# reboot



root@ip-172-31-24-176:~# vi /etc/hostname
node1
#ip-172-31-24-176
~
~
root@ip-172-31-24-176:~# reboot

root@ip-172-31-17-168:~# vi /etc/hostname
node2
#ip-172-31-17-168
~
~
root@ip-172-31-17-168:~# reboot"


=============INITIALIZING DOCKER SWARM=============
 root@manager:~# docker info
Client: Docker Engine - Community
 Version:    27.1.2
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.16.2
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.29.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
...
 Swarm: inactive    <---------------<<
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 ...

###ACTIVATING DOCKER SWARM###
root@manager:~# docker swarm init
Swarm initialized: current node (j0jcwtg4xqq9wotyz37q7nc3m) is now a manager.       <---------------<< this node is a manager now

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377       <---------------<< to join worker node (to be part of cluster)

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.      <---------------<< to join as manager

docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377


root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2               

===JOINING THE DOCKER WORKER NODE===
root@node1:~# docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377
This node joined a swarm as a worker.

root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2



ubuntu@node2:~$ sudo su -
root@node2:~# docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377
This node joined a swarm as a worker.

root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2           <--------------<< * means leader here under ID
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2
yibgo99hdxkhkjk4bm6txef1f     node2      Ready     Active                          27.1.2


=== TO DEMOTE, PROMOTE, REMOVE, AND JOIN THE WORKER NODES IN DOCKER SWARM CLUSTER ===
## worker node can be promoted as the manager role

root@manager:~# docker node promote node2
Node node2 promoted to a manager in the swarm.
root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2
yibgo99hdxkhkjk4bm6txef1f     node2      Ready     Active         Reachable        27.1.2       <-----------------<< node is reachable as manager status

## worker node can be demoted from the manager role

root@manager:~# docker node demote node2
Manager node2 demoted in the swarm.
root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2
yibgo99hdxkhkjk4bm6txef1f     node2      Ready     Active                          27.1.2       <---------------<< no longer manager

## to remove docker node from the cluster
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2
yibgo99hdxkhkjk4bm6txef1f     node2      Ready     Active                          27.1.2
root@manager:~# docker node rm ndocker node rm node2
Error response from daemon: rpc error: code = FailedPrecondition desc = node yibgo99hdxkhkjk4bm6txef1f is not down and #can't be removed

root@node2:~# docker swarm leave
Node left the swarm.


root@manager:~# docker node rm node2
node2
root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2

## GENERATING TOKEN TO JOIN THE WORKER 
root@manager:~# docker swarm join-token worker
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377

## PASTE IT IN NODE2
root@node2:~# docker swarm join --token SWMTKN-1-0sorxhqqi5487iqlw7v89yldz37uh50hdmoni845otx49yczk5-cgtwqs66wdomb50wxj5p0y8jj 172.31.30.94:2377
This node joined a swarm as a worker.


## DOCKER JOINS BACK ON THE DOCKER SWARM CLUSTER
root@manager:~# docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
j0jcwtg4xqq9wotyz37q7nc3m *   manager    Ready     Active         Leader           27.1.2
bcq22d016xslovrt72oi3pqv8     node1      Ready     Active                          27.1.2
8o5zajfmm9qe14z54uewsmu0o     node2      Ready     Active                          27.1.2

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## I WANT TO CREATE A DEPLOYMENT WITH 3 REPLICAS AND THOSE REPLICAS NEEDS TO BE CREATED IN SWARM CLUSTER

## ERROR how to remove garbage service that gets created
root@manager:~# docker service create mydep --replicas 3 -p 8080:80 nginx:latest
image mydep:latest could not be accessed on a registry to record
its digest. Each node will access mydep:latest independently,
possibly leading to different nodes running different
versions of the image.

norrpgdi3zlkg0oz22o04uy51
overall progress: 0 out of 1 tasks
1/1: No such image: mydep:latest
^COperation continuing in background.
Use `docker service ps norrpgdi3zlkg0oz22o04uy51` to check progress.
root@manager:~# docker service ps
"docker service ps" requires at least 1 argument.
See 'docker service ps --help'.

Usage:  docker service ps [OPTIONS] SERVICE [SERVICE...]

List the tasks of one or more services
root@manager:~# docker service ls
ID             NAME                MODE         REPLICAS   IMAGE          PORTS
norrpgdi3zlk   interesting_kilby   replicated   0/1        mydep:latest
root@manager:~# docker service rm norrpgdi3zlk
norrpgdi3zlk
root@manager:~# docker service ls
ID        NAME      MODE      REPLICAS   IMAGE     PORTS

=======CREATING 3 REPLICAS SERVICE=======
root@manager:~# docker service create --name mydep --replicas 3 -p 8080:80 nginx:latest
ykkef645oxxbdbzf8l09qt50e
overall progress: 3 out of 3 tasks
1/3: running   [==================================================>]
2/3: running   [==================================================>]
3/3: running   [==================================================>]
verify: Service ykkef645oxxbdbzf8l09qt50e converged

root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp

GO TO AWS AND OPEN PORT 8080 IN SECURITY GROUP:
step1: go to security group
step2: click on edit inbound rules
    1. add a rule custom TCP 
    2. from drop down add port range 8080
    3 from drop down select (Anywhere-IPv4)
    4. save rules
step3: go to ec2 Insctances
    1. copy any instance public ip address
    2. go to new tab paste it <IP>:8080
step4: ---ON BROWSER---
Welcome to nginx!
If you see this page, the nginx web server is successfully installed and working. Further configuration is required.


## 3 replicas are running but I dont know on which nodes these 3 replicas are running
## I want to check that
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp

root@manager:~# docker service ps mydep                                             <------------------------------<< to check which nodes are running which replicas
ID             NAME      IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
fxsm19t15ama   mydep.1   nginx:latest   manager   Running         Running 8 minutes ago
ktxge1cd0khk   mydep.2   nginx:latest   node1     Running         Running 8 minutes ago
mj58o9b31gmv   mydep.3   nginx:latest   node2     Running         Running 8 minutes ago

## if you dont define the replicas, it will create single deployment
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   1/1        nginx:latest   *:9090->80/tcp
## to find which node is running this container 
root@manager:~# docker service ps newapp
ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
9tuc2q36n80y   newapp.1   nginx:latest   manager   Running         Running 49 seconds ago

======== TO SCALE UP THE CONTAINER FOR FAULT-TOLERANCE ========
root@manager:~# docker service scale newapp=3
newapp scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================>]
2/3: running   [==================================================>]
3/3: running   [==================================================>]
verify: Service newapp converged
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   3/3        nginx:latest   *:9090->80/tcp

root@manager:~# docker service pdocker service ps newapp
ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
9tuc2q36n80y   newapp.1   nginx:latest   manager   Running         Running 3 minutes ago
39wh6tkpqeuf   newapp.2   nginx:latest   node1     Running         Running 59 seconds ago
ms49vece2xi8   newapp.3   nginx:latest   node2     Running         Running 58 seconds ago

root@manager:~# docker service scale newapp=5
newapp scaled to 5
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service newapp converged
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   5/5        nginx:latest   *:9090->80/tcp

root@manager:~# docker service ps newapp
ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE                ERROR     PORTS
9tuc2q36n80y   newapp.1   nginx:latest   manager   Running         Running 4 minutes ago
39wh6tkpqeuf   newapp.2   nginx:latest   node1     Running         Running about a minute ago
ms49vece2xi8   newapp.3   nginx:latest   node2     Running         Running about a minute ago
0fef6yf25dbq   newapp.7   nginx:latest   node2     Running         Running 17 seconds ago
ocr3upz88ek7   newapp.8   nginx:latest   manager   Running         Running 18 seconds ago

=== TO SCALE DOWN ===
root@manager:~# docker service scale newapp=2
newapp scaled to 2
overall progress: 2 out of 2 tasks
1/2: running   [==================================================>]
2/2: running   [==================================================>]
verify: Service newapp converged
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   2/2        nginx:latest   *:9090->80/tcp
root@manager:~# docker service ps newapp
ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
9tuc2q36n80y   newapp.1   nginx:latest   manager   Running         Running 5 minutes ago
39wh6tkpqeuf   newapp.2   nginx:latest   node1     Running         Running 3 minutes ago

=== 2ND METHOD TO SCALE UP USING UPDATE COMMAND === 
root@manager:~# docker service update --replicas 4 newapp
newapp
overall progress: 4 out of 4 tasks
1/4: running   [==================================================>]
2/4: running   [==================================================>]
3/4: running   [==================================================>]
4/4: running   [==================================================>]
verify: Service newapp converged
root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   4/4        nginx:latest   *:9090->80/tcp

root@manager:~# docker service ps newapp
ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
9tuc2q36n80y   newapp.1   nginx:latest   manager   Running         Running 6 minutes ago
39wh6tkpqeuf   newapp.2   nginx:latest   node1     Running         Running 4 minutes ago
q6h39yt0ifx6   newapp.3   nginx:latest   node2     Running         Running 34 seconds ago
uh5i7k6hhr7b   newapp.4   nginx:latest   node2     Running         Running 34 seconds ago

===== IN A SITUATION WHERE I WANT TO PLACE ONE OF THE POD IN EACH NODE AND EVEN IN FUTURE NEW NODES ARE ADDED TO SWARM CLUSTER, AUTOMATICALLY THAT CONTAINER NEEDS TO BE DEPLOYED NODE NUMBER 3. AND IF SOMEONE IS REMOVEING NODES FROM MY CLUSTER, SO THOSE CONTAINERS WHICH WERE DELETED FROM THE DELETED NODES SHOULDNT BE CREATED ANYWHERE ELSE ======
THIS FEATURE IS CALLED GLOBAL IN DOCKER SWARM AND IN KUBERNETES IS DEAMON SET 
#(In Kubernetes, the equivalent feature to Docker Swarm's "global" service is called a DaemonSet.
#A DaemonSet ensures that a copy of a particular pod runs on all (or some) nodes in the cluster. This is similar to Docker Swarm's "global" mode, where a service is deployed to every node in the swarm.)
=== GLOBAL FEAUTURE OF DOCKER SWARM ===
Scenerio 1:
ğŸ–¥ï¸
ğŸ’»
ğŸ–¥ï¸ + ğŸ–¥ï¸ ++ ğŸ–¥ï¸
ğŸ’» + ğŸ’» ++ ğŸ’»

Scenerio 2:
ğŸ–¥ï¸
ğŸ’»
ğŸ–¥ï¸ || - ğŸ–¥ï¸ - ğŸ–¥ï¸
ğŸ’»       -    -  

## NOTE: even though I am not mentioning the replicas, 3 replicas will be created and each one container replicas will be placed in each node (all available nodes)

root@manager:~# docker service create --name monapp --mode global nginx:latest
vqrknolroob4k8tspy7t4puv1
overall progress: 3 out of 3 tasks
bcq22d016xsl: running   [==================================================>]
j0jcwtg4xqq9: running   [==================================================>]
8o5zajfmm9qe: running   [==================================================>]
verify: Service vqrknolroob4k8tspy7t4puv1 converged

root@manager:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
vqrknolroob4   monapp    global       3/3        nginx:latest                   <-------------- MODE=global
ykkef645oxxb   mydep     replicated   3/3        nginx:latest   *:8080->80/tcp
rdod9ohawv6r   newapp    replicated   4/4        nginx:latest   *:9090->80/tcp

## if you see all three replicas are distributed in global mode meaning dispersed in each node
root@manager:~# docker service ps monapp
ID             NAME                               IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
w417qcocp76c   monapp.8o5zajfmm9qe14z54uewsmu0o   nginx:latest   node2     Running         Running 2 minutes ago
l1aehk14t7lq   monapp.bcq22d016xslovrt72oi3pqv8   nginx:latest   node1     Running         Running 2 minutes ago
pdyezzxpjyqa   monapp.j0jcwtg4xqq9wotyz37q7nc3m   nginx:latest   manager   Running         Running 2 minutes ago

