 Go to Security Group
1. click on create a security group
  a. give security group name (mykube)
  b. description (kubernetes internal communcation)
  c. keep the vpc to default
  d. click on create 
2. add inboud rule
  a. under type drop down menu, select (all traffic)
  b. under Cidr/block drop down next to source, select (mykube) security group
3. add rule for SSH
  a. add rule for ssh under type drown down menu select (SSH)
  b. under source drop down menu, select (Anywhere-IPv4)
  c. cidr/block next to source is (0.0.0.0/0)
save the rules

Create Ec2 Instance
1. create on (launch instances)
  a. name the node and tag (node)
  b. select image (ubuntu)
  c. instance type (t2.medium)
  d. create a keypair or if you have existing select that.
  e. In Network select (Select existing Security Group: mykube)
  f. click on advance setting
    aa. scroll down to (User data - optional) 
==== ADD THIS SCRIPT ====
#!/bin/bash
sudo swapoff -a

sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl
sudo mkdir -p -m 755 /etc/apt/keyrings
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
sudo echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update && sudo apt-get install -y kubelet kubeadm kubectl

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
sudo cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

sudo lsmod | grep br_netfilter
sudo lsmod | grep overlay

sudo sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward


OS=xUbuntu_22.04
VERSION=1.28

sudo echo "deb [signed-by=/usr/share/keyrings/libcontainers-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list

sudo echo "deb [signed-by=/usr/share/keyrings/libcontainers-crio-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list


sudo mkdir -p /usr/share/keyrings

sudo curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | gpg --dearmor -o /usr/share/keyrings/libcontainers-archive-keyring.gpg

sudo curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/Release.key | gpg --dearmor -o /usr/share/keyrings/libcontainers-crio-archive-keyring.gpg

sudo apt-get update && apt-get install cri-o cri-o-runc cri-tools -y


sudo systemctl start crio.service


sudo systemctl enable crio.service


sudo systemctl status crio.service

########################################################################################################################

### CHANGE THE HOSTNAMES ###
vi /etc/hostname 
master
wq!

vi /etc/hostname 
node1
wq!

vi /etc/hostname 
node2
wq!

##Reboot the systems
---------

==== To Initialize the MASTER AND WORKER servers ====

root@manager:~# kubeadm init                      <---------------------<<
I0902 23:11:04.216440    1656 version.go:256] remote version is much newer: v1.31.0; falling back to: stable-1.28
[init] Using Kubernetes version: v1.28.13
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local manager] and IPs [10.96.0.1 172.31.28.213]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost manager] and IPs [172.31.28.213 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost manager] and IPs [172.31.28.213 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 5.003051 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node manager as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node manager as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 7kovac.41djmjk39wv2tz86
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.28.213:6443 --token 7kovac.41djmjk39wv2tz86 \                <---------------------<<
        --discovery-token-ca-cert-hash sha256:587aa7bdeed983db54e7011e8872cd456a37f869a55d89544b2b84c3a57b7312   
root@manager:~# 

---------------------------------------------------------------------------------------------------------------------------------------

==== COPYING KUBE/CONFIG FILE ====
## kubectl file looks for admin.conf file (/etc/kubernetes/admin.conf)
root@manager:~# kubectl get pods 
E0902 23:38:34.476226    2991 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0902 23:38:34.476459    2991 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0902 23:38:34.477711    2991 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0902 23:38:34.478044    2991 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0902 23:38:34.479725    2991 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@manager:~# 


## why kube/config file is important because, whenever you will run the kubectl command it will be looking for .kube/config file. This file contains the information about the 
## kubernetes cluster. 
root@manager:~# mkdir -p $HOME/.kube
root@manager:~# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@manager:~# chown $(id -u):$(id -g) $HOME/.kube/config

## CLUSTER AND USER INFORMATION WILL BE STORED IN THE .kube/config file which you copy it from //etc/kubernetes/admin.conf
vi .kube/config

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJUVlqT1psS09hbFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBNU1ESXlNekEyTVRaYUZ3MHpOREE0TXpFeU16RXhNVFphTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURVK1lDVFp1dkhWMDBabU9JNnVnd1NrMCthTkM4R1QzcmRIRndjMVZnNnhRMExNTVhsdjdUbndJZEcKUG4xckN0bjg4aTNEQkd5WHhadkQ4azRPRlhmVjJWZFQxOWhZdCtTY29WakRyM1k0YmJUYlFTUnltZVlmZ204dwpwYjBYbVVGRmxZL0JJTFZSVU1zbi9ZSXlxcjhlTDhDL0dWN20yblMya2x6Z3UzbEdLTUhFZmdBOUZNTFFTT3VICmVtSHdrNGxaelNQbHF6WEZYMEhZSG1LZ1JHN1V1dWI0L2FDSVNTdktrN21UZ25iR0xtaUhGclB3TGhMdlR3MXoKcHN1WkdvSWd0ellDL0xQREprUTZaN2RNa1NIK0lsVUxScWF4KzRxd0pKS0h3eHJmZTN1L0xOOUc5VlUwa3pYQgpSdlM4WmZ2eTVKZHFiT1ozbTVRSkdiMXZXU2xWQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUZzhKdzRrelhxSkZiWWFzUkJUL1pNNHEvVFV6QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRRERxM0ZJdk41UApYd3ZmWCs1VDJrLzJtME96VWpVck9lMXdDNFdXTUQ0M1ZObm5valkrbUxxNjVVR0F1bG9HTHFHZ3FNNGQ4UmNMCis3Zmx4V1AxRVpIMlFVWjJMMW9adVh6anR0Nnd6cStkNzJPZVJyUTVKUlVFZm51UTJPL2loQmFzV01PK0wzdU0KbGNjczJwSUc3Q2hUTTRoa01jUUt1ZEpZam14NThwaUVEYlRsVTFnbDlVNVZaMGg3ZUxUZDRwSzdjZFhmam9NNgp4TDhncTREbXBuWnAwNkJNNFk0NG1ZTkUvVWtlWlNPL0pIOTZoZHM2VXNZdlkrK3U0VWU2b1l3eFh3MnRTcHBzCjA2UXQzbEpCMTRSajd5aG5MOVJ4SE9MeUhIZXhWQXZWdDdhVWxQcTAvcHNDWTBtVUVYZitQQTY4RHFXcUlQdVMKdnI5Q2dLNWJLdTdYCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K  <--------CLUSTER CERTIFICATE
    server: https://172.31.28.213:6443  <--------SERVER ADDRESS WHERE YOUR SERVER IS LOCATED (MASTER NODE) 
  name: kubernetes  <--------NAME OF THER SERVER
contexts:  <--------CONTEXT IS COMBINATION OF USER AND CLUSTER
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config  <--------KIND OF A FILE IS CONFIG
preferences: {}
users:  <--------USER CERTIFICATE AND CLIENT 
- name: kubernetes-admin
  user:  
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJVGJuK0tKSlN2NU13RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBNU1ESXlNekEyTVRaYUZ3MHlOVEE1TURJeU16RXhNVGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTVWeXBCVlpkdnJuaTVsWWEKTDI2WHorbnFzZGhnVzMzUkI4RGl6MnNDckJwcnM3SnVNcFJuZEdYcVVmVHhsbDErYW5ubzBmOGtLUnk1ZnlLNgpjYVhQR0lPdlJUZVVLWFFQQzBDTU8vbXcvclprSnU1ZzBmTFNVSnVwdTRqTVRET0VleFBzR2tMbW1RMGhOM2lHClE3RFhvWnJMSGhjSG1uaExxaHhsOXptcEtSQXMvMFB3VDVIU3U0QUwrMEFObUU1ektFU0VyL0QvT3hRbDFjUkMKL0VQYlh5VkRQR24zdVdIM3RSQm9MQnJLcW9sQ3BjaUdOYnhzaFBrbmdxZVdKemVSM0VKUEhjMmo3RElHYWdPRQpQM09iRUlJVWhCbWJHNzBxUUdLeUcza2tlVmxhM2hySnhQWWJNUlVGdk1keGIzZUFUV01kYTB2T0FVRnNGc3p3ClhaZHRyUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JUZzhKdzRrelhxSkZiWWFzUkJUL1pNNHEvVApVekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBVThvRGNpVmFqS0Z2S0owL1F2VWllajBFN1BMZk9rQ1dreEFmCkFLaklFU1JpeTdBTmF1TFNEWnlhNjhGODVXNFBjK0ZJcWtSZTd1bEN1VzFJZ3V1aTk3bUtTeFJMYnIwcExMWVgKWk41d3Z6UDFkRHdlQmZ3UjJNb3NiWklVb21NaHVDOEJ1Tzc5MVdyVFQ1bXErOEgrdDdvdmlXdXdWaHNkWGhNYgpBdmxZOU5IdFZMbWVnT2VVd3FsRnVRTmwzSnNvdGJTOWFnYlo4UkFoYjc4d3hCUWNvMTJCekZROTdCVzc3NkV1CmF4TU4zK1g4UUVyVXRpYy8rL3I2eWJWNCtQR1RMUExKd2VhQzlHSlBmSTRyZE43ZVpQbTlQT3FtWDBKTWg3YW4KNmdMZ29uM3Bla3EvWDYrb25FUVQzekdMVlFtZ2NRR3YxQVRZaEY2TFlXcXdycmZreFE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
root@manager:~#


## now you will be able to run kubectl command 
root@manager:~# kubectl get pods
No resources found in default namespace.
root@manager:~#


===== APPLY POD NETWORK CNI =====
## Q which pod network we will be installing? 
## to install calico
# curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml

### instead of curl, you can use to apply the cni calico in one shot using the command below:
# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml

root@manager:~# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml
poddisruptionbudget.policy/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
serviceaccount/calico-node created
serviceaccount/calico-cni-plugin created
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created
daemonset.apps/calico-node created
deployment.apps/calico-kube-controllers created
root@manager:~#

==== KUBERNETES ARCHITECTURES RUN IN THE FORM OF POD, RUNNING IN THE NAME SPACE KUBE-SYSTEM ====
root@manager:~# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-787f445f84-6z8ts   1/1     Running   0          2m50s
calico-node-lrsjm                          1/1     Running   0          2m50s
coredns-5dd5756b68-bz9dt                   1/1     Running   0          63m
coredns-5dd5756b68-hggmg                   1/1     Running   0          63m
etcd-manager                               1/1     Running   0          63m
kube-apiserver-manager                     1/1     Running   0          63m
kube-controller-manager-manager            1/1     Running   0          63m
kube-proxy-xrslf                           1/1     Running   0          63m
kube-scheduler-manager                     1/1     Running   0          63m
root@manager:~#


==== ATTACHING WORKER NODES TO THE MASTER NODE TO FORM CLUSTER ====
## we need to add these nodes to another worker nodes to this cluster
root@manager:~# kubectl get nodes
NAME      STATUS   ROLES           AGE   VERSION
manager   Ready    control-plane   74m   v1.28.13
root@manager:~#

ubuntu@node1:~$ sudo su -
root@node1:~# kubeadm join 172.31.28.213:6443 --token 7kovac.41djmjk39wv2tz86 --discovery-token-ca-cert-hash sha256:587aa7bdeed983db54e7011e8872cd456a37f869a55d89544b2b84c3a57b7312
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@node1:~#

ubuntu@node2:~$ sudo su -
root@node2:~# kubeadm join 172.31.28.213:6443 --token 7kovac.41djmjk39wv2tz86 --discovery-token-ca-cert-hash sha256:587aa7bdeed983db54e7011e8872cd456a37f869a55d89544b2b84c3a57b7312
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@node2:~#

QQ: how communication is enabled between all nodes? 
A: through common Security group in AWS (mykube)

==== LINUX AUTOCOMPLETION TIP FOR KUBECTL COMMAND IN BASHRC ===
root@manager:~# vi .bashrc
#add this line 
source <(kubectl completion bash)
:wq!
root@manager:~# source .bashrc

==== CREATE A POD ====
## IN DOCKER 
##docker run -d --name app01 -p 8080:80 nginx:latest

## IN KUBERNETES
root@manager:~# kubectl run mypod --image nginx:latest --port=80
pod/mypod created
root@manager:~# kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
mypod   1/1     Running   0          11s
root@manager:~#

### I want to know the ip address of the pod
root@manager:~# kubectl get pod -o wide
NAME    READY   STATUS    RESTARTS   AGE    IP              NODE    NOMINATED NODE   READINESS GATES
mypod   1/1     Running   0          107s   192.168.104.1   node2   <none>           <none>
root@manager:~#

==== LOGGING INTO THE POD'S CONTAINER ====
#### IMPERATIVE THROUGH COMMAND #### 
##hostname is mypod
## impperative way of creating pods have limitations like you cannot configure multiple container in a pod, you cannot define volume configuration etc etc 
## always go for declarative way 

root@manager:~# kubectl exec -it mypod -- bash
root@mypod:/# ls
bin  boot  dev	docker-entrypoint.d  docker-entrypoint.sh  etc	home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@mypod:/#





==== DECLARATIVE THROUGH YAML ====
---- TO CREATE A YAML FILE FROM SKELETON COMMAND ----
## this way you can get the output in a yaml format and you can tweak and enhance the declarative yaml code to create your pod/deployment/services
## this gives you a skeleton for yaml file
root@manager:~# kubectl run mypod --image nginx:latest --port=80 --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: mypod
  name: mypod
spec:
  containers:
  - image: nginx:latest
    name: mypod
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@manager:~#

### REDIRECT BUT GIVES GARBAGE
root@manager:~# kubectl run mypod --image nginx:latest --port=80 --dry-run=client -o yaml > pod.yaml
root@manager:~# cat pod.yaml ###### COMMENTED ARE GARBAGE FOR NOW
apiVersion: v1   <---------- ALL THE FUNCTIONS ARE DEFINED IN V1. THIS apiVersionv1 contains so many type of kubernetes object definitions. (everything below you see in file are the part of apiversion1 function)
kind: Pod
metadata:
#  creationTimestamp: null
  labels:
    run: mypod
  name: mypod
spec:
  containers:
  - image: nginx:latest
    name: mypod
    ports:
    - containerPort: 80
    resources: {}
#  dnsPolicy: ClusterFirst
#  restartPolicy: Always
#status: {}
root@manager:~#

---- ACTUAL FILE: ----
"pod.yaml" 12L, 169B                                                                                                             6,13          All
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
  - image: nginx:latest
    ports:
    - containerPort: 80

root@manager:~# vi pod.yaml
root@manager:~# kubectl create -f pod.yaml
Error from server (AlreadyExists): error when creating "pod.yaml": pods "mypod" already exists
root@manager:~#

root@manager:~# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
mypod   1/1     Running   0          3h
root@manager:~#

apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
~

root@manager:~# kubectl create -f pod.yaml
pod/newpod created
root@manager:~# kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
mypod    1/1     Running   0          3h1m
newpod   1/1     Running   0          7s
root@manager:~#

==== TO DELETE THE POD ====
root@manager:~# kubectl delete pod mypod
pod "mypod" deleted

### but I want to delete it through my yaml file 
### I want to delete all the objects that are defined in the pod.yaml

root@manager:~# kubectl delete -f pod.yaml
pod "newpod" deleted
root@manager:~#

root@manager:~# kubectl get pods
No resources found in default namespace.
root@manager:~#

==== CREATING THE PODS FROM POD.YAML ==== 

root@manager:~# kubectl create -f pod.yaml
pod/newpod created
root@manager:~# kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
newpod   1/1     Running   0          6s
root@manager:~#

==== TO KNOW WHAT IS HAPPENING UNDER THE HUD TO CREATE A OBJECTS (DESCRIBE COMMAND). TO KNOW WHAT EXACTLY IS KUBERNETES IS DOING TO CREATE AN OBJECT ====
root@manager:~# kubectl describe pod newpod
Name:             newpod
Namespace:        default
Priority:         0
Service Account:  default
Node:             node2/172.31.28.35
Start Time:       Tue, 03 Sep 2024 04:38:17 +0000
Labels:           app=myapp
Annotations:      cni.projectcalico.org/containerID: 046ffe57cd70fbde9656e259c4bf8c4227f0675dc7df879edd3077f0bc8b3b84
                  cni.projectcalico.org/podIP: 192.168.104.2/32
                  cni.projectcalico.org/podIPs: 192.168.104.2/32
Status:           Running
IP:               192.168.104.2
IPs:
  IP:  192.168.104.2
Containers:
  con1:
    Container ID:   cri-o://b48aee57c2e42c34ab6827244113eb6e5ca5b816fea4059df96753a40756bf9d
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:447a8665cc1dab95b1ca778e162215839ccbb9189104c79d7ec3a81e14577add
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Sep 2024 04:38:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dk75r (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-dk75r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  68s   default-scheduler  Successfully assigned default/newpod to node2
  Normal  Pulling    67s   kubelet            Pulling image "nginx:latest"
  Normal  Pulled     67s   kubelet            Successfully pulled image "nginx:latest" in 128ms (128ms including waiting)
  Normal  Created    67s   kubelet            Created container con1
  Normal  Started    67s   kubelet            Started container con1
root@manager:~#
----------------------- CONTAINER DESCRIBE ENDS -----------------------

==== CREATING MULTIPLE CONTAINERS ====
root@manager:~# mkdir app
root@manager:~# cp pod.yaml app/
root@manager:~# cd app/
root@manager:~/app# cp pod.yaml multi-con.yaml


==== NODEPORT SERVICE AND CREATING YAML FILE WITH DRY RUN ====
root@manager:~/app# kubectl expose pod newpod --type NodePort  --dry-run=client -o yaml
## GARBAGE SCRIPT DRAFT
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: myapp
  name: newpod
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: myapp
  type: NodePort
status:
  loadBalancer: {}

## redirecting to a yaml file
root@manager:~/app# kubectl expose pod newpod --type NodePort  --dry-run=client -o yaml > pod-svc.yaml

----FIXED----
apiVersion: v1
kind: Service
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80



root@manager:~/app# vim pod-svc.yaml
root@manager:~/app# kubectl create -f pod-svc.yaml
service/newpod-svc created

root@manager:~/app# kubectl get service
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        17h
newpod-svc   NodePort    10.109.216.200   <none>        80:31507/TCP   40s <------RANDOM PORT WILL BE ASSIGNED #LOOK NEXT TO 80:

## you can define the port in your yaml if you want a fixed port to map the pod port to 
## nodePort IP range: 30000 - 32767

root@manager:~/app# vim pod-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: newpod-svc
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30123 <------------------------FIXED PORT MAPPING

root@manager:~/app# kubectl create -f pod-svc.yaml
service/newpod-svc created
root@manager:~/app# kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
newpod   1/1     Running   1          12h
root@manager:~/app# kubectl get service
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        18h
newpod-svc   NodePort    10.103.17.74   <none>        80:30123/TCP   25s

==== ACCESSING APPLICATION OUTSIDE OF THE CLUSTER ==== 
Get the public ip address of node1 from aws
54.xxx.xxx.xxx:30123 ## <--- paste it in the browser
you need to enable the port 30123 in the security group from aws for inbound or you can define the whole range for kubernetes service from 30000 - 32767
1. go to security group
2. edit inbound rule
 a. add rule
 b. in port range add 30000 - 32767
save the rule

Welcome to nginx!
If you see this page, the nginx web server is successfully installed and working. Further configuration is required.


==== MULTI CONTAINER POD ==== 

root@manager:~/app# vi multi-con.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mycon-pod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080
~


root@manager:~/app# kubectl create -f multi-con.yaml
pod/mycon-pod created

root@manager:~/app# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
mycon-pod   2/2     Running   0          30s
newpod      1/1     Running   1          13h

root@manager:~/app# kubectl describe pod mycon-pod
Name:             mycon-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             node1/172.31.19.148
Start Time:       Tue, 03 Sep 2024 17:48:48 +0000
Labels:           app=myapp
Annotations:      cni.projectcalico.org/containerID: 74f3710055e462c080626810f99e63118a4c7f674ac27dcc8ce3d8e58d0fa4ba
                  cni.projectcalico.org/podIP: 192.168.166.130/32
                  cni.projectcalico.org/podIPs: 192.168.166.130/32
Status:           Running
IP:               192.168.166.130
IPs:
  IP:  192.168.166.130
Containers:
  con1:
    Container ID:   cri-o://be0ed2fb8867565d4842d5e99608bcc6a029a0d879c33bac234ab23ded6ff8f4
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:447a8665cc1dab95b1ca778e162215839ccbb9189104c79d7ec3a81e14577add
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Sep 2024 17:48:49 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jf7jk (ro)
  con2:
    Container ID:   cri-o://3e55f468258a0242037d6c581301552088a0b7d192e6a52ce5c706e9453ee204
    Image:          tomcat:latest
    Image ID:       docker.io/library/tomcat@sha256:221f95c46bbd428f2c7bce68ecc421fb68f21844f3e9e0f15cf5e7d955929b1a
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Sep 2024 17:48:56 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jf7jk (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-jf7jk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  107s  default-scheduler  Successfully assigned default/mycon-pod to node1
  Normal  Pulling    106s  kubelet            Pulling image "nginx:latest"
  Normal  Pulled     106s  kubelet            Successfully pulled image "nginx:latest" in 279ms (279ms including waiting)
  Normal  Created    106s  kubelet            Created container con1
  Normal  Started    106s  kubelet            Started container con1
  Normal  Pulling    106s  kubelet            Pulling image "tomcat:latest"
  Normal  Pulled     99s   kubelet            Successfully pulled image "tomcat:latest" in 6.835s (6.835s including waiting)
  Normal  Created    99s   kubelet            Created container con2
  Normal  Started    99s   kubelet            Started container con2

==== LOGGING INTO CONTAINER 1 and 2 FROM MYCON-POD ON NODE1 ====

root@manager:~/app# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
mycon-pod   2/2     Running   0          3m59s
newpod      1/1     Running   1          13h
root@manager:~/app# kubectl exec -it mycon-pod -c con1 -- bash
root@mycon-pod:/# ls
bin   dev		   docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc			 lib   media  opt  root  sbin  sys  usr
exit

root@manager:~/app# kubectl exec -it mycon-pod -c con2 -- bash
root@mycon-pod:/usr/local/tomcat# ls
bin           conf             lib      logs            NOTICE     RELEASE-NOTES  temp     webapps.dist
BUILDING.txt  CONTRIBUTING.md  LICENSE  native-jni-lib  README.md  RUNNING.txt    webapps  work
root@mycon-pod:/usr/local/tomcat#
exit
==== TROUBLESHOOTING TO READ THE LOGS OF THE CONTAINER ====

root@manager:~/app# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
mycon-pod   2/2     Running   0          7m50s
newpod      1/1     Running   1          13h

root@manager:~/app# kubectl logs newpod
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/09/03 16:22:37 [notice] 1#1: using the "epoll" event method
2024/09/03 16:22:37 [notice] 1#1: nginx/1.27.1
2024/09/03 16:22:37 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)
2024/09/03 16:22:37 [notice] 1#1: OS: Linux 6.8.0-1012-aws
2024/09/03 16:22:37 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/09/03 16:22:37 [notice] 1#1: start worker processes
2024/09/03 16:22:37 [notice] 1#1: start worker process 28
2024/09/03 16:22:37 [notice] 1#1: start worker process 29
192.168.166.128 - - [03/Sep/2024:17:25:08 +0000] "GET / HTTP/1.1" 200 615 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15" "-"
2024/09/03 17:25:08 [error] 28#28: *1 open() "/usr/share/nginx/html/favicon.ico" failed (2: No such file or directory), client: 192.168.166.128, server: localhost, request: "GET /favicon.ico HTTP/1.1", host: "54.226.206.164:30123", referrer: "http://54.226.206.164:30123/"
192.168.166.128 - - [03/Sep/2024:17:25:08 +0000] "GET /favicon.ico HTTP/1.1" 404 153 "http://54.226.206.164:30123/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15" "-"
192.168.166.128 - - [03/Sep/2024:17:34:00 +0000] "GET /apple-touch-icon-precomposed.png HTTP/1.1" 404 153 "-" "Safari/19618.3.11.11.5 CFNetwork/1498.700.2 Darwin/23.6.0" "-"
2024/09/03 17:34:00 [error] 28#28: *4 open() "/usr/share/nginx/html/apple-touch-icon-precomposed.png" failed (2: No such file or directory), client: 192.168.166.128, server: localhost, request: "GET /apple-touch-icon-precomposed.png HTTP/1.1", host: "54.226.206.164:30123"
2024/09/03 17:34:01 [error] 28#28: *4 open() "/usr/share/nginx/html/apple-touch-icon.png" failed (2: No such file or directory), client: 192.168.166.128, server: localhost, request: "GET /apple-touch-icon.png HTTP/1.1", host: "54.226.206.164:30123"
192.168.166.128 - - [03/Sep/2024:17:34:01 +0000] "GET /apple-touch-icon.png HTTP/1.1" 404 153 "-" "Safari/19618.3.11.11.5 CFNetwork/1498.700.2 Darwin/23.6.0" "-"

==== IF YOU HAVE MULTIPLE CONTAINERS IN THE POD AND YOU WANT TO READ THE LOG OF EACH CONTAINER ==== 

root@manager:~/app# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
mycon-pod   2/2     Running   0          7m50s
newpod      1/1     Running   1          13h

---- LOGS FOR THE CONTAINER 1
root@manager:~/app# kubectl logs mycon-pod -c con1
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/09/03 17:48:49 [notice] 1#1: using the "epoll" event method
2024/09/03 17:48:49 [notice] 1#1: nginx/1.27.1
2024/09/03 17:48:49 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)
2024/09/03 17:48:49 [notice] 1#1: OS: Linux 6.8.0-1012-aws
2024/09/03 17:48:49 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/09/03 17:48:49 [notice] 1#1: start worker processes
2024/09/03 17:48:49 [notice] 1#1: start worker process 28
2024/09/03 17:48:49 [notice] 1#1: start worker process 29

---- LOGS FOR CONTAINER 2 
root@manager:~/app# kubectl logs -f mycon-pod -c con2 <-------  -f is to stream the logs
03-Sep-2024 17:48:56.810 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/10.1.28
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          Aug 2 2024 15:14:43 UTC
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 10.1.28.0
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Name:               Linux
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Version:            6.8.0-1012-aws
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Architecture:          amd64
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Java Home:             /opt/java/openjdk
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Version:           21.0.4+7-LTS
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Vendor:            Eclipse Adoptium
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_BASE:         /usr/local/tomcat
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_HOME:         /usr/local/tomcat
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize=2048
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs=org.apache.catalina.webresources
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dorg.apache.catalina.security.SecurityListener.UMASK=0027
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.lang=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.io=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.util=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.base=/usr/local/tomcat
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.home=/usr/local/tomcat
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.io.tmpdir=/usr/local/tomcat/temp
03-Sep-2024 17:48:56.838 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent Loaded Apache Tomcat Native library [2.0.8] using APR version [1.7.2].
03-Sep-2024 17:48:56.841 INFO [main] org.apache.catalina.core.AprLifecycleListener.initializeSSL OpenSSL successfully initialized [OpenSSL 3.0.13 30 Jan 2024]
03-Sep-2024 17:48:57.183 INFO [main] org.apache.coyote.AbstractProtocol.init Initializing ProtocolHandler ["http-nio-8080"]
03-Sep-2024 17:48:57.202 INFO [main] org.apache.catalina.startup.Catalina.load Server initialization in [570] milliseconds
03-Sep-2024 17:48:57.240 INFO [main] org.apache.catalina.core.StandardService.startInternal Starting service [Catalina]
03-Sep-2024 17:48:57.241 INFO [main] org.apache.catalina.core.StandardEngine.startInternal Starting Servlet engine: [Apache Tomcat/10.1.28]
03-Sep-2024 17:48:57.261 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
03-Sep-2024 17:48:57.275 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [72] milliseconds


===== COMMAND TO STREAM THE LOGS =====
2024/09/03 17:48:49 [notice] 1#1: start worker process 29
root@manager:~/app# kubectl logs mycon-pod -c con2
03-Sep-2024 17:48:56.810 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/10.1.28
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          Aug 2 2024 15:14:43 UTC
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 10.1.28.0
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Name:               Linux
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Version:            6.8.0-1012-aws
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Architecture:          amd64
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Java Home:             /opt/java/openjdk
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Version:           21.0.4+7-LTS
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Vendor:            Eclipse Adoptium
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_BASE:         /usr/local/tomcat
03-Sep-2024 17:48:56.818 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_HOME:         /usr/local/tomcat
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize=2048
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs=org.apache.catalina.webresources
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dorg.apache.catalina.security.SecurityListener.UMASK=0027
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.lang=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.io=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.util=ALL-UNNAMED
03-Sep-2024 17:48:56.832 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.base=/usr/local/tomcat
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.home=/usr/local/tomcat
03-Sep-2024 17:48:56.833 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.io.tmpdir=/usr/local/tomcat/temp
03-Sep-2024 17:48:56.838 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent Loaded Apache Tomcat Native library [2.0.8] using APR version [1.7.2].
03-Sep-2024 17:48:56.841 INFO [main] org.apache.catalina.core.AprLifecycleListener.initializeSSL OpenSSL successfully initialized [OpenSSL 3.0.13 30 Jan 2024]
03-Sep-2024 17:48:57.183 INFO [main] org.apache.coyote.AbstractProtocol.init Initializing ProtocolHandler ["http-nio-8080"]
03-Sep-2024 17:48:57.202 INFO [main] org.apache.catalina.startup.Catalina.load Server initialization in [570] milliseconds
03-Sep-2024 17:48:57.240 INFO [main] org.apache.catalina.core.StandardService.startInternal Starting service [Catalina]
03-Sep-2024 17:48:57.241 INFO [main] org.apache.catalina.core.StandardEngine.startInternal Starting Servlet engine: [Apache Tomcat/10.1.28]
03-Sep-2024 17:48:57.261 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
03-Sep-2024 17:48:57.275 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [72] milliseconds

press ctrl -c to come out of it
