# previously we learned
1. NodeName: Scheduling based on nodename in our yaml file to deploy pod in particular node
2. NodeSelector: this was based on node labels 

NODE AFFINITY --- based on the node label
POD AFFINITY --- based on the pod label

# today we will learn about 
3. Affinity - based on node labels
two types of Affinities:
  a. RequiredDuringScheduling - means if no node label assigned on anynode, it will go in pending status
  b. PreferredDuringScheduling - prefer a node that is having a label. If there is no node with any label, then go deploy in any node. It will not go into the pending status.
  c. requiredDuringSchedulingIgnoredDuringExecution - even it is prefered during scheduling, which is having our required label get deployed to the node. If the label is deleted/changed, just ignore and continue your execution wherever the pod was deployed
4. Anti-Affinity
Anti-affinity have similar two sub-rules of it:
  a. RequiredDuringScheduling - strict rule forcefully applying the rule: dont go and deploy in a node which is having defined label. If all the nodes are having labels, then go in pending status.
  b. PreferredDuringScheduling - ok prefer not to go in a node which is having label.

# copy the Syntax from the documentation
# use the below
https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
# ignore below (only for reading purpose)
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

### we are looking at the node affinity, we can look at the pod affinity as well
nano aff-req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu
            operator: In  #if you change this to NotIn that will not deploy the pod in the node which will have label as cpu=ryzen
            values:
            - ryzen

################# NODE AFFINITY BEGINS #################
root@master:~/app/scheduling# nano aff-req.yaml
root@master:~/app/scheduling# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nodesel   1/1     Running   0          138m
root@master:~/app/scheduling# kubectl delete po nodesel
pod "nodesel" deleted
root@master:~/app/scheduling# kubectl create -f aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          9s    172.17.166.138   node1   <none>           <none>
root@master:~/app/scheduling#

# why it got deployed to the node1? because node1 has the label cpu=ryzen
root@master:~/app/scheduling# kubectl get node --show-labels|grep "cpu=ryzen"
node1    Ready    <none>          34d   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=ryzen,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
root@master:~/app/scheduling#

### let's remove the label
root@master:~/app/scheduling# kubectl label node node1 cpu-
node/node1 unlabeled
root@master:~/app/scheduling#
### it is still running, because of the (requiredDuringSchedulingIgnoredDuringExecution:)
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          2m21s   172.17.166.138   node1   <none>           <none>
root@master:~/app/scheduling#

### now if you delete the pod and re-deploy it will go into pending state, because our node is forcefully requiredDuringScheduling to required label and node1 hasnt been labeled as cpu=ryzen
root@master:~/app/scheduling# kubectl create -f aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
newpod   0/1     Pending   0          4s    <none>   <none>   <none>           <none>
root@master:~/app/scheduling#

### as soon as the label cpu=ryzen gets assinged on the node1, pending status will go-away and the node1 will be assigned
root@master:~/app/scheduling# kubectl label node node1 cpu=ryzen
node/node1 labeled
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          2m53s   172.17.166.139   node1   <none>           <none>
root@master:~/app/scheduling#

======  PREFERRED DURING SCHEDULING ======
root@master:~/app/scheduling# nano preferredduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: cpu
            operator: In #to change it into Anti-affinity just add NotIn e.g operator: NotIn
            values:
            - ryzen

root@master:~/app/scheduling# kubectl create -f preferredduringscheduling.yaml
pod/newpod created
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          9m33s   172.17.166.140   node1   <none>           <none>
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl delete -f preferredduringscheduling.yaml
pod "newpod" deleted
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl create -f preferredduringscheduling.yaml
pod/newpod created

### adding anti-affinity in preferredduringscheduling.yaml with (operator: NotIn) the pod will be deployed in the node2, because the node2 doesnt have the label cpu=ryzen tagged to it
root@master:~/app/scheduling# kubectl get pod -o wide
NAME     READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATES
newpod   0/1     ContainerCreating   0          8s    <none>   node2   <none>           <none>
root@master:~/app/scheduling#



################# NODE AFFINITY ENDS #################
# imagine all the nodes are having the label of cpu=ryzen, then your pod will be deployed any node. It will not keep the pod into the pending status. 


################# NODE ANTI-AFFINITY BEGINS #################
Q: what is Anti-Affinity? 
Dont go and get deployed that has a label of cpu=ryzen

root@master:~/app/scheduling# nano anti-aff-req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu
            operator: NotIn
            values:
            - ryzen

root@master:~/app/scheduling# 

### notice that the pod has been deployed into the node2, because in operator section we have specified (operator: NotIn) not to be deployed in a node with the key=pair value of cpu=ryzen
### so kubernetes scheduler will look for the node that doesnt have the label with cpu=ryzen and gets deployed in node2 because we havent tagged that label on node2
root@master:~/app/scheduling# kubectl create -f anti-aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pod -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          16s   172.17.104.12   node2   <none>           <none>
root@master:~/app/scheduling#


################# NODE ANTI-AFFINITY ENDS #################

################# POD AFFINITY BEGINS #################
### pod affinity is similar to the node affinity, the only thing that you will find different is the affinity is getting applied on the pod, based on the pod label.

################# POD AFFINITY ENDS #################
