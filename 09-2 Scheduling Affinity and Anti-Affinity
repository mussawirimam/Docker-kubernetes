NODE AFFINITY AND ANTI AFFINITY DOC: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
POD AFFINITY AND ANTI AFFINITY DOC (ctrl + f: pod affinity) :   https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity

# previously we learned
1. NodeName: Scheduling based on nodename in our yaml file to deploy pod in particular node
2. NodeSelector: this was based on node labels 

NODE AFFINITY --- based on the node label
POD AFFINITY --- based on the pod label

# today we will learn about 
3. Affinity - based on node labels
two types of Affinities:
  a. RequiredDuringScheduling - means if no node label assigned on anynode, it will go in pending status
  b. PreferredDuringScheduling - prefer a node that is having a label. If there is no node with any label, then go deploy in any node. It will not go into the pending status.
  c. requiredDuringSchedulingIgnoredDuringExecution - even it is prefered during scheduling, which is having our required label get deployed to the node. If the label is deleted/changed, just ignore and continue your execution wherever the pod was deployed
4. Anti-Affinity
Anti-affinity have similar two sub-rules of it:
  a. RequiredDuringScheduling - strict rule forcefully applying the rule: dont go and deploy in a node which is having defined label. If all the nodes are having labels, then go in pending status.
  b. PreferredDuringScheduling - ok prefer not to go in a node which is having label.

# copy the Syntax from the documentation
# use the below
https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
# ignore below (only for reading purpose)
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

### we are looking at the node affinity, we can look at the pod affinity as well
nano aff-req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu
            operator: In  #if you change this to NotIn that will not deploy the pod in the node which will have label as cpu=ryzen
            values:
            - ryzen

################# NODE AFFINITY BEGINS #################
root@master:~/app/scheduling# nano aff-req.yaml
root@master:~/app/scheduling# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nodesel   1/1     Running   0          138m
root@master:~/app/scheduling# kubectl delete po nodesel
pod "nodesel" deleted
root@master:~/app/scheduling# kubectl create -f aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          9s    172.17.166.138   node1   <none>           <none>
root@master:~/app/scheduling#

# why it got deployed to the node1? because node1 has the label cpu=ryzen
root@master:~/app/scheduling# kubectl get node --show-labels|grep "cpu=ryzen"
node1    Ready    <none>          34d   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=ryzen,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
root@master:~/app/scheduling#

### let's remove the label
root@master:~/app/scheduling# kubectl label node node1 cpu-
node/node1 unlabeled
root@master:~/app/scheduling#
### it is still running, because of the (requiredDuringSchedulingIgnoredDuringExecution:)
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          2m21s   172.17.166.138   node1   <none>           <none>
root@master:~/app/scheduling#

### now if you delete the pod and re-deploy it will go into pending state, because our node is forcefully requiredDuringScheduling to required label and node1 hasnt been labeled as cpu=ryzen
root@master:~/app/scheduling# kubectl create -f aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
newpod   0/1     Pending   0          4s    <none>   <none>   <none>           <none>
root@master:~/app/scheduling#

### as soon as the label cpu=ryzen gets assinged on the node1, pending status will go-away and the node1 will be assigned
root@master:~/app/scheduling# kubectl label node node1 cpu=ryzen
node/node1 labeled
root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          2m53s   172.17.166.139   node1   <none>           <none>
root@master:~/app/scheduling#

======  PREFERRED DURING SCHEDULING ======
root@master:~/app/scheduling# nano preferredduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: cpu
            operator: In #to change it into Anti-affinity just add NotIn e.g operator: NotIn
            values:
            - ryzen

root@master:~/app/scheduling# kubectl create -f preferredduringscheduling.yaml
pod/newpod created
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl get pods -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          9m33s   172.17.166.140   node1   <none>           <none>
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl delete -f preferredduringscheduling.yaml
pod "newpod" deleted
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl create -f preferredduringscheduling.yaml
pod/newpod created

### adding anti-affinity in preferredduringscheduling.yaml with (operator: NotIn) the pod will be deployed in the node2, because the node2 doesnt have the label cpu=ryzen tagged to it
root@master:~/app/scheduling# kubectl get pod -o wide
NAME     READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATES
newpod   0/1     ContainerCreating   0          8s    <none>   node2   <none>           <none>
root@master:~/app/scheduling#



################# NODE AFFINITY ENDS #################
# imagine all the nodes are having the label of cpu=ryzen, then your pod will be deployed any node. It will not keep the pod into the pending status. 


################# NODE ANTI-AFFINITY BEGINS #################
Q: what is Anti-Affinity? 
Dont go and get deployed that has a label of cpu=ryzen

root@master:~/app/scheduling# nano anti-aff-req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu
            operator: NotIn
            values:
            - ryzen

root@master:~/app/scheduling# 

### notice that the pod has been deployed into the node2, because in operator section we have specified (operator: NotIn) not to be deployed in a node with the key=pair value of cpu=ryzen
### so kubernetes scheduler will look for the node that doesnt have the label with cpu=ryzen and gets deployed in node2 because we havent tagged that label on node2
root@master:~/app/scheduling# kubectl create -f anti-aff-req.yaml
pod/newpod created
root@master:~/app/scheduling# kubectl get pod -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
newpod   1/1     Running   0          16s   172.17.104.12   node2   <none>           <none>
root@master:~/app/scheduling#


################# NODE ANTI-AFFINITY ENDS #################

################# POD AFFINITY BEGINS #################
### pod affinity is similar to the node affinity, the only thing that you will find different is the affinity is getting applied on the pod, based on the pod label.
# create a pod first
nano pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mango-pod
  labels:
    app: mango
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80

root@master:~/app/scheduling# kubectl create -f pod.yaml
pod/mango-pod created

root@master:~/app/scheduling# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
mango-pod   1/1     Running   0          13s
root@master:~/app/scheduling#

############# POD AFFINITY REQUIRED DURING SCHEDULING ############# 
nano pod-aff-requiredduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: podaff-app
  labels:
    app: mango
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - mango
        topologyKey: kubernetes.io/hostname # based on the hostname, you want to search the node based on the availability zones (you can define it in here) (original statement: topology.kubernetes.io/zone)
### you can see the kuberenetes.io/hostname will be added inside the topologyKey 
root@master:~/app/scheduling# kubectl get nodes --show-labels
NAME     STATUS   ROLES           AGE   VERSION    LABELS
master   Ready    control-plane   37d   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
node1    Ready    <none>          36d   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=ryzen,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
node2    Ready    <none>          34d   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux
root@master:~/app/scheduling#

### where this pod will get deployed? in a node which is already having a pod with the label of app=mango

root@master:~/app/scheduling# kubectl get pod -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mango   1/1     Running   0          37m   172.17.166.141   node1   <none>           <none>

root@master:~/app/scheduling# kubectl get pod --show-labels
NAME    READY   STATUS    RESTARTS   AGE   LABELS
mango   1/1     Running   0          38m   app=mango
root@master:~/app/scheduling#

root@master:~/app/scheduling# nano pod-aff-requiredduringscheduling.yaml
root@master:~/app/scheduling# kubectl create -f pod-aff-requiredduringscheduling.yaml
pod/podaff-app created
root@master:~/app/scheduling#

### as you see the podaff-app is deployed to the node1, because mango-pod had the label app=mango and it was deployed in node1
### and that is why podaff-app has the affinity rule applied to it so it finds the label with app=mango in which node it is deployed in,
### it will associate that node for the podaff-app
root@master:~/app/scheduling# kubectl get pod -o wide
NAME         READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mango-pod    1/1     Running   0          6m43s   172.17.166.142   node1   <none>           <none>
podaff-app   1/1     Running   0          3s      172.17.166.143   node1   <none>           <none>

### affinity required during scheduling
root@master:~/app/scheduling# kubectl get pod --show-labels
NAME         READY   STATUS    RESTARTS   AGE    LABELS
mango-pod    1/1     Running   0          10m    app=mango
podaff-app   1/1     Running   0          4m8s   app=mango

### to make the Anti-Affinity at pod level, dont change the operator
root@master:~/app/scheduling# nano pod-anti-aff-requiredduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: podaff-app
  labels:
    app: mango
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    podAntiAffinity: #Affnity and Anti-Affinity applies here
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - mango
        topologyKey: kubernetes.io/hostname

root@master:~/app/scheduling# kubectl create -f pod-anti-aff-requiredduringscheduling.yaml
pod/podaff-app created

### now the pod is deployed in the node, because anti-affinity has been applied which states that find the pod with label app=mango and dont deploy in the same node; hence, the pod was deployed in node2
root@master:~/app/scheduling# kkgp -o wide
NAME         READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mango-pod    1/1     Running   0          17m     172.17.166.142   node1   <none>           <none>
podaff-app   1/1     Running   0          2m34s   172.17.104.14    node2   <none>           <none>
root@master:~/app/scheduling# kkgp --show-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
mango-pod    1/1     Running   0          17m     app=mango
podaff-app   1/1     Running   0          2m44s   app=mango

root@master:~/app/scheduling# kk delete -f pod-anti-aff-requiredduringscheduling.yaml
pod "podaff-app" deleted
root@master:~/app/scheduling#

######### POD AFFINITY PREFERRED DURINGS SHEDULING #########
root@master:~/app/scheduling# nano pod-aff-preferredduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: podaff-pref-pod
  labels:
    app: mango
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100 # greater or lower level significance
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - mango
          topologyKey: kubernetes.io/hostname

### this pod will be preferring a node which is having defined label as app=mango, if there is no pod running with the label app=mango, it will go to any node.
root@master:~/app/scheduling# kubectl create -f pod-aff-preferredduringscheduling.yaml
pod/podaff-pref-pod created

### the pod was deployed on the node1, because directions in yaml were to prefer the pod with label app=mango, which is deployed in node1. So, the podaff-pref-pod will be deployed in the node1 as well
root@master:~/app/scheduling# kubectl get pod -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mango-pod         1/1     Running   0          38m   172.17.166.142   node1   <none>           <none>
podaff-pref-pod   1/1     Running   0          9s    172.17.166.144   node1   <none>           <none>
root@master:~/app/scheduling#

root@master:~/app/scheduling# kubectl delete -f pod-aff-preferredduringscheduling.yaml
pod "podaff-pref-pod" deleted
root@master:~/app/scheduling#

### POD ANTI PREFERRED DURING SCHEDULING AFFINITY ###
root@master:~/app/scheduling# nano pod-anti-aff-preferedduringscheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: podaff-pref-pod
  labels:
    app: mango
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    podAntiAffinity: # for affinity use PodAffinity here...
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100 # greater or lower level significance
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - mango
          topologyKey: kubernetes.io/hostname

### now the preference is to not add the podaff-pref-pod to node1, because yaml direction states in podAntiAffinity to not to deploy the pod in a node with which the pod with label app=mango is already present.
### that is why it is deployed in the node2
root@master:~/app/scheduling# kubectl create -f pod-anti-aff-preferedduringscheduling.yaml
pod/podaff-pref-pod created
root@master:~/app/scheduling# kubectl get pod -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mango-pod         1/1     Running   0          47m   172.17.166.142   node1   <none>           <none>
podaff-pref-pod   1/1     Running   0          6s    172.17.104.16    node2   <none>           <none>
root@master:~/app/scheduling#

################# POD AFFINITY ENDS #################
