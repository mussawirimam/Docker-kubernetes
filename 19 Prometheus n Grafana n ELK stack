# first we will learn about the Prometheus and Grafana and then we will learn about the ELK stack
### in order for you to install the prometheus or any monitoring solution you need to have the METRIC SERVER CONTAINER INSTLLED ON THE KUBERNETES
### to install and configure the metric server, go to the previous notes. 
1. first you will install the metric server from the github
2. you will add the kubelet insecure tls flag in the component file for the metric server

# root@master:~# kubectl get --raw /metrics # this will give you lots of logs on your screen


# Installing metric server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl get deployment metrics-server -n kube-system
kubectl get --raw /metrics

# Prometheus deployment
kubectl create namespace prometheus
kubectl get ns

#install local pvc 
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml


helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

helm install prometheus prometheus-community/prometheus \
    --namespace prometheus \
    --set alertmanager.persistentVolume.storageClass="local-path" \
    --set server.persistentVolume.storageClass="local-path"

kubectl get all -n prometheus

### change prometheus-server service to NodePort Service 
kubectl -n prometheus edit svc prometheus-server

# Installing Grafana
kubectl create namespace grafana
kubectl get ns

helm repo add grafana https://grafana.github.io/helm-charts

## values yaml file
vi grafana.yaml
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.prometheus.svc.cluster.local
      access: proxy
      isDefault: true
 

helm install grafana grafana/grafana \
    --namespace grafana \
    --set persistence.storageClassName="local-path" \
    --set persistence.enabled=true \
    --set adminPassword='admin' \
    --values grafana.yaml \
    --set service.type=NodePort

kubectl get all -n grafana

#Get ELB address
export ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "http://$ELB"

# login usr= "admin"
# Passwd: get password
kubectl get secret --namespace grafana grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

Grfana Dashboard Id

Cluster Monitoring = 15757
Namespace monitoring = 15758
Node Monitoring = 15759 , 1860
Pod level monitoring 5225
PVC usage - 13646



# Cleanup

helm uninstall prometheus --namespace prometheus
kubectl delete ns prometheus

helm uninstall grafana --namespace grafana
kubectl delete ns grafana


------------------------------------------------------------------------
########################### PROMETHEUS ########################### 

root@master:/usr/bin/helm-3.17.0/scripts# helm repo ls
NAME                    URL
bitnami                 https://charts.bitnami.com/bitnami
prometheus-community    https://prometheus-community.github.io/helm-charts
root@master:/usr/bin/helm-3.17.0/scripts#
root@master:/usr/bin/helm-3.17.0/scripts#
root@master:/usr/bin/helm-3.17.0/scripts#
root@master:/usr/bin/helm-3.17.0/scripts# helm repo update prometheus-community
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. ⎈Happy Helming!⎈
root@master:/usr/bin/helm-3.17.0/scripts#


root@master:~# kubectl get all -n prometheus
NAME                                                    READY   STATUS    RESTARTS   AGE
pod/prometheus-alertmanager-0                           0/1     Pending   0          112m
pod/prometheus-kube-state-metrics-586c84f44c-lx8lv      1/1     Running   0          112m
pod/prometheus-prometheus-node-exporter-b8d85           1/1     Running   0          112m
pod/prometheus-prometheus-node-exporter-gw7tv           1/1     Running   0          112m
pod/prometheus-prometheus-pushgateway-d777b5496-fhw56   1/1     Running   0          112m
pod/prometheus-server-575bcfd8db-hrbrp                  2/2     Running   0          112m

NAME                                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/prometheus-alertmanager               ClusterIP   10.108.46.3      <none>        9093/TCP   112m
service/prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   112m
service/prometheus-kube-state-metrics         ClusterIP   10.111.19.231    <none>        8080/TCP   112m
service/prometheus-prometheus-node-exporter   ClusterIP   10.98.66.123     <none>        9100/TCP   112m
service/prometheus-prometheus-pushgateway     ClusterIP   10.100.95.166    <none>        9091/TCP   112m
service/prometheus-server                     ClusterIP   10.102.236.175   <none>        80/TCP     112m

NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/prometheus-prometheus-node-exporter   2         2         2       2            2           kubernetes.io/os=linux   112m

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-kube-state-metrics       1/1     1            1           112m
deployment.apps/prometheus-prometheus-pushgateway   1/1     1            1           112m
deployment.apps/prometheus-server                   1/1     1            1           112m

NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-kube-state-metrics-586c84f44c      1         1         1       112m
replicaset.apps/prometheus-prometheus-pushgateway-d777b5496   1         1         1       112m
replicaset.apps/prometheus-server-575bcfd8db                  1         1         1       112m

NAME                                       READY   AGE
statefulset.apps/prometheus-alertmanager   0/1     112m
root@master:~#

root@master:~# kubectl -n prometheus edit service/prometheus-server
...
app.kubernetes.io/component: server
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/name: prometheus
  sessionAffinity: None
  type: NodePort          <--------------------- changed this from ClusterIp to NodePort
status:
  loadBalancer: {}
...
service/prometheus-server edited

service/prometheus-prometheus-node-exporter   ClusterIP   10.98.66.123     <none>        9100/TCP       115m
service/prometheus-prometheus-pushgateway     ClusterIP   10.100.95.166    <none>        9091/TCP       115m
service/prometheus-server                     NodePort    10.102.236.175   <none>        80:30363/TCP   115m

==== ON BROWSER ====
### put this one your web- browser to get the access of the Prometheus dashboard.
http://18.208.215.221:30363/query

# on the prometheus dashboard, you can search for the server metrics. 

### Turn the nodePort back again to the Cluster IP so it can access the cluster internally. 
root@master:~# kubectl -n prometheus edit service/prometheus-server
...
app.kubernetes.io/instance: prometheus
    app.kubernetes.io/name: prometheus
  sessionAffinity: None
  type: ClusterIP     <------------------------------ This was changed from the NodePort to ClusterIP
status:
  loadBalancer: {}
...

service/prometheus-server edited
root@master:~#

########################### GRAFANA ########################### 

### installing grafana
root@master:~# kubectl create namespace grafana
namespace/grafana created

root@master:~# kubectl get ns
NAME                 STATUS   AGE
argocd               Active   2d3h
default              Active   4d3h
grafana              Active   100s
kube-node-lease      Active   4d3h
kube-public          Active   4d3h
kube-system          Active   4d3h
local-path-storage   Active   28h
prometheus           Active   28h
root@master:~#

### add the grafana repo 
root@master:~# helm repo add grafana https://grafana.github.io/helm-charts
"grafana" has been added to your repositories
root@master:~#

### update the grafana repo 
root@master:~# helm repo update grafana
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "grafana" chart repository
Update Complete. ⎈Happy Helming!⎈
root@master:~#

### we need to create the values.yaml file, where we will define where the Prometheus is running.

root@master:~# vi grafana.yaml
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.prometheus.svc.cluster.local #       <------------------ deployment name, this is the serve, the location of my prometheus. Which is accessible through the ClusterIP service. (url: http://prometheus-server.prometheus.svc.cluster.local) prormetheus server.prometheusservice.cluster local is the fqdn.
      access: proxy
      isDefault: true
:wq!

### installing grafana through the helm chart
root@master:~# helm install grafana grafana/grafana \
    --namespace grafana \
    --set persistence.storageClassName="local-path" \
    --set persistence.enabled=true \
    --set adminPassword='admin' \
    --values grafana.yaml \
    --set service.type=NodePort
NAME: grafana
LAST DEPLOYED: Tue Feb 11 01:14:05 2025
NAMESPACE: grafana
STATUS: deployed
REVISION: 1
NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace grafana grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo


2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.grafana.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:
     export NODE_PORT=$(kubectl get --namespace grafana -o jsonpath="{.spec.ports[0].nodePort}" services grafana)
     export NODE_IP=$(kubectl get nodes --namespace grafana -o jsonpath="{.items[0].status.addresses[0].address}")
     echo http://$NODE_IP:$NODE_PORT

3. Login with the password from step 1 and the username: admin
root@master:~#

root@master:~# kubectl get all -n grafana
NAME                           READY   STATUS    RESTARTS   AGE
pod/grafana-858cc5cf44-29dlp   0/1     Evicted   0          21s
pod/grafana-858cc5cf44-6pj8x   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-94hpm   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-b8t77   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-b9mgs   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-cgdk7   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-cjfd4   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-dblbc   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-dswjk   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-f96w6   0/1     Error     0          42s
pod/grafana-858cc5cf44-fc64w   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-h9jjh   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-kcjph   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-kmqmd   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-m8z62   0/1     Evicted   0          23s
pod/grafana-858cc5cf44-mvnhv   0/1     Pending   0          20s
pod/grafana-858cc5cf44-n7wgz   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-p8gf6   0/1     Evicted   0          21s
pod/grafana-858cc5cf44-pn8mz   0/1     Evicted   0          21s
pod/grafana-858cc5cf44-sdp4t   0/1     Evicted   0          22s
pod/grafana-858cc5cf44-xff6n   0/1     Evicted   0          23s

NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/grafana   NodePort   10.105.178.98   <none>        80:32603/TCP   42s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana   0/1     1            0           42s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-858cc5cf44   1         1         0       42s
root@master:~#
