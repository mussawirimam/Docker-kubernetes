################ REPLICASET ################
### replicaSet uses equality based Selectors
# you can use condition which kinds of pods to be selected and which should be ignored

replicaSets supports set based selector. 
  Meaning, we can define the Operators like In, NotIn, Exists.
  whenever you find the key app: myapp count it as replica
  matchExpressions: 
  - key: app
  operator: In
  values: [myapp, mango] 
  - key: env
  operator: NotIn
  values: [dev, uat, prod] 

  #matchExpressions is used in the replicaSet, because of the matchExpressions, it will not be counting any other pod in it's replicas.

Deployment - 
  
  ### creating a pod with two labels app=myapp and env=dev
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:        <----------------------------
    app: myapp
    env: dev
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
    startupProbe:
      httpGet: # because nginx is webserver, so use the http probe
        path: /
        port: 80
      initialDelaySeconds: 5 # before performing this probe, how much time do we have to wait. E.g how much time it will take nginx to be first deployed an>

    livenessProbe:
      httpGet: # because nginx is webserver, so use the http probe
        path: /index.html
        port: 80
      initialDelaySeconds: 5 # before performing this probe, how much time do we have to wait. E.g how much time it will take nginx to be first deployed an>
    readinessProbe:
      httpGet: # because nginx is webserver, so use the http probe
        path: /index.html
        port: 80
      initialDelaySeconds: 5 # before performing this probe, how much time do we have to wait. E.g how much time it will take nginx to be first deployed an>      periodSeconds: 5 # every 5 second perform the probeCheck
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/replication# kk create -f pod.yaml
pod/newpod created

root@master:~/app/replication# kkgp --show-labels
NAME     READY   STATUS    RESTARTS   AGE   LABELS
newpod   2/2     Running   0          11m   app=myapp,env=dev
root@master:~/app/replication#

### replicaSet
root@master:~/app/replication# nano replicaSets.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myrs
spec:
  replicas: 3
  selector:
    matchExpressions: # Now if you are deploying, it will be only counting myapp to the replicas, and not deploying the env key
      - key: app
        operator: In
        values: ["myapp"]
      - key: env
        operator: NotIn
        values: ["dev"]
  template: # if there is no pod available on the keyvalue above, then based on the template below, your pod will be created where each pod will get
    metadata:  #label and containers
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080

root@master:~/app/replication# nano replicaSets.yaml
root@master:~/app/replication# kk create -f replicaSets.yaml
replicaset.apps/myrs created
root@master:~/app/replication# kk get replicaset
NAME   DESIRED   CURRENT   READY   AGE
myrs   3         3         3       33s

root@master:~/app/replication# kk get po
NAME         READY   STATUS    RESTARTS   AGE
myrs-84xnv   2/2     Running   0          7m23s
myrs-kfp4s   2/2     Running   0          7m23s
myrs-t8bwh   2/2     Running   0          7m23s
newpod       2/2     Running   0          49m

### SCALING IN I want to increase the replicas SCALING REPLICAS

root@master:~/app/replication# kk scale replicaset myrs --replicas 5
replicaset.apps/myrs scaled
root@master:~/app/replication# kk get rs
NAME   DESIRED   CURRENT   READY   AGE
myrs   5         5         5       10m
root@master:~/app/replication#

### SCALING OUT

root@master:~/app/replication# kk scale replicaset myrs --replicas 2
replicaset.apps/myrs scaled
root@master:~/app/replication# kk get rs
NAME   DESIRED   CURRENT   READY   AGE
myrs   2         2         2       11m
root@master:~/app/replication#

################ DEPLOYMENT ################
######## go back to the video at 26:00 to understand the logic
### deployment with 3 replicas
mydep.yaml
# creates a random string for replicas in the background to make it unique
  replica 1 mydep--abc 123
  replica 2 mydep--abc 124
  replica 3 mydep--abc 125
# assume that you are having rolling update, as soon as you are having triggering command kubectl sec image to update the image
# When you do deployment logically it creates a replicaSet in the background with xyz id and then it attaches that id to deployment 

######## go back to the video at 26:00 to understand the logic END

### CREATING DEPLOYMENT
root@master:~/app/replication# nano deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
spec:
  replicas: 3
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset is automatically is assigned to the pod. One additional label will be replicaSet id to the pod
template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080


root@master:~/app/replication# kkgp
NAME         READY   STATUS    RESTARTS   AGE
myrs-84xnv   2/2     Running   0          7h57m
myrs-t8bwh   2/2     Running   0          7h57m
newpod       2/2     Running   0          8h
root@master:~/app/replication# kk delete replicasets.apps myrs
replicaset.apps "myrs" deleted
root@master:~/app/replication#

root@master:~/app/replication# kk create -f deployment.yaml
deployment.apps/mydep created

root@master:~/app/replication# kkg deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mydep   3/3     3            3           75s
root@master:~/app/replication#

### replicaset id concept displayed here
root@master:~/app/replication# kubectl get replicasets.apps
NAME               DESIRED   CURRENT   READY   AGE
mydep-76449b474c   3         3         3       11m
root@master:~/app/replication#

### match the id with replicasets above and you will it is similar (76449b474c) to make it unique, random numbers and strings are added at suffix
root@master:~/app/replication# kkgp
NAME                     READY   STATUS    RESTARTS   AGE
mydep-76449b474c-jr827   1/1     Running   0          12m
mydep-76449b474c-njx24   1/1     Running   0          12m
mydep-76449b474c-ww7dt   1/1     Running   0          12m
newpod                   2/2     Running   0          8h
root@master:~/app/replication#

### TO SCALE UP REPLICAS MANUALLY
root@master:~/app/replication# kubectl scale deployment mydep --replicas 5
deployment.apps/mydep scaled
root@master:~/app/replication# kubectl get replicasets.apps
NAME               DESIRED   CURRENT   READY   AGE
mydep-76449b474c   5         5         5       14m
root@master:~/app/replication# kubectl get deployments.apps
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mydep   5/5     5            5           14m
root@master:~/app/replication# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
mydep-76449b474c-7s597   1/1     Running   0          38s
mydep-76449b474c-jr827   1/1     Running   0          15m
mydep-76449b474c-k65cf   1/1     Running   0          38s
mydep-76449b474c-njx24   1/1     Running   0          15m
mydep-76449b474c-ww7dt   1/1     Running   0          15m
newpod                   2/2     Running   0          8h
root@master:~/app/replication#

### TO SCALE DOWN REPLICAS MANUALLY
root@master:~/app/replication# kubectl scale deployment mydep --replicas 3
deployment.apps/mydep scaled
root@master:~/app/replication# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
mydep-76449b474c-7s597   1/1     Running   0          90s
mydep-76449b474c-jr827   1/1     Running   0          15m
mydep-76449b474c-ww7dt   1/1     Running   0          15m
newpod                   2/2     Running   0          8h
root@master:~/app/replication#

########### DEPLOYMENT AUTOSCALLING
### autoscalling works with the average cpu usage of your pod, how it will be calulating that if there are no tools installed on the host machine, on the cluster?
### to check that you need to use kubectl top pod

### its not installed, how to install it?
root@master:~/app/replication# kubectl top pods
error: Metrics API not available
root@master:~/app/replication#

### METRIC SERVER NEEDS TO BE INSTALLED in the cluster
# how to check if it is installed or not?
# you can check it within the namespace
# since we are not able to see any metric pod running in the kube-system namespace, then we need to install it.
root@master:~/app/replication# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS       AGE
calico-kube-controllers-787f445f84-gdc7t   1/1     Running   5              40d
calico-node-mrm6j                          1/1     Running   4              40d
calico-node-vhd49                          1/1     Running   5              40d
calico-node-x4q65                          1/1     Running   4              40d
coredns-5dd5756b68-84w4t                   1/1     Running   8              42d
coredns-5dd5756b68-m64mp                   1/1     Running   8              42d
etcd-master                                1/1     Running   8              42d
kube-apiserver-master                      1/1     Running   8              42d
kube-controller-manager-master             1/1     Running   12 (27h ago)   42d
kube-proxy-k4pzl                           1/1     Running   8              42d
kube-proxy-k88rh                           1/1     Running   5              41d
kube-proxy-sr6kt                           1/1     Running   5              40d
kube-scheduler-master                      1/1     Running   11 (27h ago)   42d
root@master:~/app/replication#

### INSTALLING METRIC SERVER
1. google kubernetes metric server
2. open github link
  https://github.com/kubernetes-sigs/metrics-server
3. scroll down there will be installation section, copy the command.
  command: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
4. paste the above command in your terminal and ###### NOT SURE pass in the option for non-tls --kubelet-insecure-tls
5. kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml --kubelet-insecure-tls ###### LOOK BELOW BEFORE APPLYING

### dependent objects are installed as well, like clusterrole....cluster rolebinding etc
root@master:~/app/replication# kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
root@master:~/app/replication#

### IT IS FAILING BECAUSE THE TLS CERTIFICATES ARE NOT SET SINCE THIS IS MY LOCAL MACHINE
root@master:~/app/replication# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS       AGE
calico-kube-controllers-787f445f84-gdc7t   1/1     Running   5              40d
calico-node-mrm6j                          1/1     Running   4              40d
calico-node-vhd49                          1/1     Running   5              40d
calico-node-x4q65                          1/1     Running   4              40d
coredns-5dd5756b68-84w4t                   1/1     Running   8              42d
coredns-5dd5756b68-m64mp                   1/1     Running   8              42d
etcd-master                                1/1     Running   8              42d
kube-apiserver-master                      1/1     Running   8              42d
kube-controller-manager-master             1/1     Running   12 (27h ago)   42d
kube-proxy-k4pzl                           1/1     Running   8              42d
kube-proxy-k88rh                           1/1     Running   5              41d
kube-proxy-sr6kt                           1/1     Running   5              40d
kube-scheduler-master                      1/1     Running   11 (27h ago)   42d
metrics-server-75bf97fcc9-zswhf            0/1     Running   0              2m29s  <----------------------------------
root@master:~/app/replication#

### TO RESOLVE THE ABOVE TLS ISSUE IS TO LOOK FOR NON-TLS METRIC. LOOK FOR IT ON GITHUB PAGE
#Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server)
# if you are deploying in any of the cloud kubernetes, then you will not need to do anything with metric server. We are facing this issue because it is a local machine.

root@master:~/app/replication# kubectl -n kube-system edit deployments.apps metrics-server


....
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --kubelet-insecure-tls  <------------------------------ ADD THIS LINE TO THE FILE
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
....

root@master:~# kubectl -n kube-system edit deployments.apps metrics-server
deployment.apps/metrics-server edited
root@master:~#

### recreating the pods
root@master:~# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS       AGE
calico-kube-controllers-787f445f84-gdc7t   1/1     Running   5              40d
calico-node-mrm6j                          1/1     Running   4              40d
calico-node-vhd49                          1/1     Running   5              40d
calico-node-x4q65                          1/1     Running   4              40d
coredns-5dd5756b68-84w4t                   1/1     Running   8              42d
coredns-5dd5756b68-m64mp                   1/1     Running   8              42d
etcd-master                                1/1     Running   8              42d
kube-apiserver-master                      1/1     Running   8              42d
kube-controller-manager-master             1/1     Running   12 (27h ago)   42d
kube-proxy-k4pzl                           1/1     Running   8              42d
kube-proxy-k88rh                           1/1     Running   5              41d
kube-proxy-sr6kt                           1/1     Running   5              40d
kube-scheduler-master                      1/1     Running   11 (27h ago)   42d
metrics-server-58ff449c8d-bch5w            0/1     Running   0              28s
metrics-server-75bf97fcc9-zswhf            0/1     Running   0              26m
root@master:~#

### METRIC SERVER IS STARTED NOW
root@master:~# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS       AGE
calico-kube-controllers-787f445f84-gdc7t   1/1     Running   5              40d
calico-node-mrm6j                          1/1     Running   4              40d
calico-node-vhd49                          1/1     Running   5              40d
calico-node-x4q65                          1/1     Running   4              40d
coredns-5dd5756b68-84w4t                   1/1     Running   8              42d
coredns-5dd5756b68-m64mp                   1/1     Running   8              42d
etcd-master                                1/1     Running   8              42d
kube-apiserver-master                      1/1     Running   8              42d
kube-controller-manager-master             1/1     Running   12 (27h ago)   42d
kube-proxy-k4pzl                           1/1     Running   8              42d
kube-proxy-k88rh                           1/1     Running   5              41d
kube-proxy-sr6kt                           1/1     Running   5              40d
kube-scheduler-master                      1/1     Running   11 (27h ago)   42d
metrics-server-58ff449c8d-bch5w            1/1     Running   0              51s

### KUBECTL TOP

root@master:~# kubectl top pods
NAME                     CPU(cores)   MEMORY(bytes)
mydep-76449b474c-7s597   0m           2Mi
mydep-76449b474c-jr827   0m           2Mi
mydep-76449b474c-ww7dt   0m           2Mi
newpod                   2m           93Mi
root@master:~#

root@master:~# kubectl top node
NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master   128m         3%     3649Mi          46%
node1    42m          2%     1981Mi          52%
node2    50m          2%     2281Mi          60%
root@master:~#

##### AUTO SCALING USING METRIC FOR HPA
# while having multiple replicas, how you would be accessing the replicas outside of the cluster?
# we are having loadbalancer or nodeport service
##### EXPOSING REPLICAS TO NODEPORT SERVICE TO ACCESS FROM OUTSIDE THE CLUSTER
root@master:~# kubectl expose deployment mydep --port 80 --type NodePort --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: mydep
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: myapp
  type: NodePort
status:
  loadBalancer: {}
root@master:~#


root@master:~# kubectl expose deployment mydep --port 80 --type NodePort
service/mydep exposed
root@master:~# kubectl get service
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        43d
mydep        NodePort    10.110.18.118   <none>        80:32412/TCP   13s  <-------------- copy the port 32412
newpod       ClusterIP   10.102.85.76    <none>        80/TCP         3d23h
root@master:~#

### copy the ip address of any node and plug it in the browser with the NodePort port 
# traffic is flowing from service to the pod
http://192.168.29.134:32412/
Welcome to nginx!
If you see this page, the nginx web server is successfully installed and working. Further configuration is required.

############ LOGS
### to know which pod is serving the request, go manually in the pod and check the log 
NAME                     READY   STATUS    RESTARTS   AGE
mydep-76449b474c-7s597   1/1     Running   0          14h
mydep-76449b474c-jr827   1/1     Running   0          15h
mydep-76449b474c-ww7dt   1/1     Running   0          15h
newpod                   2/2     Running   0          23h
root@master:~# kubectl logs mydep-76449b474c-7s597
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/12/30 21:57:55 [notice] 1#1: using the "epoll" event method
2024/12/30 21:57:55 [notice] 1#1: nginx/1.27.3
2024/12/30 21:57:55 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)
2024/12/30 21:57:55 [notice] 1#1: OS: Linux 6.8.0-49-generic
2024/12/30 21:57:55 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/12/30 21:57:55 [notice] 1#1: start worker processes
2024/12/30 21:57:55 [notice] 1#1: start worker process 29
2024/12/30 21:57:55 [notice] 1#1: start worker process 30
root@master:~#

### this pod is getting the traffic
root@master:~# kubectl logs mydep-76449b474c-ww7dt
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/12/30 21:43:33 [notice] 1#1: using the "epoll" event method
2024/12/30 21:43:33 [notice] 1#1: nginx/1.27.3
2024/12/30 21:43:33 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)
2024/12/30 21:43:33 [notice] 1#1: OS: Linux 6.8.0-49-generic
2024/12/30 21:43:33 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/12/30 21:43:33 [notice] 1#1: start worker processes
2024/12/30 21:43:33 [notice] 1#1: start worker process 28
2024/12/30 21:43:33 [notice] 1#1: start worker process 29
172.17.219.64 - - [31/Dec/2024:12:52:38 +0000] "GET / HTTP/1.1" 200 615 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0" "-"
2024/12/31 12:52:38 [error] 28#28: *1 open() "/usr/share/nginx/html/favicon.ico" failed (2: No such file or directory), client: 172.17.219.64, server: localhost, request: "GET /favicon.ico HTTP/1.1", host: "192.168.29.134:32412", referrer: "http://192.168.29.134:32412/"
172.17.219.64 - - [31/Dec/2024:12:52:38 +0000] "GET /favicon.ico HTTP/1.1" 404 555 "http://192.168.29.134:32412/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0" "-"
root@master:~#

######## ISTIO ########
# generally what happens is to find out the logs above, we have a service mesh called ISTIO. 
# under it you will have to install the plugin called KIALI. It will be showing you live dashboard, which pod is currently serving the traffic. 
# traffic flow will be displayed in Kiali, it is third party tool. (just for knowledge)

### we will be exposing the service

root@master:~# kkgd
NAME    READY   UP-TO-DATE   AVAILABLE   AGE  <------------- 3 are available to serve the traffic
mydep   3/3     3            3           15h  
root@master:~#
root@master:~# kkgd -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
mydep   3/3     3            3           15h   con1         nginx:latest   app=myapp
root@master:~#

root@master:~# kubectl describe deployments.apps mydep
Name:                   mydep
Namespace:              default
CreationTimestamp:      Mon, 30 Dec 2024 16:43:31 -0500
Labels:                 <none>            <-------------- no label defined in our deployment
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=myapp  <---------------------- label
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate      <----------- IMPORTANT: right now we are only discussing RollingUpdate strategy
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge  <--------- IMPORTATNT: imagine we are having 100% pods, first 25% pods will be deleted and as soon as those 25% are updated. Only then, another 25% will be recreated as soon as those are ready. And so on it will be deleting 25% and recreating and updating.. this is the reason we have 100% UPTIME
Pod Template:
  Labels:  app=myapp
  Containers:
   con1:
    Image:        nginx:latest
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   mydep-76449b474c (3/3 replicas created)
Events:          <none>
root@master:~#

#LEARN ABOUT ROLLINGUPDATE STRATEGY AND OTHER STRATEGIES.
============ROLLING UPDATE============
root@master:~# kkgd -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
mydep   3/3     3            3           15h   con1         nginx:latest   app=myapp
root@master:~# kubectl set image deployment mydep con1=http:latest
deployment.apps/mydep image updated
root@master:~#

root@master:~# kkgd -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
mydep   3/3     1            3           15h   con1         http:latest   app=myapp

### new replicaset is created
root@master:~# kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-76449b474c   3         3         3       15h
mydep-7cb7ffc4cf   1         1         0       24s
root@master:~#

### if first 25% pod is not updated, then what happens is that some Error will be there in new image, it will not be touching another pods.
root@master:~# kkgp
NAME                     READY   STATUS             RESTARTS   AGE
mydep-76449b474c-7s597   1/1     Running            0          15h
mydep-76449b474c-jr827   1/1     Running            0          15h
mydep-76449b474c-ww7dt   1/1     Running            0          15h
mydep-7cb7ffc4cf-t6hbs   0/1     ImagePullBackOff   0          94s  <----------------
newpod                   2/2     Running            0          24h
root@master:~#
root@master:~#

### purposefully we made the error when setting the image above.
### let's correct it.
root@master:~# kubectl set image deployment mydep con1=httpd:latest
deployment.apps/mydep image updated
# new replicaset is
root@master:~# kk get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   1         1         0       7s
mydep-76449b474c   3         3         3       16h
mydep-7cb7ffc4cf   0         0         0       7m43s
root@master:~#
root@master:~# kkgp
NAME                     READY   STATUS    RESTARTS   AGE
mydep-66d7c976c5-mr2rm   1/1     Running   0          34s
mydep-66d7c976c5-tq2sp   1/1     Running   0          42s
mydep-66d7c976c5-wvcgk   1/1     Running   0          27s
newpod                   2/2     Running   0          24h
root@master:~#
### slowly all pods are replace because of the Rolling Strategy Update
root@master:~# kkgp
NAME                     READY   STATUS    RESTARTS   AGE
mydep-66d7c976c5-mr2rm   1/1     Running   0          65s
mydep-66d7c976c5-tq2sp   1/1     Running   0          73s
mydep-66d7c976c5-wvcgk   1/1     Running   0          58s
newpod                   2/2     Running   0          24h
root@master:~#

### how we will be checking the history of the deployment
1 way check the replicasets for the deployment they should have different ID and the previousl replicasets arent running
root@master:~# kkgd
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mydep   3/3     3            3           16h
root@master:~# kkgrs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   3         3         3       2m14s
mydep-76449b474c   0         0         0       16h    <---------------
mydep-7cb7ffc4cf   0         0         0       9m50s <---------------
root@master:~#

### lets suppose there are 100 of pods, then it is not good idea to check each pod and replicaset.
### then we should check the history 
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
1         <none>      <---------- change-cause is showing none is, because we didnt mentioned any annotations
2         <none>
3         <none>
root@master:~#

### VERY IMPORTATNT
### if the change cause is not mentioned, then what should we do? lets say the what user did or which image the user used at the rollingUpdate?

root@master:~# kubectl rollout history deployment mydep --revision 2
deployment.apps/mydep with revision #2
Pod Template:
  Labels:       app=myapp
        pod-template-hash=7cb7ffc4cf
  Containers:
   con1:
    Image:      http:latest
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

root@master:~#

### VERY IMPORTANT
### HOW TO WRITE THE CHANGE CAUSE?
# only last annotation can be updated, you cannot modify the previous revisions

root@master:~# kubectl annotate deployments.apps mydep kubernetes.io/change-cause="updated to httpd latest image"
deployment.apps/mydep annotated
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         updated to httpd latest image

root@master:~#

### VERY IMPORTANT
# now I want to go back and do a ROLLBACK
# to go to the revision number 1

root@master:~# kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   3         3         3       14m  <------------ these pods from the replicaset will be deleted, once you do the rollback
mydep-76449b474c   0         0         0       16h
mydep-7cb7ffc4cf   0         0         0       21m
root@master:~#

# on browser
http://192.168.29.134:32412/
It works!

### new pods created and previous ones are getting deleted
root@master:~# kubectl rollout undo deployment mydep --to-revision 1
deployment.apps/mydep rolled back
root@master:~# kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   1         1         1       16m
mydep-76449b474c   3         3         2       16h
mydep-7cb7ffc4cf   0         0         0       24m
root@master:~#

# on browser
http://192.168.29.134:32412/
Welcome to nginx!
If you see this page, the nginx web server is successfully installed and working. Further configuration is required.

### see the revision number got changed to last 4
root@master:~# kubectl get deployments.apps -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
mydep   3/3     3            3           16h   con1         nginx:latest   app=myapp
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
3         updated to httpd latest image
4         <none>

### write something in annotation

root@master:~# kubectl annotate deployments.apps mydep kubernetes.io/change-cause="rolled back to nginx latest"
deployment.apps/mydep annotated
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
3         updated to httpd latest image
4         rolled back to nginx latest

root@master:~#
root@master:~#

### why it is copying the the annotation?
root@master:~# kubectl set image deployment mydep con1=tomcat:latest
deployment.apps/mydep image updated
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
3         updated to httpd latest image
4         rolled back to nginx latest
5         rolled back to nginx latest

# because one annotation is added in your metadata section from previous rollout which you annotated
root@master:~# kubectl edit deployments.apps mydep
root@master:~#
  annotations:
    deployment.kubernetes.io/revision: "5"
    kubernetes.io/change-cause: rolled back to nginx latest
  creationTimestamp: "2024-12-30T21:43:31Z"

### lets annotated. Annotation will increase as the replicaset increases. 

root@master:~# kubectl edit deployments.apps mydep
Edit cancelled, no changes made.
root@master:~# kubectl annotate deployments.apps mydep kubernetes.io/change-cause="updated image to tomcat"
deployment.apps/mydep annotated
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
3         updated to httpd latest image
4         rolled back to nginx latest
5         updated image to tomcat
root@master:~#

# it is because you will be having replicasets been increased 
root@master:~# kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   0         0         0       24m
mydep-76449b474c   0         0         0       16h
mydep-79f47f56b7   3         3         3       3m17s
mydep-7cb7ffc4cf   0         0         0       31m
root@master:~#

# it will fail on browser, because port 80 is mentioned and tomcat uses port 8080
http://192.168.29.134:32412/

### ROLLBACK
root@master:~# kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   0         0         0       22h
mydep-76449b474c   0         0         0       39h
mydep-79f47f56b7   3         3         3       22h
mydep-7cb7ffc4cf   0         0         0       23h
root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
3         updated to httpd latest image
4         rolled back to nginx latest
5         updated image to tomcat

root@master:~# kubectl rollout undo deployment mydep --to-revision 3
deployment.apps/mydep rolled back
root@master:~# kk get rs
NAME               DESIRED   CURRENT   READY   AGE
mydep-66d7c976c5   3         3         2       22h
mydep-76449b474c   0         0         0       39h
mydep-79f47f56b7   1         1         1       22h
mydep-7cb7ffc4cf   0         0         0       23h
root@master:~# kk get pod
NAME                     READY   STATUS    RESTARTS   AGE
mydep-66d7c976c5-9bhxl   1/1     Running   0          16s
mydep-66d7c976c5-fxtkd   1/1     Running   0          12s
mydep-66d7c976c5-m9s8l   1/1     Running   0          20s
newpod                   2/2     Running   0          47h
root@master:~#

root@master:~# kubectl rollout history deployment mydep
deployment.apps/mydep
REVISION  CHANGE-CAUSE
2         <none>
4         rolled back to nginx latest
5         updated image to tomcat
6         updated to httpd latest image

root@master:~#

########### AUTO SCALING ###########
## if in yaml file you mention 5 and for autoscaling the number 3 then the replicas will decrease.
### auto scale deployment called mydep to minimum 3 and maximum to 10, once the cpu percentage reaches to the 75 percent.

root@master:~# kk get deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mydep   3/3     3            3           39h
root@master:~# kubectl autoscale deployment mydep --min 3 --max 10 --cpu-percent 75 --dry-run=client -o yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: mydep
spec:
  maxReplicas: 10
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mydep
  targetCPUUtilizationPercentage: 75
status:
  currentReplicas: 0
  desiredReplicas: 0
root@master:~#


root@master:~/app/replication# nano deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
spec:
  replicas: 3
  selector:
    matchLabels: #in deployment you will not see problems in replicaset or replicationcontroller
      app: myapp   #reason for that is that whenever your pod deployment is created, it creates the replicaSet in the background and this replicaset id xyz>
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: mydep
spec:
  maxReplicas: 10
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mydep
  targetCPUUtilizationPercentage: 75
### keep the autoscaling in yaml with the deployment, because most of the time whenever you are creating HPA in the imperative way with the
### commands, when you are removing the deployment. The autoscaler will be left as it is. It is a bad idea to keep the unused autoscaler in the cluster
### keep the autoscaler logic in the yaml with the deployment, so whenever you are deleting the yaml with delete command, whatever objects are
# mentioned in the yaml file those will be removed as well

### using the imperative way for demonstration purpose
root@master:~/app/replication# kubectl autoscale deployment mydep --min 3 --max 10 --cpu-percent 75
horizontalpodautoscaler.autoscaling/mydep autoscaled

######## KUBECTL GET HPA
root@master:~/app/replication# kubectl get hpa
NAME    REFERENCE          TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mydep   Deployment/mydep   <unknown>/75%   3         10        0          11s
root@master:~/app/replication#

### if you are seeing unknown, there are two reason
# 1st check metric server is installed or not
# 2nd your application is not having enough traffic, it cannot calculate the cpu usage.

####### GOOD EXCERSISE FOR HOMEWORK AUTOSCALING KUBERNETES EXAMPLE:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
root@master:~/app/replication# vi hpa-example-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache

root@master:~/app/replication# kubectl delete deployments.apps mydep
deployment.apps "mydep" deleted
root@master:~/app/replication#

root@master:~/app/replication# kubectl create -f hpa-example-dep.yaml
deployment.apps/php-apache created
service/php-apache created
root@master:~/app/replication# kubectl get hpa
NAME    REFERENCE          TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mydep   Deployment/mydep   <unknown>/75%   3         10        3          18m
root@master:~/app/replication# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/php-apache autoscaled
root@master:~/app/replication# kubectl get hpa
NAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mydep        Deployment/mydep        <unknown>/75%   3         10        3          18m
php-apache   Deployment/php-apache   <unknown>/50%   1         10        0          2s
root@master:~/app/replication#

# generating the load and open in another terminal
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"


root@master:~/app/replication# kubectl get pods
NAME                          READY   STATUS              RESTARTS   AGE
load-generator                1/1     Running             0          20s
newpod                        2/2     Running             0          2d
php-apache-598b474864-j6ntv   0/1     ContainerCreating   0          11s
php-apache-598b474864-pchtx   1/1     Running             0          3m15s
php-apache-598b474864-vtfvj   1/1     Running             0          11s
root@master:~/app/replication# kubectl get hpa
NAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mydep        Deployment/mydep        <unknown>/75%   3         10        3          21m
php-apache   Deployment/php-apache   144%/50%        1         10        3          2m42s
root@master:~/app/replication#

### as the load is increasing the pods are being created
root@master:~/app/replication# kubectl get hpa
NAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mydep        Deployment/mydep        <unknown>/75%   3         10        3          22m
php-apache   Deployment/php-apache   100%/50%        1         10        6          3m59s
root@master:~/app/replication# kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
load-generator                1/1     Running   0          2m3s
newpod                        2/2     Running   0          2d
php-apache-598b474864-5t4cp   1/1     Running   0          39s
php-apache-598b474864-g8tbh   1/1     Running   0          39s
php-apache-598b474864-j6ntv   1/1     Running   0          114s
php-apache-598b474864-pchtx   1/1     Running   0          4m58s
php-apache-598b474864-vtfvj   1/1     Running   0          114s
php-apache-598b474864-w5l7f   1/1     Running   0          39s
root@master:~/app/replication#

### my question is how it will auto-scale down, if the cpu usage is now low? 
