########## DAEMONSET ##########
# let's suppose you have 3 workder node and I have to deploy application pod in each of the worker node
# till now we deploy the replicas of our pod through replicaController, ReplicaSet, and deployment. 
# Now see that, if you are going with above objects, they will create the replicas but doesnt guarantee that my application will be deployed in each of the worker node
# Let's suppose I am having some of the application which is primarily using for logging and monitoring
# whenever we are trying to deploy some of the logging monitoring application like ELK file beat monitoring applications
# these application should be deployed in each of the worker node, so that we able to collect logs and metrics.
# In this situation, the deployment will not be helping you out! Why?
# it will be guaranteeing you to define the number of replicas, but it will not guaranteeing you that it's replica will be going in each of the worker nodes. 

# the thing is that, I want to deploy the pod replica in each of the worker node. Let's assume that, my pod got deployed in each of the node.
# I have three worker nodes. Somehow, worker node3 is not up and running. The pod will not be getting deployed somewhere. It will be in its place. 
# if I add one more node in cluster, a new pod will be deployed in the new node in the cluster. 
node1
node2
node3

node4 

### IMPORTANT
### whenever we deploy an application, which is required to run on the each of the node. Then we will go for the DaemonSets
### what kind of those application can be? 
### logging and monitoring application, to collect the logs and metrics. Such applications requires to go and deployed in each of the node, so that we can collect information
# from the each of the worker node. It could be logging or metric (cpu or memory) We will go for the DAEMONSET

### what is the need for DaemonSet?
# A DaemonSet is a Kubernetes resource used to ensure that a specific pod is running on all (or some) nodes in a Kubernetes cluster. It plays a crucial role in scenarios where you need to deploy background tasks,
#monitoring agents, log collectors, or other system-level services across every node in the cluster.

### once you are creating DaemonSet, your pod will be getting created in each of the worker node. You dont need to define the number of replicas, it depends on the number of worker nodes.
# you can deploy a daemonset on the master node as well if it is an on-prem server or cop server.
# if it is a cloud kubernetes, you can see master node server on cloudwatch or some other services.
# if some worker node is failing or going down. What will happen is, in such scenerio the pod will node be re-scheduling on other nodes.
# if the worker node is back again, the pod will be back again.
# if a new worker node is connected to cluster, the new DAEMON POD will be deployed and get the logs and metrics. 
# DaemonSets gets deployed based on the worker nodes.

# we will be copying the yaml from the kubernetes documentation below:
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/

root@master:~/app/replication# nano daemonsetforworkernodes.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: con1
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
root@master:~/app/replication#

root@master:~/app/replication# kubectl create -f daemonsetforworkernodes.yaml
daemonset.apps/my-daemonset created
root@master:~/app/replication# kubectl get daemonsets.apps
NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE  <-------- we can specify a particular Node, affinity, apply tait toleration to it as well.
my-daemonset   2         2         0       2            0           <none>          7s
root@master:~/app/replication#

### why one daemon is not deployed in the master node? because there is a TAINT on the master node so that nothing on the master node will be deployed. 
root@master:~/app/replication# kkgp
NAME                          READY   STATUS    RESTARTS   AGE
my-daemonset-l5tn2            1/1     Running   0          21s
my-daemonset-mdfjk            1/1     Running   0          21s
newpod                        2/2     Running   0          2d7h
php-apache-598b474864-pchtx   1/1     Running   0          7h3m
root@master:~/app/replication#

### I want to install the daemonSet in the master node as well, what should I do?
### I need to add the TAINT in the DAEMONSET

root@master:~/app/replication# kk describe nodes master | grep -A 2 "Taint"
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Unschedulable:      false
Lease:
root@master:~/app/replication#

root@master:~/app/replication# nano daemonsetTaintedformasternodes.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      containers:
      - name: con1
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
root@master:~/app/replication#


root@master:~/app/replication# kk create -f daemonsetTaintedTolerationformasternodes.yaml
daemonset.apps/my-daemonset created
root@master:~/app/replication#


root@master:~/app/replication# kkgp -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
my-daemonset-2d4c5            1/1     Running   0          38s     172.17.166.187   node1    <none>           <none>
my-daemonset-4qmdz            1/1     Running   0          38s     172.17.219.74    master   <none>           <none>        <------------------- Now with the toleration, it is deployed on the masterNode as well
my-daemonset-lql94            1/1     Running   0          38s     172.17.104.52    node2    <none>           <none>
newpod                        2/2     Running   0          2d12h   172.17.166.168   node1    <none>           <none>
php-apache-598b474864-pchtx   1/1     Running   0          11h     172.17.166.181   node1    <none>           <none>
root@master:~/app/replication#

### if you want to deploy it on the particular node, you can do it from Node Selector. We have covered that in the Scheduling Section
======================================================================================================================================
################## VOLUME ##################  

Volume Type: 
1. emptyDir - is a temporary volume which can be created to share the data between the containers within the pod. 
2. HostPath - this allows you to access the data from the hostmachine. It is used to host the path of the hostmachine on the pod

### deployment on same nodes will have same data and deployment on different node will have different data. 
### if you want to keep the same data across all the nodes on pods, you will have to implement the NFS or EFS volumes

Volume Provisioning:
1. Stativ volume Provisioning - 
2. Dynamic volume Provisioning - 

Static volume provisioining issue:
# what is the difference between these two types of provisioning
static volume provisioning is gone case now (not being used much) lets you want to use a volume that is created through the static volume provisioning. How it works:
to create a volume in your cloud --> then create a pv (persistent-volume) --> PVC (claim it) --> mount-pod 
# whenever you need a volume, you need to ask the infra team to create a ticket in service now and create the volume with aws account etc 
# your team mate will be creating the volume and infra team will be creating volume and then you will be adding that volume in your k8s by creating pv and you will be claiming the volume. 
# and then you will be using your pod. For some how you deleted your deployment and then back again you will go to infra team and tell them to delete it. (IT TAKES TIME)
  still you can use it for the EBS/NFS

Dynamic volume provisioning solution for the static volume:
Storage Class > pvc 10GB 
Storage Class will have the permissions (additional permissions will be applied to your storage class to spin up volume or deletion of your volume in your cloud account)
Storage classes are very important to create the pvc

# do the dynamic volume in killercoda or
# on kubeadm, we will have to install rancher local volume provisioner:
https://github.com/rancher/local-path-provisioner

================= EMPTYDIR ================= 

# imagine we have container 1 and 2 in a pod
# how we are connecting one container to another container? 
  # based on the port numbers.
imagine, we have files on con1 and we want to share it on con2
how can we share the these files? We can do that through the emptyDir which can be created through inside the pod. 
you have to mount this emptyDir volume to your container. 
con1
con2

whenever we are deploying applications in kubernetes, we have to follow microservice architecture. Applications needs to be divided in different modules and deployed. 
My application is running in con1. Con1 application is generating some data and I want to forward the logs from con1 to elk or splunk. How are we going to do that?
We are going to install splunk forwarder or logstash, so that I can connect the log forwarder to my splunk or elasticsearch. How is this possible? 
if I am installing multiple applications within the con1, then I am breaking the rule of microservice architecture. Multiple applications should not be deployed in on place. We have to be Monolithic. 
What kind of problems we can face if we install log forwarder inside application container? we can face lack of resources because of both application and log forwarder is running at the same time.
2nd issue can be that your container will be over burdened, because of two applications. To make it lightweight, k8s says that we should only keep two containers in the pod. 
So, we can install another container in pod con1 will be your application and con2 will be your log forwarder. 


============  SIDE CAR CONTAINER ============  
Pod
-----------------------------------------------------------
con1 = app: /app/log
con2 = log-fwdr : /var/log/app 
[emptyDir] = connected to both con1 and con2 within a pod
-----------------------------------------------------------
root@master:~/app# nano emptyDir.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-emp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:  # volumeMount
        - name: myvol
          mountPath: /data  # if the directory isnt present, it will create it
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts: # volumeMount
        - name: myvol
          mountPath: /mydata # if the directory isnt present, it will create it
      volumes:
      - name: myvol
        emptyDir: {}  # volume type
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-emp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /mydata
      volumes:
      - name: myvol
        emptyDir: {}

root@master:~/app/volume# kubectl create -f EmptyDir.yaml
deployment.apps/mydep-emp created
root@master:~/app/volume# kkgp
NAME                         READY   STATUS              RESTARTS   AGE
mydep-emp-5fb7d76c48-57hvc   2/2     Running             0          3s
mydep-emp-5fb7d76c48-6qzmq   2/2     Running             0          3s
mydep-emp-5fb7d76c48-c5c9b   0/2     ContainerCreating   0          3s

# the important part is that emptyDir scope is within the pod. Meaning that even if you are creating the replicas it will not be having the same data in all the replicas
# data will be varying as per your application. Application in one container will be stored and shared to another container through the emptyDir volume. 

root@master:~/app/volume# kubectl exec -it mydep-emp-5fb7d76c48-57hvc -c con1 -- bash
root@mydep-emp-5fb7d76c48-57hvc:/# ls
bin   data  docker-entrypoint.d   etc   lib    media  opt   root  sbin  sys  usr
boot  dev   docker-entrypoint.sh  home  lib64  mnt    proc  run   srv   tmp  var
root@mydep-emp-5fb7d76c48-57hvc:/#
root@mydep-emp-5fb7d76c48-57hvc:/# cd data
root@mydep-emp-5fb7d76c48-57hvc:/data#

root@mydep-emp-5fb7d76c48-57hvc:/data# touch mango.txt
root@mydep-emp-5fb7d76c48-57hvc:/data# ls
mango.txt
root@mydep-emp-5fb7d76c48-57hvc:/data#

# exit from the above container and login into container2
root@mydep-emp-5fb7d76c48-57hvc:/data# exit
exit
root@master:~/app/volume# kubectl exec -it mydep-emp-5fb7d76c48-57hvc -c con2 -- bash
root@mydep-emp-5fb7d76c48-57hvc:/usr/local/tomcat# cd /mydata/
root@mydep-emp-5fb7d76c48-57hvc:/mydata# ls
mango.txt              <------------------------------------------------ # you can see the file (mango.txt) that you created from the container1 is present in the container2
root@mydep-emp-5fb7d76c48-57hvc:/mydata#


### if you login into the same replica pod on the same node in this case, you can see there is no data. Because the scope is only within the pod not the node dddddddddddddddddddddddddddddddddddddddddddddddddddddddds

root@mydep-emp-5fb7d76c48-57hvc:/mydata# exit
exit
root@master:~/app/volume# kkgp -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mydep-emp-5fb7d76c48-57hvc   2/2     Running   0          6m51s   172.17.166.129   node1   <none>           <none>
mydep-emp-5fb7d76c48-6qzmq   2/2     Running   0          6m51s   172.17.104.58    node2   <none>           <none>
mydep-emp-5fb7d76c48-c5c9b   2/2     Running   0          6m51s   172.17.166.130   node1   <none>           <none>
root@master:~/app/volume#

### remember emptyDir has the scope for within the the same pods. 
root@master:~/app/volume# kk exec -it mydep-emp-5fb7d76c48-c5c9b -c con1 -- bash
root@mydep-emp-5fb7d76c48-c5c9b:/# ls
bin   data  docker-entrypoint.d   etc   lib    media  opt   root  sbin  sys  usr
boot  dev   docker-entrypoint.sh  home  lib64  mnt    proc  run   srv   tmp  var
root@mydep-emp-5fb7d76c48-c5c9b:/# ls data
root@mydep-emp-5fb7d76c48-c5c9b:/# cd data
root@mydep-emp-5fb7d76c48-c5c9b:/data# ls
root@mydep-emp-5fb7d76c48-c5c9b:/data#

IMPORTANT: 
emptyDir is an emphemeral volumes. As soon as the pod is deleted, the volume will be deleted as well. Whatever data you were having in the EmptyDir volume, will be recoverable. 

================== HOST PATH ================== 

NODE:
pod
node /path on host machine ON LINUX you need mount to store all the logs /var/log 

root@master:~# ls /var/log
alternatives.log       boot.log       btmp                   dist-upgrade  dpkg.log.2.gz    kern.log.1         sssd
alternatives.log.1     boot.log.1     btmp.1                 dmesg         faillog          kern.log.2.gz      syslog
alternatives.log.2.gz  boot.log.2     calico                 dmesg.0       fontconfig.log   kern.log.3.gz      syslog.1
apport.log             boot.log.3     cloud-init.log         dmesg.1.gz    gdm3             lastlog            syslog.2.gz
apt                    boot.log.4     cloud-init-output.log  dmesg.2.gz    gpu-manager.log  openvpn            syslog.3.gz
auth.log               boot.log.5     containers             dmesg.3.gz    hp               pods               sysstat
auth.log.1             boot.log.6     crio                   dmesg.4.gz    installer        private            unattended-upgrades
auth.log.2.gz          boot.log.7     cups                   dpkg.log      journal          README             wtmp
auth.log.3.gz          bootstrap.log  cups-browsed           dpkg.log.1    kern.log         speech-dispatcher
root@master:~#

### so many logs are there, so you need to mount this path with your monitoring applicaiton
root@master:~# ls /var/log/containers/
calico-node-mrm6j_kube-system_calico-node-06384161e74c568f69efc11eadb2e772be087d0e58a0d7124263e5e8727cdf13.log
calico-node-mrm6j_kube-system_calico-node-203dc934ded2c87d3bd00bf59ac60dc19dc4ea749bbbe2a9f3dc41973f630302.log
calico-node-mrm6j_kube-system_calico-node-88590028f226cbf5078371f7b64558c8fcb5735e9de1c0a3ef2904478ea3bca5.log
calico-node-mrm6j_kube-system_calico-node-88ee811da9fba93f68697b44edd8e0fe9d4652dfd493bdd8d9e1fe6dbf35a749.log
calico-node-mrm6j_kube-system_calico-node-dfe279891f0518a9daa78eb83642c122daa19fc0d51112f298bd8df9ab5faa88.log.........

### how will you be accessing the data from host machine to your container? 
### there are two ways:
if i want to store my application data on the worker node
second I want to pull hostmachines data while being in the container!
### HOST VOLUME ---> CONTAINER VOLUME MOUNT
root@master:~/app/volume# nano hostpath-volume.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-hpath
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /mydata
      volumes:
      - name: myvol
        hostPath:
          path: /var/log/containers/ # the path that you want to access from hostmachine on container

root@master:~/app/volume# kk create -f hostpath-volume.yaml
deployment.apps/mydep-hpath created
root@master:~/app/volume#

root@master:~/app/volume# kkgp -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mydep-hpath-654c89df45-27tfm   2/2     Running   0          37s   172.17.104.59    node2   <none>           <none>
mydep-hpath-654c89df45-ggdgz   2/2     Running   0          37s   172.17.166.132   node1   <none>           <none>
mydep-hpath-654c89df45-m62fv   2/2     Running   0          37s   172.17.166.131   node1   <none>           <none>
root@master:~/app/volume#

root@master:~/app/volume# kubectl exec -it mydep-hpath-654c89df45-27tfm -c con1 -- bash
root@mydep-hpath-654c89df45-27tfm:/#

### all the logs from the host machine are mounted within the /data
root@mydep-hpath-654c89df45-27tfm:/# cd /data
root@mydep-hpath-654c89df45-27tfm:/data# ls
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-                                               controllers-238243b364f7bf80cf9e068c9b3e9b8d2921e17b08c36889ccf7db8b8b602e8a.log
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-controllers-268e6f92065553a461f683f0d39ea6722cdcd2c1e77043ecdaea63285db75dce.log
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-controllers-2ab84183dc3048a797b8d84ea9c8c9632da94455107ecb7b3a7b35505b8f4fd1.log
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-controllers-2be2866bda11962a85f5d0e2993c157c516f46d5963b74db1512b0096debc43c.log
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-controllers-a192da7569b63818630ab14808d4fd8b4248df7507532858181d2e4eee1a680f.log
calico-kube-controllers-787f445f84-gdc7t_kube-system_calico-kube-controllers-f879aba05978d5866d3bf163a16250ee33208126e81d3915075b648d75f8525f.log..........

root@master:~/app/volume# kk delete -f hostpath-volume.yaml
deployment.apps "mydep-hpath" deleted
root@master:~/app/volume# ls

# I noticed one thing that if I create anything on the in hostmachine path it is not showing up on the volume mount within the container

CONTAINER VOLUME MOUNT ---> HOST VOLUME  
### this scenerio, I will be storing the data from container to host machine
# the director will be created on the host machine, whenever my pod is scheduled and my data will be stored into desired path /appdata 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-hpath
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /mydata
      volumes:
      - name: myvol
        hostPath:
          path: /appdata # this directory will be created on the hostmachine and whenever my pod is scheduled and my data will be stored in this pathapiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-hpath
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /mydata
      volumes:
      - name: myvol
        hostPath:
          path: /appdata # this directory will be created on the hostmachine and whenever my pod is scheduled and my data will be stored in this path

root@master:~/app/volume# kk create -f hostpath-volume.yaml
deployment.apps/mydep-hpath created
root@master:~/app/volume# kkgp
NAME                           READY   STATUS              RESTARTS   AGE
mydep-hpath-848c6f9f6d-nxb26   0/2     ContainerCreating   0          2s
mydep-hpath-848c6f9f6d-t2sb9   0/2     ContainerCreating   0          2s
mydep-hpath-848c6f9f6d-w9nlb   2/2     Running             0          3s
root@master:~/app/volume# kubectl exec -it mydep-hpath-848c6f9f6d-nxb26 -c con1 -- bash
root@mydep-hpath-848c6f9f6d-nxb26:/# cd data
root@mydep-hpath-848c6f9f6d-nxb26:/data# ls
root@mydep-hpath-848c6f9f6d-nxb26:/data# cat > mango.txt
hello this is a mango file..."
^Z
[1]+  Stopped                 cat > mango.txt
root@mydep-hpath-848c6f9f6d-nxb26:/data# ls
mango.txt
root@mydep-hpath-848c6f9f6d-nxb26:/data# cat mango.txt
hello this is a mango file..."
root@mydep-hpath-848c6f9f6d-nxb26:/data#
### both containers will have the same data

# deployment on node 2 container 1, we created another file called apple 
root@mydep-hpath-848c6f9f6d-t2sb9:/data# ls
apple.txt
root@mydep-hpath-848c6f9f6d-t2sb9:/data#

### If i delete the deployment the directory from the host machine will not be deleted and whenever I re-create that deployment 

### you will not be able to see the data from node1 to node2 and vice wersa
root@master:~/app/volume# kkgp -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mydep-hpath-848c6f9f6d-nxb26   2/2     Running   0          8m31s   172.17.166.134   node1   <none>           <none>
mydep-hpath-848c6f9f6d-t2sb9   2/2     Running   0          8m31s   172.17.104.60    node2   <none>           <none>
mydep-hpath-848c6f9f6d-w9nlb   2/2     Running   0          8m32s   172.17.166.133   node1   <none>           <none>

### delete the deployments and see if the volume path is giving us the same data that we previously created in each containers? 
### if you re-create the deployment again and access into the container, you will see the data present there.

root@master:~/app/volume# kk delete -f hostpath-volume.yaml
deployment.apps "mydep-hpath" deleted
root@master:~/app/volume# kubectl get pod
No resources found in default namespace.
root@master:~/app/volume# kk create -f hostpath-volume.yaml
deployment.apps/mydep-hpath created
root@master:~/app/volume# kk get pod -o wide^C
root@master:~/app/volume# kkgp -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mydep-hpath-848c6f9f6d-dhzjj   2/2     Running   0          15s   172.17.166.135   node1   <none>           <none>
mydep-hpath-848c6f9f6d-npsgb   2/2     Running   0          15s   172.17.166.136   node1   <none>           <none>
mydep-hpath-848c6f9f6d-wzjk8   2/2     Running   0          15s   172.17.104.61    node2   <none>           <none>
root@master:~/app/volume# kk exec -it mydep-hpath-848c6f9f6d-dhzjj -c con1 -- bash
root@mydep-hpath-848c6f9f6d-dhzjj:/# ls
bin   data  docker-entrypoint.d   etc   lib    media  opt   root  sbin  sys  usr
boot  dev   docker-entrypoint.sh  home  lib64  mnt    proc  run   srv   tmp  var
root@mydep-hpath-848c6f9f6d-dhzjj:/# cd data
root@mydep-hpath-848c6f9f6d-dhzjj:/data# ls
mango.txt
root@mydep-hpath-848c6f9f6d-dhzjj:/data# cat mango.txt
hello this is a mango file..."
root@mydep-hpath-848c6f9f6d-dhzjj:/data#

### if you ssh into the worker node, you will be able to see the data that lies on the hostmachine. 
root@master:~/app/volume# kubectl get node -o wide
NAME     STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
master   Ready    control-plane   46d   v1.28.15   192.168.29.134   <none>        Ubuntu 24.04.1 LTS   6.8.0-49-generic   cri-o://1.28.4
node1    Ready    <none>          45d   v1.28.15   192.168.29.135   <none>        Ubuntu 24.04.1 LTS   6.8.0-49-generic   cri-o://1.28.4
node2    Ready    <none>          43d   v1.28.15   192.168.29.136   <none>        Ubuntu 24.04.1 LTS   6.8.0-49-generic   cri-o://1.28.4
root@master:~/app/volume#

PS C:\Users\mussa> ssh node1@192.168.29.135
root@node1:/# ls -ltr /
...
drwxr-xr-x   2 root root       4096 Jan  3 07:50 appdata
...

root@node1:/# cd appdata/
root@node1:/appdata# ls
mango.txt 
root@node1:/appdata#

PS C:\Users\mussa> ssh node2@192.168.29.136
root@node2:/# cd appdata/
root@node2:/appdata# ls
apple.txt
root@node2:/appdata#

### if I tried deleting file from the container, it was deleted from the hostmachine as well. My question is, how to delete files, without deleting them from 
# hostmachines? 

IMPORTANT
### I am having multiple nodes, I cant login for any reason (lets say I dont have credentials) how can I go and check the issues or data on the worker nodes on 
# k8s ?
### we have a command KUBECTL DEBUG 
### debug container will help you to login into the node and do your debugging for any issues. Not a complete access for the node will be given to you. But enough to debug. 
root@master:~/app/volume# kubectl debug node/node2 -it --image=ubuntu
Creating debugging pod node-debugger-node2-fcqm2 with container debugger on node node2.
If you don't see a command prompt, try pressing enter.
root@node2:/#


=============== DYNAMIC VOLUME PROVISIONING ===============
### Storage Class --> PVC <#of>GB

### https://github.com/rancher/local-path-provisioner
root@master:~/app/volume# kubectl get sc
No resources found
root@master:~/app/volume#
