########## DAEMONSET ##########
# let's suppose you have 3 workder node and I have to deploy application pod in each of the worker node
# till now we deploy the replicas of our pod through replicaController, ReplicaSet, and deployment. 
# Now see that, if you are going with above objects, they will create the replicas but doesnt guarantee that my application will be deployed in each of the worker node
# Let's suppose I am having some of the application which is primarily using for logging and monitoring
# whenever we are trying to deploy some of the logging monitoring application like ELK file beat monitoring applications
# these application should be deployed in each of the worker node, so that we able to collect logs and metrics.
# In this situation, the deployment will not be helping you out! Why?
# it will be guaranteeing you to define the number of replicas, but it will not guaranteeing you that it's replica will be going in each of the worker nodes. 

# the thing is that, I want to deploy the pod replica in each of the worker node. Let's assume that, my pod got deployed in each of the node.
# I have three worker nodes. Somehow, worker node3 is not up and running. The pod will not be getting deployed somewhere. It will be in its place. 
# if I add one more node in cluster, a new pod will be deployed in the new node in the cluster. 
node1
node2
node3

node4 

### IMPORTANT
### whenever we deploy an application, which is required to run on the each of the node. Then we will go for the DaemonSets
### what kind of those application can be? 
### logging and monitoring application, to collect the logs and metrics. Such applications requires to go and deployed in each of the node, so that we can collect information
# from the each of the worker node. It could be logging or metric (cpu or memory) We will go for the DAEMONSET

### what is the need for DaemonSet?
# A DaemonSet is a Kubernetes resource used to ensure that a specific pod is running on all (or some) nodes in a Kubernetes cluster. It plays a crucial role in scenarios where you need to deploy background tasks,
#monitoring agents, log collectors, or other system-level services across every node in the cluster.

### once you are creating DaemonSet, your pod will be getting created in each of the worker node. You dont need to define the number of replicas, it depends on the number of worker nodes.
# you can deploy a daemonset on the master node as well if it is an on-prem server or cop server.
# if it is a cloud kubernetes, you can see master node server on cloudwatch or some other services.
# if some worker node is failing or going down. What will happen is, in such scenerio the pod will node be re-scheduling on other nodes.
# if the worker node is back again, the pod will be back again.
# if a new worker node is connected to cluster, the new DAEMON POD will be deployed and get the logs and metrics. 
# DaemonSets gets deployed based on the worker nodes.

# we will be copying the yaml from the kubernetes documentation below:
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/

root@master:~/app/replication# nano daemonsetforworkernodes.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: con1
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
root@master:~/app/replication#

root@master:~/app/replication# kubectl create -f daemonsetforworkernodes.yaml
daemonset.apps/my-daemonset created
root@master:~/app/replication# kubectl get daemonsets.apps
NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE  <-------- we can specify a particular Node, affinity, apply tait toleration to it as well.
my-daemonset   2         2         0       2            0           <none>          7s
root@master:~/app/replication#

### why one daemon is not deployed in the master node? because there is a TAINT on the master node so that nothing on the master node will be deployed. 
root@master:~/app/replication# kkgp
NAME                          READY   STATUS    RESTARTS   AGE
my-daemonset-l5tn2            1/1     Running   0          21s
my-daemonset-mdfjk            1/1     Running   0          21s
newpod                        2/2     Running   0          2d7h
php-apache-598b474864-pchtx   1/1     Running   0          7h3m
root@master:~/app/replication#

### I want to install the daemonSet in the master node as well, what should I do?
### I need to add the TAINT in the DAEMONSET

root@master:~/app/replication# kk describe nodes master | grep -A 2 "Taint"
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Unschedulable:      false
Lease:
root@master:~/app/replication#

root@master:~/app/replication# nano daemonsetTaintedformasternodes.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      containers:
      - name: con1
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
root@master:~/app/replication#


root@master:~/app/replication# kk create -f daemonsetTaintedTolerationformasternodes.yaml
daemonset.apps/my-daemonset created
root@master:~/app/replication#


root@master:~/app/replication# kkgp -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
my-daemonset-2d4c5            1/1     Running   0          38s     172.17.166.187   node1    <none>           <none>
my-daemonset-4qmdz            1/1     Running   0          38s     172.17.219.74    master   <none>           <none>        <------------------- Now with the toleration, it is deployed on the masterNode as well
my-daemonset-lql94            1/1     Running   0          38s     172.17.104.52    node2    <none>           <none>
newpod                        2/2     Running   0          2d12h   172.17.166.168   node1    <none>           <none>
php-apache-598b474864-pchtx   1/1     Running   0          11h     172.17.166.181   node1    <none>           <none>
root@master:~/app/replication#

### if you want to deploy it on the particular node, you can do it from Node Selector. We have covered that in the Scheduling Section
======================================================================================================================================
################## VOLUME ##################  

Volume Type: 
1. emptyDir - is a temporary volume which can be created to share the data between the containers within the pod. 
2. HostPath - this allows you to access the data from the hostmachine. It is used to host the path of the hostmachine on the pod
3. 

================= EMPTYDIR ================= 

# imagine we have container 1 and 2 in a pod
# how we are connecting one container to another container? 
  # based on the port numbers.
imagine, we have files on con1 and we want to share it on con2
how can we share the these files? We can do that through the emptyDir which can be created through inside the pod. 
you have to mount this emptyDir volume to your container. 
con1
con2

whenever we are deploying applications in kubernetes, we have to follow microservice architecture. Applications needs to be divided in different modules and deployed. 
My application is running in con1. Con1 application is generating some data and I want to forward the logs from con1 to elk or splunk. How are we going to do that?
We are going to install splunk forwarder or logstash, so that I can connect the log forwarder to my splunk or elasticsearch. How is this possible? 
if I am installing multiple applications within the con1, then I am breaking the rule of microservice architecture. Multiple applications should not be deployed in on place. We have to be Monolithic. 
What kind of problems we can face if we install log forwarder inside application container? we can face lack of resources because of both application and log forwarder is running at the same time.
2nd issue can be that your container will be over burdened, because of two applications. To make it lightweight, k8s says that we should only keep two containers in the pod. 
So, we can install another container in pod con1 will be your application and con2 will be your log forwarder. 


============  SIDE CAR CONTAINER ============  
Pod
-----------------------------------------------------------
con1 = app: /app/log
con2 = log-fwdr : /var/log/app 
[emptyDir] = connected to both con1 and con2 within a pod
-----------------------------------------------------------
root@master:~/app# nano emptyDir.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-emp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:  # volumeMount
        - name: myvol
          mountPath: /data  # if the directory isnt present, it will create it
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts: # volumeMount
        - name: myvol
          mountPath: /mydata # if the directory isnt present, it will create it
      volumes:
      - name: myvol
        emptyDir: {}  # volume type
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep-emp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /mydata
      volumes:
      - name: myvol
        emptyDir: {}

root@master:~/app/volume# kubectl create -f EmptyDir.yaml
deployment.apps/mydep-emp created
root@master:~/app/volume# kkgp
NAME                         READY   STATUS              RESTARTS   AGE
mydep-emp-5fb7d76c48-57hvc   2/2     Running             0          3s
mydep-emp-5fb7d76c48-6qzmq   2/2     Running             0          3s
mydep-emp-5fb7d76c48-c5c9b   0/2     ContainerCreating   0          3s

# the important part is that emptyDir scope is within the pod. Meaning that even if you are creating the replicas it will not be having the same data in all the replicas
# data will be varying as per your application. Application in one container will be stored and shared to another container through the emptyDir volume. 

root@master:~/app/volume# kubectl exec -it mydep-emp-5fb7d76c48-57hvc -c con1 -- bash
root@mydep-emp-5fb7d76c48-57hvc:/# ls
bin   data  docker-entrypoint.d   etc   lib    media  opt   root  sbin  sys  usr
boot  dev   docker-entrypoint.sh  home  lib64  mnt    proc  run   srv   tmp  var
root@mydep-emp-5fb7d76c48-57hvc:/#
root@mydep-emp-5fb7d76c48-57hvc:/# cd data
root@mydep-emp-5fb7d76c48-57hvc:/data#

root@mydep-emp-5fb7d76c48-57hvc:/data# touch mango.txt
root@mydep-emp-5fb7d76c48-57hvc:/data# ls
mango.txt
root@mydep-emp-5fb7d76c48-57hvc:/data#

# exit from the above container and login into container2
root@mydep-emp-5fb7d76c48-57hvc:/data# exit
exit
root@master:~/app/volume# kubectl exec -it mydep-emp-5fb7d76c48-57hvc -c con2 -- bash
root@mydep-emp-5fb7d76c48-57hvc:/usr/local/tomcat# cd /mydata/
root@mydep-emp-5fb7d76c48-57hvc:/mydata# ls
mango.txt              <------------------------------------------------ # you can see the file (mango.txt) that you created from the container1 is present in the container2
root@mydep-emp-5fb7d76c48-57hvc:/mydata#


### if you login into the same replica pod on the same node in this case, you can see there is no data. Because the scope is only within the pod not the node dddddddddddddddddddddddddddddddddddddddddddddddddddddddds

root@mydep-emp-5fb7d76c48-57hvc:/mydata# exit
exit
root@master:~/app/volume# kkgp -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mydep-emp-5fb7d76c48-57hvc   2/2     Running   0          6m51s   172.17.166.129   node1   <none>           <none>
mydep-emp-5fb7d76c48-6qzmq   2/2     Running   0          6m51s   172.17.104.58    node2   <none>           <none>
mydep-emp-5fb7d76c48-c5c9b   2/2     Running   0          6m51s   172.17.166.130   node1   <none>           <none>
root@master:~/app/volume#

### remember emptyDir has the scope for within the the same pods. 
root@master:~/app/volume# kk exec -it mydep-emp-5fb7d76c48-c5c9b -c con1 -- bash
root@mydep-emp-5fb7d76c48-c5c9b:/# ls
bin   data  docker-entrypoint.d   etc   lib    media  opt   root  sbin  sys  usr
boot  dev   docker-entrypoint.sh  home  lib64  mnt    proc  run   srv   tmp  var
root@mydep-emp-5fb7d76c48-c5c9b:/# ls data
root@mydep-emp-5fb7d76c48-c5c9b:/# cd data
root@mydep-emp-5fb7d76c48-c5c9b:/data# ls
root@mydep-emp-5fb7d76c48-c5c9b:/data#


IMPORTANT: 
emptyDir is an emphemeral volumes. As soon as the pod is deleted, the volume will be deleted as well. Whatever data you were having in the EmptyDir volume, will be recoverable. 


