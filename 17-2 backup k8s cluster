### taking the backup of the k8s cluster 
root@master:~# kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
mango-57b59fd44-ps8ms   1/1     Running   2          2d19h
root@master:~# kubectl deployment apple --image httpd:latest --port 80 --replicas 3
error: unknown command "deployment" for "kubectl"
root@master:~# kubectl create deployment apple --image httpd:latest --port 80 --replicas 3
deployment.apps/apple created
root@master:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
apple-86876d998f-48jc9   1/1     Running   0          10s
apple-86876d998f-m8tzv   1/1     Running   0          10s
apple-86876d998f-t298d   1/1     Running   0          10s
mango-57b59fd44-ps8ms    1/1     Running   2          2d19h
root@master:~# kubectl scale deployment apple -- replicas 2
error: required flag(s) "replicas" not set
root@master:~# kubectl scale deployment apple --replicas 2
deployment.apps/apple scaled
root@master:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
apple-86876d998f-48jc9   1/1     Running   0          2m14s
apple-86876d998f-m8tzv   1/1     Running   0          2m14s
mango-57b59fd44-ps8ms    1/1     Running   2          2d19h
root@master:~#

### create backup etcd
# if the etcdctl command is not available on the server, it will throw an error. 
example:
ETCDCTL_API=3 etcdctl snnapshot save snapshotdb --endpoints=http://192.168.1.20:2379 --cacert=/etc/Kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/Kubernetes/pki/etcd/server.key

in our case, it should be a private ip address of the master node. Copy it from the aws console. 
ETCDCTL_API=3 etcdctl snnapshot save snapshotdb --endpoints=http://172.31.29.34:2379 --cacert=/etc/Kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/Kubernetes/pki/etcd/server.key


# this is the certificate location and key location which is available on my master node 
# it is needed to authenticate and allow the backup 
--cacert=/etc/kubernetes/pki/etcd/ca.crt 
--key=/etc/kubernetes/pki/etcd/server.key

# to interact with etcd, you have to define the cluster certificate. Because it will be authenticating it and only then it will allow the backup

###Installing ETCDCTL
 root@master:~# apt install etcd-client
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  etcd-client
0 upgraded, 1 newly installed, 0 to remove and 67 not upgraded.
Need to get 5297 kB of archives.
After this operation, 17.7 MB of additional disk space will be used.
Get:1 http://us-east-1.ec2.archive.ubuntu.com/ubuntu noble-updates/universe amd64 etcd-client amd64 3.4.30-1ubuntu0.24.04.2 [5297 kB]
Fetched 5297 kB in 1s (8186 kB/s)
Selecting previously unselected package etcd-client.
(Reading database ... 70744 files and directories currently installed.)
Preparing to unpack .../etcd-client_3.4.30-1ubuntu0.24.04.2_amd64.deb ...
Unpacking etcd-client (3.4.30-1ubuntu0.24.04.2) ...
Setting up etcd-client (3.4.30-1ubuntu0.24.04.2) ...
Processing triggers for man-db (2.12.0-4build2) ...
Scanning processes...
Scanning candidates...
Scanning linux images...

Running kernel seems to be up-to-date.

No services need to be restarted.

No containers need to be restarted.

User sessions running outdated binaries:
 ubuntu @ session #1: sshd[2314]

No VM guests are running outdated hypervisor (qemu) binaries on this host.
root@master:~#

### TROUBLE-SHOOT TIME
#private ip address of the master node in the end point

root@master:~# ETCDCTL_API=3 etcdctl --debug snapshot save snapshotdb --endpoints=http://172.31.29.34:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
ETCDCTL_COMMAND_TIMEOUT=5s
ETCDCTL_DEBUG=true
ETCDCTL_DIAL_TIMEOUT=2s
ETCDCTL_DISCOVERY_SRV=
ETCDCTL_DISCOVERY_SRV_NAME=
ETCDCTL_ENDPOINTS=[http://172.31.29.34:2379]
ETCDCTL_HEX=false
ETCDCTL_INSECURE_DISCOVERY=true
ETCDCTL_INSECURE_SKIP_TLS_VERIFY=false
ETCDCTL_INSECURE_TRANSPORT=true
ETCDCTL_KEEPALIVE_TIME=2s
ETCDCTL_KEEPALIVE_TIMEOUT=6s
ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key
ETCDCTL_PASSWORD=
ETCDCTL_USER=
ETCDCTL_WRITE_OUT=simple
WARNING: 2025/02/06 16:45:41 [core]Adjusting keepalive ping interval to minimum period of 10s
WARNING: 2025/02/06 16:45:41 [core]Adjusting keepalive ping interval to minimum period of 10s
INFO: 2025/02/06 16:45:41 [core]parsed scheme: "etcd-endpoints"
INFO: 2025/02/06 16:45:41 [core]ccResolverWrapper: sending update to cc: {[{172.31.29.34:2379 172.31.29.34:2379 <nil> 0 <nil>}] 0xc00029c360 <nil>}
INFO: 2025/02/06 16:45:41 [core]ClientConn switching balancer to "round_robin"
INFO: 2025/02/06 16:45:41 [core]Channel switches to new LB policy "round_robin"
INFO: 2025/02/06 16:45:41 [balancer]base.baseBalancer: got new ClientConn state: {{[{172.31.29.34:2379 172.31.29.34:2379 <nil> 0 <nil>}] 0xc00029c360 <nil>} <nil>}
INFO: 2025/02/06 16:45:41 [core]Subchannel Connectivity change to CONNECTING
INFO: 2025/02/06 16:45:41 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, CONNECTING
INFO: 2025/02/06 16:45:41 [core]Channel Connectivity change to CONNECTING
INFO: 2025/02/06 16:45:41 [core]Subchannel picks a new address "172.31.29.34:2379" to connect
{"level":"info","ts":1738860341.3422008,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"snapshotdb.part"}
INFO: 2025/02/06 16:45:41 [core]Subchannel Connectivity change to TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:41 [transport]transport: loopyWriter.run returning. connection error: desc = "transport is closing"
INFO: 2025/02/06 16:45:41 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:41 [core]Channel Connectivity change to TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:42 [core]Subchannel Connectivity change to CONNECTING
INFO: 2025/02/06 16:45:42 [core]Subchannel picks a new address "172.31.29.34:2379" to connect
INFO: 2025/02/06 16:45:42 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, CONNECTING
INFO: 2025/02/06 16:45:42 [core]Subchannel Connectivity change to TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:42 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:42 [transport]transport: loopyWriter.run returning. connection error: desc = "transport is closing"
INFO: 2025/02/06 16:45:43 [core]Subchannel Connectivity change to CONNECTING
INFO: 2025/02/06 16:45:43 [core]Subchannel picks a new address "172.31.29.34:2379" to connect
INFO: 2025/02/06 16:45:43 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, CONNECTING
INFO: 2025/02/06 16:45:43 [core]Subchannel Connectivity change to TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:43 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:43 [transport]transport: loopyWriter.run returning. connection error: desc = "transport is closing"
INFO: 2025/02/06 16:45:46 [core]Subchannel Connectivity change to CONNECTING
INFO: 2025/02/06 16:45:46 [core]Subchannel picks a new address "172.31.29.34:2379" to connect
INFO: 2025/02/06 16:45:46 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, CONNECTING
INFO: 2025/02/06 16:45:46 [core]Subchannel Connectivity change to TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:46 [balancer]base.baseBalancer: handle SubConn state change: 0xc0002a42b0, TRANSIENT_FAILURE
INFO: 2025/02/06 16:45:46 [transport]transport: loopyWriter.run returning. connection error: desc = "transport is closing"


################ FIX: instead of the https, I had http in the command:
root@master:~# ETCDCTL_API=3 etcdctl snapshot save snapshotdb --endpoints=https://172.31.29.34:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
{"level":"info","ts":1738867319.6600091,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"snapshotdb.part"}
{"level":"info","ts":"2025-02-06T18:41:59.666017Z","caller":"clientv3/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1738867319.6660562,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://172.31.29.34:2379"}
{"level":"info","ts":"2025-02-06T18:41:59.766545Z","caller":"clientv3/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":1738867319.785996,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://172.31.29.34:2379","size":"7.8 MB","took":0.125918941}
{"level":"info","ts":1738867319.7861328,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"snapshotdb"}
Snapshot saved at snapshotdb
root@master:~# ls
cilium.sh  csr.yaml  history.txt  root.csr  root.key  snap  snapshotdb
root@master:~#

####### i removed the snapshotdb and then did again in the with debug option 
root@master:~# ETCDCTL_API=3 etcdctl --debug snapshot save snapshotdb --endpoints=https://172.31.29.34:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
ETCDCTL_COMMAND_TIMEOUT=5s
ETCDCTL_DEBUG=true
ETCDCTL_DIAL_TIMEOUT=2s
ETCDCTL_DISCOVERY_SRV=
ETCDCTL_DISCOVERY_SRV_NAME=
ETCDCTL_ENDPOINTS=[https://172.31.29.34:2379]
ETCDCTL_HEX=false
ETCDCTL_INSECURE_DISCOVERY=true
ETCDCTL_INSECURE_SKIP_TLS_VERIFY=false
ETCDCTL_INSECURE_TRANSPORT=true
ETCDCTL_KEEPALIVE_TIME=2s
ETCDCTL_KEEPALIVE_TIMEOUT=6s
ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key
ETCDCTL_PASSWORD=
ETCDCTL_USER=
ETCDCTL_WRITE_OUT=simple
WARNING: 2025/02/06 18:50:53 [core]Adjusting keepalive ping interval to minimum period of 10s
WARNING: 2025/02/06 18:50:53 [core]Adjusting keepalive ping interval to minimum period of 10s
INFO: 2025/02/06 18:50:53 [core]parsed scheme: "etcd-endpoints"
INFO: 2025/02/06 18:50:53 [core]ccResolverWrapper: sending update to cc: {[{172.31.29.34:2379 172.31.29.34:2379 <nil> 0 <nil>}] 0xc00020fee0 <nil>}
INFO: 2025/02/06 18:50:53 [core]ClientConn switching balancer to "round_robin"
INFO: 2025/02/06 18:50:53 [core]Channel switches to new LB policy "round_robin"
INFO: 2025/02/06 18:50:53 [balancer]base.baseBalancer: got new ClientConn state: {{[{172.31.29.34:2379 172.31.29.34:2379 <nil> 0 <nil>}] 0xc00020fee0 <nil>} <nil>}
INFO: 2025/02/06 18:50:53 [core]Subchannel Connectivity change to CONNECTING
{"level":"info","ts":1738867853.8474095,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"snapshotdb.part"}
INFO: 2025/02/06 18:50:53 [balancer]base.baseBalancer: handle SubConn state change: 0xc000213830, CONNECTING
INFO: 2025/02/06 18:50:53 [core]Subchannel picks a new address "172.31.29.34:2379" to connect
INFO: 2025/02/06 18:50:53 [core]Channel Connectivity change to CONNECTING
INFO: 2025/02/06 18:50:53 [core]Subchannel Connectivity change to READY
INFO: 2025/02/06 18:50:53 [balancer]base.baseBalancer: handle SubConn state change: 0xc000213830, READY
INFO: 2025/02/06 18:50:53 [roundrobin]roundrobinPicker: newPicker called with info: {map[0xc000213830:{{172.31.29.34:2379 172.31.29.34:2379 <nil> 0 <nil>}}]}
INFO: 2025/02/06 18:50:53 [core]Channel Connectivity change to READY
{"level":"info","ts":"2025-02-06T18:50:53.853036Z","caller":"clientv3/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1738867853.8531032,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://172.31.29.34:2379"}
{"level":"info","ts":"2025-02-06T18:50:53.946635Z","caller":"clientv3/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":1738867853.96987,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://172.31.29.34:2379","size":"7.8 MB","took":0.122379499}
{"level":"info","ts":1738867853.969943,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"snapshotdb"}
INFO: 2025/02/06 18:50:53 [core]Channel Connectivity change to SHUTDOWN
INFO: 2025/02/06 18:50:53 [core]Subchannel Connectivity change to SHUTDOWN
Snapshot saved at snapshotdb
root@master:~# ls
cilium.sh  csr.yaml  history.txt  root.csr  root.key  snap  snapshotdb
root@master:~# ^C
root@master:~#
####################### FIX/DEBUG ENDS

### lets see the running pods and delete then and we will recover it back from the etcd cluster back that we have created.
root@master:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
apple-86876d998f-48jc9   1/1     Running   0          4h45m
apple-86876d998f-m8tzv   1/1     Running   0          4h45m
mango-57b59fd44-ps8ms    1/1     Running   2          3d
root@master:~#
root@master:~#
root@master:~# kubectl delete deployments.apps apple mango
deployment.apps "apple" deleted
deployment.apps "mango" deleted
root@master:~# kubectl get pods
No resources found in default namespace.

### I have the backup with me and I want to restore our environment

root@master:~# ls
cilium.sh  csr.yaml  history.txt  root.csr  root.key  snap  snapshotdb
root@master:~#


# command below is how we are going to use to restore the cluster
# ETCDCTL_API=3 etcdctl restore snapshot --name server --initial-cluster server --endpoints=https://172.31.29.34:2380 --initial-advertise-peers-urls https://https://172.31.29.34:2380 --data-dir /var/lib/etcd-5

# how we know the name of the server? it is defined in the /etc/kubernetes/manifests/etcd.yaml
# you will find this --> --initial-advertise-peers-urls https://https://172.31.29.34:2380 in /etc/kubernetes/manifests/etcd.yaml file
# in our case the name is not server, it is master
root@master:~# vi /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.31.29.34:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://172.31.29.34:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://172.31.29.34:2380
    - --initial-cluster=master=https://172.31.29.34:2380    <--------------------------------------------------
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.31.29.34:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://172.31.29.34:2380
    - --name=master                                          <------------------------------------------------------- this is the name defined in the yaml file, so change your command according to what is the server name defined in the yaml file. 
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
....
  - hostPath:
      path: /var/lib/etcd                                 <------------------------------------- etcd-5 is for this where the data is store kept on machine... if you change it to etcd-1, it will create the file and extract the file from that location.
...
### we are changing the -hostPath in the etcd.yaml file 
...
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-1
      type: DirectoryOrCreate
...

### modify the restore command accordingly
ETCDCTL_API=3 etcdctl restore snapshot --name master --initial-cluster server --endpoints=https://172.31.29.34:2380 --initial-advertise-peers-urls https://https://172.31.29.34:2380 --data-dir /var/lib/etcd-1
root@master:~# ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-1
{"level":"info","ts":1738873748.2377806,"caller":"snapshot/v3_snapshot.go:306","msg":"restoring snapshot","path":"snapshotdb","wal-dir":"/var/lib/etcd-1/member/wal","data-dir":"/var/lib/etcd-1","snap-dir":"/var/lib/etcd-1/member/snap"}
{"level":"info","ts":1738873748.296419,"caller":"mvcc/kvstore.go:388","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":154122}
{"level":"info","ts":1738873748.3082893,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"8d7e874a26730a46","local-member-id":"0","added-peer-id":"4fbad6725f6738fd","added-peer-peer-urls":["https://172.31.29.34:2380"]}
{"level":"info","ts":1738873748.3188345,"caller":"snapshot/v3_snapshot.go:326","msg":"restored snapshot","path":"snapshotdb","wal-dir":"/var/lib/etcd-1/member/wal","data-dir":"/var/lib/etcd-1","snap-dir":"/var/lib/etcd-1/member/snap"}
root@master:~#

### the reason get pods didnt work is because the etcd is being recreated. Till it doesnt get into the success status it will not be created. 
root@master:~# kubectl get pods
No resources found in default namespace.
root@master:~# crictl ps
CONTAINER           IMAGE                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
463619bd87574       a9e7e6b294baf1695fccb862d956c5d3ad8510e1e4ca1535f35dc09f247abbfc   59 minutes ago      Running             etcd                      0                   a07004618791b       etcd-master
e441b0df7fffe       724efdc6b8440d2c78ced040ad90bb8af5553b7ed46439937b567cca86ae5e1b   59 minutes ago      Running             kube-apiserver            2                   ddef10ba9e47e       kube-apiserver-master
fe2077dbe6d28       42b8a40668702c6f34141af8c536b486852dd3b2483c9b50a608d2377da8c8e8   59 minutes ago      Running             kube-scheduler            3                   d773a3bff1f5a       kube-scheduler-master
4494b642d29e3       04dd549807d4487a115aab24e9c53dbb8c711ed9a3b138a206e161800b9975ab   About an hour ago   Running             kube-controller-manager   3                   64a55c71cb8e6       kube-controller-manager-master
0195086a057a1       cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4   6 hours ago         Running             coredns                   0                   ca90902698725       coredns-76f75df574-bsjch
7cb75e29cfa65       f20cf1600da6cce7b7d3fdd3b5ff91243983ea8be3907cccaee1a956770a2f15   6 hours ago         Running             kube-proxy                1                   7cbad53acd9c0       kube-proxy-lpj82
81e06bf8deb71       b38a7071cbb74b7dac0cc0d2538c3e57493271b35cd77ff3cc80e301a34ce51a   6 hours ago         Running             cilium-envoy              2                   4147f04ec5977       cilium-envoy-t26zh
root@master:~#

# if you are using the docker run time, then docker ps command 





### apply the command to restore your cluster.

### see the kube-api etcd is restarted about 17 second and now if you do the kubectl get pods, you will get the pods back. 
CONTAINER           IMAGE                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
3fe75e8cd1ba2       d781bfd0e519b886e895b2253f23aaa958fd0fddb2e6013cabbec79ee3cf775d   17 seconds ago      Running             cilium-agent              18                  449015c8787cf       cilium-c9cd2
463619bd87574       a9e7e6b294baf1695fccb862d956c5d3ad8510e1e4ca1535f35dc09f247abbfc   About an hour ago   Running             etcd                      0                   a07004618791b       etcd-master
e441b0df7fffe       724efdc6b8440d2c78ced040ad90bb8af5553b7ed46439937b567cca86ae5e1b   About an hour ago   Running             kube-apiserver            2                   ddef10ba9e47e       kube-apiserver-master
fe2077dbe6d28       42b8a40668702c6f34141af8c536b486852dd3b2483c9b50a608d2377da8c8e8   About an hour ago   Running             kube-scheduler            3                   d773a3bff1f5a       kube-scheduler-master
4494b642d29e3       04dd549807d4487a115aab24e9c53dbb8c711ed9a3b138a206e161800b9975ab   About an hour ago   Running             kube-controller-manager   3                   64a55c71cb8e6       kube-controller-manager-master
0195086a057a1       cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4   6 hours ago         Running             coredns                   0                   ca90902698725       coredns-76f75df574-bsjch
7cb75e29cfa65       f20cf1600da6cce7b7d3fdd3b5ff91243983ea8be3907cccaee1a956770a2f15   6 hours ago         Running             kube-proxy                1                   7cbad53acd9c0       kube-proxy-lpj82
81e06bf8deb71       b38a7071cbb74b7dac0cc0d2538c3e57493271b35cd77ff3cc80e301a34ce51a   6 hours ago         Running             cilium-envoy              2                   4147f04ec5977       cilium-envoy-t26zh
root@master:~#

#################### TROUBLE SHOOT TIME: after crictl ps containers are not recreating and the backup didnt restored

root@master:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
apple-86876d998f-68gp7   0/1     Pending   0          71s
apple-86876d998f-cczgv   0/1     Pending   0          71s
apple-86876d998f-jcjw6   0/1     Pending   0          71s
root@master:~#

root@master:~# kubectl describe pods apple
...
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  4s (x13 over 100s)  default-scheduler  no nodes available to schedule pods

root@master:~#
...

root@master:~# kubectl get nodes -o wide
No resources found
root@master:~#

root@master:~# kubeadm reset
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0206 21:09:25.270227   11841 reset.go:124] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: configmaps "kubeadm-config" is forbidden: User "kubernetes-admin" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0206 21:09:25.270308   11841 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0206 21:09:27.109962   11841 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd-1
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
W0206 21:09:58.268035   11841 cleanupnode.go:99] [reset] Failed to remove containers: failed to stop running pod ca90902698725c064b539dad551b74ab4df04092af9e418d924608cb7f83ede5: output: E0206 21:09:37.495902   11928 remote_runtime.go:222] "StopPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" podSandboxID="ca90902698725c064b539dad551b74ab4df04092af9e418d924608cb7f83ede5"
time="2025-02-06T21:09:37Z" level=fatal msg="stopping the pod sandbox \"ca90902698725c064b539dad551b74ab4df04092af9e418d924608cb7f83ede5\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
: exit status 1
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
root@master:~#

root@master:~# kubectl get nodes
The connection to the server 172.31.29.34:6443 was refused - did you specify the right host or port?
root@master:~#   mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
cp: cannot stat '/etc/kubernetes/admin.conf': No such file or directory
root@master:~# mkdir -p $HOME/.kube
root@master:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
cp: cannot stat '/etc/kubernetes/admin.conf': No such file or directory
root@master:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config
root@master:~# kubeadm init
I0206 21:11:49.853100   12245 version.go:256] remote version is much newer: v1.32.1; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.13
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
root@master:~# rm -rf /var/lib/etcd/
root@master:~# kubeadm init
I0206 21:12:06.727276   12276 version.go:256] remote version is much newer: v1.32.1; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.13
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 172.31.29.34]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [172.31.29.34 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [172.31.29.34 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 5.504068 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: fv3pf4.f2kj93fvn6owg7mc
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.29.34:6443 --token fv3pf4.f2kj93fvn6owg7mc \
        --discovery-token-ca-cert-hash sha256:5142d1dbd6b55117cb18c5591cb30dbb089749ebf5bf42f3338b20719e0ef04e
root@master:~#

# join the worker node
root@node1:~# kubeadm join 172.31.29.34:6443 --token fv3pf4.f2kj93fvn6owg7mc         --discovery-token-ca-cert-hash sha256:5142d1dbd6b55117cb18c5591cb30dbb089749ebf5bf42f3338b20719e0ef04e
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check 

root@node1:~# lsof -i :10250
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
kubelet 1005 root   17u  IPv6   8027      0t0  TCP *:10250 (LISTEN)
root@node1:~#

root@node1:~# netstat -tuln | grep :10250
tcp6       0      0 :::10250                :::*                    LISTEN
root@node1:~#

# or if this still is bothering you, on worker node do the kubeadm reset. 
# and then join the worker node to the master node through the join command. 

############## FIX: I just repeated the steps, instead of deleting the etcd-1, vi /etc/kubernetes/manifest/etcd.yaml file and change the path to etcd-2 or 3 or 4 etc and keep on rebuilding it. 
  453  vi /etc/kubernetes/manifests/etcd.yaml
  454  ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-3
  455  kubectl get pods
  456  clear
  457  history
  458  kubectl get rolebindings --all-namespaces kubectl create clusterrolebinding kubernetes-admin-binding --clusterrole=cluster-admin --user=kubernetes-admin
  459  kubectl get pods
  460  kubectl delete deployments.apps apple
  461  history
root@master:~# !454
ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-3
Error: data-dir "/var/lib/etcd-3" exists
root@master:~# ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-^C
root@master:~# ^C
root@master:~# ^C
root@master:~# !443
vi /etc/kubernetes/manifests/etcd.yaml
root@master:~# ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-4
Error: data-dir "/var/lib/etcd-4" exists
root@master:~# kubectl get pods
^C
root@master:~# vi /etc/kubernetes/manifests/etcd.yaml
root@master:~# ETCDCTL_API=3 etcdctl snapshot restore snapshotdb --name master --initial-cluster master=https://172.31.29.34:2380 --initial-advertise-peer-urls https://172.31.29.34:2380 --data-dir /var/lib/etcd-5
{"level":"info","ts":1738879682.4637136,"caller":"snapshot/v3_snapshot.go:306","msg":"restoring snapshot","path":"snapshotdb","wal-dir":"/var/lib/etcd-5/member/wal","data-dir":"/var/lib/etcd-5","snap-dir":"/var/lib/etcd-5/member/snap"}
{"level":"info","ts":1738879682.5185933,"caller":"mvcc/kvstore.go:388","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":2545}
{"level":"info","ts":1738879682.5281403,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"8d7e874a26730a46","local-member-id":"0","added-peer-id":"4fbad6725f6738fd","added-peer-peer-urls":["https://172.31.29.34:2380"]}
{"level":"info","ts":1738879682.5438771,"caller":"snapshot/v3_snapshot.go:326","msg":"restored snapshot","path":"snapshotdb","wal-dir":"/var/lib/etcd-5/member/wal","data-dir":"/var/lib/etcd-5","snap-dir":"/var/lib/etcd-5/member/snap"}


# ETCDCTL_API=3 etcdctl get /registry/secrets/default/testing-two --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key



