############ STATEFULSET ############ 

assume that we have a deployment 
Deploymenet
my-dep 
  dep1.  mydep-qwe098
  dep2.  mydep-xyz12
           pod1. mydep-xyz12-a23
           pod2. mydep-xyz12-a23
           pod3. mydep-qwe098-abc125

Assume that we have 2 deployment running and in those there are three pods running in it. If I am in pod1 how can I access the pod2 while being into the pod1?
how we can communicate? how this communication will happen? We need to create a service. e.g a ClusterIP Service. Through ClusterIP, I can communicate. 
I will be using the service name as a hostname and I will be communicating to mydep-svc, mydep-svc can be used as the hostname. Randomy, any pod will be getting the 
traffic but if I want to reach out to a specific pod, I cannot do that through the deployment. Specific pods are not allowed to access in deployment.

Deployment creates c pod with a dynamic id's. Deployment will not be having static pod name, everytime a pod with a new name will be created if there's a pod failure or 
node failiure. e.g:

root@master:~/app/volume# kubectl create deployment mydep --image nginx --port 80 --replicas 3
deployment.apps/mydep created
root@master:~/app/volume# kkgp
NAME                     READY   STATUS              RESTARTS   AGE
mydep-58c68db89f-cvvtj   0/1     ContainerCreating   0          1s
mydep-58c68db89f-d4cgm   0/1     ContainerCreating   0          1s
mydep-58c68db89f-w6gjg   0/1     ContainerCreating   0          1s

### see the naming convention, deploymentname, replicasetid, and podid.
# let's suppose somehow, this pod got deleted 
root@master:~/app/volume# kkgp
NAME                     READY   STATUS    RESTARTS   AGE
mydep-58c68db89f-cvvtj   1/1     Running   0          15s
mydep-58c68db89f-d4cgm   1/1     Running   0          15s
mydep-58c68db89f-w6gjg   1/1     Running   0          15s
root@master:~/app/volume#

### this is not having a static naming with the pod
# see that a new pod was created with a new id
root@master:~/app/volume# kubectl delete pod mydep-58c68db89f-cvvtj
pod "mydep-58c68db89f-cvvtj" deleted
kkroot@master:~/app/volume# kkgp
NAME                     READY   STATUS    RESTARTS   AGE
mydep-58c68db89f-d4cgm   1/1     Running   0          2m7s
mydep-58c68db89f-h87hn   1/1     Running   0          2s
mydep-58c68db89f-w6gjg   1/1     Running   0          2m7s
root@master:~/app/volume#

Example through database:
Q: why we need to reach out to a specific pod, while each pods are serving the request through the ClusterIP? 
whenever we are creating a database cluster to achieve the highly availability, you will create a multiple nodes. There you will see the concept of Master and Slave nodes
We can assume that we have:
Masternode 192.xxx.xxx.xxx
slave1
slave2

Database highlyAvailability how it works, you know that there are some of the database we are having some of the operations. 
Create 
Delete 
Updata
Such queries will be perfomed on the masterNode. And each of the node will have the ip address, based on the ip address we will be accessing the nodes. 
So, whenever you want to perform queries, you will have to do that through masterNodes. 
When it comes to read the data, you can read through anyone of the nodes. So whenever you have to perform operations you have to do through master node and 
you can do the reading through any node. 

# you can keep simple cluster of mentioned above but if you are having a huge operations of database and clusters then keep that cluster outside of k8s
# and then connect that cluster of db while being inside the k8s
StatefulSet is the option that will be allowing you to reach out to a specific pod and you can make that pod a master and another to the slave.
and the pod connection wil be static in StatefulSet. 

all these pod will have the static naming convention. 
pod1 mydb-0 if pod is getting deleted from the statefulset, then the new pod that will be created will have the same name
pod2 mydb-1
pod3 mydb-2

IMPORTANT
You need to create a service called HEADLESS SERVICE (headless meaning NO IP ) service ClusterIP meaning based on the pod name you can reach out to it. 
We can use the ClusterIP with Headless Service, so that you can perform the read operations from any one of the random nodes 
Headless to reach out to any pod
Cluster to read opeartions from any node.

Stateful set comes up with the additional or advance options which is called VOLUMETEMPLATES
If you are defining the volumetemplate, you dont need to create the pvc for each of the pod. StatefulSet will automatically create a seperate pvc for each of the pods. 
Your pod will be getting a seperate volume created to store or to persist the data on your cluster. 

Coming back to the DB example,
We are performing these options Create Delete Update on master node in our DB cluster. Then, how the data is being replicated from master to the slave?
we have a tool called Percona or extraBackup. We have to install/configure these tools on each of the nodes so that the software will be replicating the data
from master to slave.

Percona needs to be added/configured in our statefulSet pods so that the data can be replicated from the masternode to the worker nodes.  

# We will look into 3 things: 
# We will use the Killercoda for this implementation.
StatefulSets
Pod naming convention
Volume template

We are copying the yaml file from the kubernetes docs
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/

controlplane $ cat statefulset.yaml 
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx" #So this serviceName is the name of your HEALDESS SERVICE
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.24
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates: # this will create the volume claims
  - metadata:
      name: www
    spec: ### similar to pvc if you see below
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local-path" #"my-storage-class" # to get the storage class kubectl get sc
      resources:
        requests:
          storage: 1Gi # each size of the storage will be 1Gi for each replicas

# get the storage class name and plug it into the yaml file at the storageClassName field. 
# in our case the storageClassName is local-path
controlplane $ kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  3d12h
controlplane $ 

controlplane $ kubectl create -f statefulset.yaml 
statefulset.apps/web created
controlplane $ kubectl get statefulsets.apps  
NAME   READY   AGE
web    0/3     8s

# as you see all of them arent getting creating at the same time, first it will create first pod successfull, then it will proceed to next and so on. 
controlplane $ kubectl get statefulsets.apps
NAME   READY   AGE
web    1/3     18s
controlplane $ 

controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          106s
web-1   1/1     Running   0          89s
web-2   1/1     Running   0          70s
controlplane $ 

### we defined www-web in our volume template 
controlplane $ kubectl get pvc 
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
www-web-0   Bound    pvc-d8f66ae9-bd37-4021-9a9b-8d5b5d952d6e   1Gi        RWO            local-path     <unset>                 2m10s
www-web-1   Bound    pvc-39dfb163-5ef9-4585-8109-635551870713   1Gi        RWO            local-path     <unset>                 113s
www-web-2   Bound    pvc-c348110b-29a4-499e-9296-79fe1fb8db66   1Gi        RWO            local-path     <unset>                 94s
controlplane $ 

### if you will scale up the statefulset, the new volumes will be created as well of 1Gi that we have defined out manifest file. 
controlplane $ kubectl scale statefulset web --replicas 5
statefulset.apps/web scaled

controlplane $ kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
www-web-0   Bound    pvc-d8f66ae9-bd37-4021-9a9b-8d5b5d952d6e   1Gi        RWO            local-path     <unset>                 5m26s
www-web-1   Bound    pvc-39dfb163-5ef9-4585-8109-635551870713   1Gi        RWO            local-path     <unset>                 5m9s
www-web-2   Bound    pvc-c348110b-29a4-499e-9296-79fe1fb8db66   1Gi        RWO            local-path     <unset>                 4m50s
www-web-3   Bound    pvc-2f71d2d4-bb64-452d-bdd0-6ede548fd164   1Gi        RWO            local-path     <unset>                 12s
www-web-4   Bound    pvc-81ae61b8-51ea-4ae7-991b-d24a24e0f0bb   1Gi        RWO            local-path     <unset>                 5s

### if we scale it down, the pods will be deleted but the volume will not be deleted. 
controlplane $ kubectl scale statefulset web --replicas 3
statefulset.apps/web scaled
controlplane $ kubectl get pvc 
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
www-web-0   Bound    pvc-d8f66ae9-bd37-4021-9a9b-8d5b5d952d6e   1Gi        RWO            local-path     <unset>                 7m39s
www-web-1   Bound    pvc-39dfb163-5ef9-4585-8109-635551870713   1Gi        RWO            local-path     <unset>                 7m22s
www-web-2   Bound    pvc-c348110b-29a4-499e-9296-79fe1fb8db66   1Gi        RWO            local-path     <unset>                 7m3s
www-web-3   Bound    pvc-2f71d2d4-bb64-452d-bdd0-6ede548fd164   1Gi        RWO            local-path     <unset>                 2m25s
www-web-4   Bound    pvc-81ae61b8-51ea-4ae7-991b-d24a24e0f0bb   1Gi        RWO            local-path     <unset>                 2m18s
controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          7m44s
web-1   1/1     Running   0          7m27s
web-2   1/1     Running   0          7m8s
controlplane $ 

# Lets assume that your pod were deleted the web-0, what will happen is that the same pod will spin up with the same name due to the fact
# these pods are created with the statefulSet. Name will not be changed and whatever volume this pod use to have, the same volume will be attached to your pod.
# no new volume will be created.

controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          7m44s
web-1   1/1     Running   0          7m27s
web-2   1/1     Running   0          7m8s
controlplane $ kubectl delete pod web-0
pod "web-0" deleted
controlplane $  kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          49s  <--------------
web-1   1/1     Running   0          9m24s
web-2   1/1     Running   0          9m5s
controlplane $ 
controlplane $ kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
www-web-0   Bound    pvc-d8f66ae9-bd37-4021-9a9b-8d5b5d952d6e   1Gi        RWO            local-path     <unset>                 11m
www-web-1   Bound    pvc-39dfb163-5ef9-4585-8109-635551870713   1Gi        RWO            local-path     <unset>                 11m
www-web-2   Bound    pvc-c348110b-29a4-499e-9296-79fe1fb8db66   1Gi        RWO            local-path     <unset>                 11m
www-web-3   Bound    pvc-2f71d2d4-bb64-452d-bdd0-6ede548fd164   1Gi        RWO            local-path     <unset>                 6m39s
www-web-4   Bound    pvc-81ae61b8-51ea-4ae7-991b-d24a24e0f0bb   1Gi        RWO            local-path     <unset>                 6m32s
controlplane $ 

controlplane $ nano statefulSet-headlessservice.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx ### this is the serviceName we have defined in StatefulSet.yaml file. This is nothing but a headless service. 
spec:
  type: ClusterIP
  clusterIP: None # because of this configuration, your clusterIP service will not getting ip address, which will be making it headless service (without ip)                             
  selector:
    app: nginx 
  ports:
  - port: 80
    name: web

### you see that after creating this headless service, the nginx ClusterIP service get none ip address. 
controlplane $ kubectl create -f statefulSet-headlessservice.yaml 
service/nginx created
controlplane $ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d12h
nginx        ClusterIP   None         <none>        80/TCP    4s  <--------------- no ip address due to the headless service 
controlplane $ 


### I want to reach out to the web-0 pod, how can i reach to the web0 pod? 
# lets create a simple pod and login inside it  
controlplane $ kubectl run test --image nginx 
pod/test created
controlplane $ kubectl exec -it test -- bash
root@test:/# 

### how to interact with to another pod? we have a headless clusterIP service 
controlplane $ kubectl exec -it test -- bash
root@test:/# apt update && apt install dnsutils -y
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB].......

### get the the name of the pod
root@test:/# exit
exit
command terminated with exit code 130
controlplane $ kubectl get pod -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP             NODE           NOMINATED NODE   READINESS GATES
test    1/1     Running   0          3m19s   192.168.1.11   node01         <none>           <none>
web-0   1/1     Running   0          16m     192.168.1.10   node01         <none>           <none>
web-1   1/1     Running   0          24m     192.168.0.5    controlplane   <none>           <none>
web-2   1/1     Running   0          24m     192.168.1.7    node01         <none>           <none>
controlplane $ 

### if I nslookup to my service it will show the ip addresses of the pod
controlplane $ kubectl exec -it test -- bash
root@test:/# nslookup nginx
;; Got recursion not available from 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   nginx.default.svc.cluster.local
Address: 192.168.1.10
Name:   nginx.default.svc.cluster.local
Address: 192.168.0.5
Name:   nginx.default.svc.cluster.local
Address: 192.168.1.7
;; Got recursion not available from 10.96.0.10
controlplane $ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d13h
nginx        ClusterIP   None         <none>        80/TCP    8m7s

controlplane $ kubectl describe svc nginx
Name:                     nginx
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=nginx
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       None
IPs:                      None
Port:                     web  80/TCP
TargetPort:               80/TCP
Endpoints:                192.168.1.10:80,192.168.0.5:80,192.168.1.7:80
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
controlplane $ 
root@test:/# 

# my requirement is to reach to the web-0 pod, before reaching to this pod see the service fqdn or the dns address of the service
# Name:   nginx.default.svc.cluster.local 
# nginx is the service name
# default is the namespace
# svc object type is service
# clusteer.local is your domain

### now how can I reach to the pod web-o
root@test:/# ns lookup web-0.nginx.default.svc.cluster.local
bash: ns: command not found
root@test:/# nslookup web-0.nginx.default.svc.cluster.local
;; Got recursion not available from 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   web-0.nginx.default.svc.cluster.local
Address: 192.168.1.10  <--------------------------
;; Got recursion not available from 10.96.0.10

root@test:/# 

### a good homework to deploy the statefulset for mysql which will include the headless service as well. 
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/

------------------------------------------------------------------------------------------------------------------------------------------

############ INIT CONTAINER ############
root@master:~/app/volume# cp emptyDir.yaml init-container.yaml
root@master:~/app/volume#

### what is init container?
lets say I want to create a pod, my pod needs some configuration to be ready before main container startsup 
Init container will be started first of all before the main container. It will be configuring some of the settings for you, once the configuration is done
then only your main container will be started. You can have multiple init container but we are going to use a simple init container. 

kind: Deployment
metadata:
  name: mydep-emp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec: 
      initContainers:
      - name: icon1
        image: ubuntu:latest
        command: ["/bin/bash", "-c","echo '<h1> Hello from init container</h1>' > /data/index.html",]
        volumeMounts:
        - name: myvol
          mountPath: /data
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /usr/share/nginx/html
      volumes:
      - name: myvol
        emptyDir: {}

root@master:~/app/volume# kubectl create -f init-container.yaml
deployment.apps/mydep-emp created
root@master:~/app/volume# kkgp
NAME                        READY   STATUS     RESTARTS   AGE
mydep-emp-5876dd77d-bm5hc   0/1     Init:0/1   0          2s
mydep-emp-5876dd77d-r75kq   0/1     Init:0/1   0          2s
mydep-emp-5876dd77d-xq79t   0/1     Init:0/1   0          2s
root@master:~/app/volume# kkgp
NAME                        READY   STATUS     RESTARTS   AGE
mydep-emp-5876dd77d-bm5hc   0/1     Init:0/1   0          2s
mydep-emp-5876dd77d-r75kq   0/1     Init:0/1   0          2s
mydep-emp-5876dd77d-xq79t   0/1     Init:0/1   0          2s
root@master:~/app/volume# kkgp
NAME                        READY   STATUS            RESTARTS   AGE
mydep-emp-5876dd77d-bm5hc   0/1     PodInitializing   0          3s
mydep-emp-5876dd77d-r75kq   0/1     PodInitializing   0          3s
mydep-emp-5876dd77d-xq79t   0/1     Init:0/1          0          3s
root@master:~/app/volume# kkgp
NAME                        READY   STATUS            RESTARTS   AGE
mydep-emp-5876dd77d-bm5hc   1/1     Running           0          4s
mydep-emp-5876dd77d-r75kq   1/1     Running           0          4s
mydep-emp-5876dd77d-xq79t   0/1     PodInitializing   0          4s
root@master:~/app/volume# kkgp
NAME                        READY   STATUS    RESTARTS   AGE
mydep-emp-5876dd77d-bm5hc   1/1     Running   0          5s
mydep-emp-5876dd77d-r75kq   1/1     Running   0          5s
mydep-emp-5876dd77d-xq79t   1/1     Running   0          5s
root@master:~/app/volume#

### see where I mounted that volume 
root@master:~/app/volume# kubectl exec -it mydep-emp-5876dd77d-bm5hc -- bash
Defaulted container "con1" out of: con1, icon1 (init)
root@mydep-emp-5876dd77d-bm5hc:/# cd /usr/share/nginx/html/
root@mydep-emp-5876dd77d-bm5hc:/usr/share/nginx/html# ls
index.html
root@mydep-emp-5876dd77d-bm5hc:/usr/share/nginx/html#
root@mydep-emp-5876dd77d-bm5hc:/usr/share/nginx/html# cat index.html
<h1> Hello from init container</h1>
root@mydep-emp-5876dd77d-bm5hc:/usr/share/nginx/html#

### we can see this through our browser as well, we just need to expose the pod to the service.
root@mydep-emp-5876dd77d-bm5hc:/usr/share/nginx/html# exit
exit
root@master:~/app/volume# kubectl expose deployment mydep-emp --port 80 --type NodePort
service/mydep-emp exposed
root@master:~/app/volume# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        49d
mydep        NodePort    10.110.18.118   <none>        80:32412/TCP   5d12h
mydep-emp    NodePort    10.103.46.57    <none>        80:30993/TCP   4s    <---------------- this is your service which is accessible on the port 30993
newpod       ClusterIP   10.102.85.76    <none>        80/TCP         9d
root@master:~/app/volume#

# look the the endpoints all the pod ip addresses are there
root@master:~/app/volume# kk describe svc mydep-emp
Name:                     mydep-emp
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=myapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.103.46.57
IPs:                      10.103.46.57
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30993/TCP
Endpoints:                172.17.104.8:80,172.17.166.146:80,172.17.166.147:80  <---------------- pod ip address on port 80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
## look at the ip below and at the Endpoints above matches. 
root@master:~/app/volume# kk get pod -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
mydep-emp-5876dd77d-bm5hc   1/1     Running   0          6m13s   172.17.104.8     node2   <none>           <none>
mydep-emp-5876dd77d-r75kq   1/1     Running   0          6m13s   172.17.166.147   node1   <none>           <none>
mydep-emp-5876dd77d-xq79t   1/1     Running   0          6m13s   172.17.166.146   node1   <none>           <none>
root@master:~/app/volume#


Get the ip address of the any node and put it in the browser with the port 80 is mapped to on the port on host machine which is 30993
root@master:~/app/volume# ifconfig
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.29.134  netmask 255.255.255.0  broadcast 192.168.29.255
        inet6 fe80::20c:29ff:fe61:8e3f  prefixlen 64  scopeid 0x20<link>

IMPORTANT:
#### this file has been placed in your main container by INIT CONTAINER
=====ON BROWSWER=====
http://192.168.29.134:30993/
Hello from init container

### INITCONTAINER is nothing but a container that will be started before your main container. Which initates the steps that you have configured in your manifest file.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

############## STATIC POD ############## 
# clean your space before we begin our practice. 
root@master:~/app/volume# kubectl delete -f init-container.yaml
deployment.apps "mydep-emp" deleted
root@master:~/app/volume# kkgp
No resources found in default namespace.
root@master:~/app/volume# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        49d
mydep        NodePort    10.110.18.118   <none>        80:32412/TCP   5d12h
mydep-emp    NodePort    10.103.46.57    <none>        80:30993/TCP   9m9s
newpod       ClusterIP   10.102.85.76    <none>        80/TCP         9d
root@master:~/app/volume# kubectl delete svc mydep mydep-emp newpod
service "mydep" deleted
service "mydep-emp" deleted
service "newpod" deleted
root@master:~/app/volume# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   49d
root@master:~/app/volume#


### we have few of the pods which are already running on cluster as static pods
### how to identify if those pods are static? 
root@master:~/app/volume# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS         AGE
calico-kube-controllers-787f445f84-gdc7t   1/1     Running   5                46d
calico-node-mrm6j                          1/1     Running   4                46d
calico-node-vhd49                          1/1     Running   5                46d
calico-node-x4q65                          1/1     Running   4                46d
coredns-5dd5756b68-84w4t                   1/1     Running   8                49d
coredns-5dd5756b68-m64mp                   1/1     Running   8                49d
etcd-master                                1/1     Running   8                49d    <---------------- suffix is master so this is a static pod
kube-apiserver-master                      1/1     Running   8                49d    <---------------- suffix is master so this is a static pod 
kube-controller-manager-master             1/1     Running   12 (7d6h ago)    49d    <---------------- suffix is master so this is a static pod
kube-proxy-k4pzl                           1/1     Running   8                49d
kube-proxy-k88rh                           1/1     Running   5                47d
kube-proxy-sr6kt                           1/1     Running   5                46d
kube-scheduler-master                      1/1     Running   14 (3d10h ago)   49d    <---------------- suffix is master so this is a static pod 
metrics-server-58ff449c8d-bch5w            1/1     Running   0                6d2h
root@master:~/app/volume#

### what is the hostname of our machine? master in our case
### anywhere you see the master as the suffix in podname, those are your static pods. 
root@master:~/app/volume# hostname
master
root@master:~/app/volume#

### static pods are nothing which is created on your node directory through kubernetes. 
### so this file you have to place in the kubernetes manifest location.
### the location for all default manifest are kept : /etc/kubernetes/manifests/

root@master:~/app# nano static-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: apppod
  labels:
    app: myapp
    env: dev
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80

root@master:~/app# ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
root@master:~/app#

### right now the static-pod.yaml location where it has been placed is at the /app. As soon as you move/copy the static-pod.yaml manifest file
### to the /etc/kubernetes/manifests/
### it will it will create the static pod for you with the suffix -master at the end of your pod name.  

root@master:~/app# cp static-pod.yaml /etc/kubernetes/manifests/

NAME            READY   STATUS    RESTARTS   AGE
apppod-master   1/1     Running   0          77s
root@master:~/app#

### IMPORTANT: 
### you cannot delete this pod by itself, it will recreate itself. 
root@master:~/app# kubectl delete po apppod-master
pod "apppod-master" deleted
root@master:~/app# kkgp
NAME            READY   STATUS    RESTARTS   AGE
apppod-master   1/1     Running   0          10s

### to remove this pod, you will have to remove the yaml file from the /etc/kubernetes/manifests/ location 
### as soon as you remove the manifest file from the location, it will be deleted. 
root@master:~/app# rm /etc/kubernetes/manifests/static-pod.yaml
root@master:~/app# kkgp
No resources found in default namespace.
root@master:~/app#

INTERVIEW QUESTION:
### what is the use case of the static pod? 
### the use case of the static pod is when you are creating a system configuration that you want to create and that shouldnot be deleted anyway, then you will
### have to create the static pod.

### if I want to create a static pod in the worker node?
### logon to the worker node and do the same thing, place the manifest file within the location of the worker node and you should have a static pod created. 
