
############## RESOURCE LIMITATION ##############
# Whenever we are creating a pod with one container or multiple container as well. The containers are getting deployed on the node. The container will not having any restrictions
# there are two types of restrictions/limit resource usage on our container based on the container.
# based on namespace also we can have limitation for resource accessibilities cpu or memory that will be covered under RBAC
root@master:~/app/scheduling# kubectl get namespaces
NAME              STATUS   AGE
default           Active   38d
kube-node-lease   Active   38d
kube-public       Active   38d
kube-system       Active   38d
root@master:~/app/scheduling#

### How to define the resource limit on our container

# untain the node1 so that it is allowed to accept the pod deployment without the taint effect
root@master:~/app/scheduling# kkdn node1|grep "Taint"
root@master:~/app/scheduling# kkdn node1|grep "Taint"
Taints:             color=green:NoExecute
root@master:~/app/scheduling# kk taint node node1 color-
node/node1 untainted
root@master:~/app/scheduling# kkdn node1|grep "Taint"
Taints:             <none>
root@master:~/app/scheduling#

# if the pods/deployments are stucked you can also delete them by force "becareful whenever you delete"
root@master:~/app/scheduling# kkgp
NAME                      READY   STATUS        RESTARTS   AGE
mydep-2-54f8467d9-5zvv2   0/1     Terminating   0          4h15m
mydep-2-54f8467d9-z4gwn   0/1     Terminating   0          4h15m
mydep-66c55fb688-8psfk    0/1     Terminating   0          4h4m
mydep-66c55fb688-9x2h9    0/1     Terminating   0          4h21m
mydep-66c55fb688-hwwwt    0/1     Terminating   0          4h21m
mydep-66c55fb688-jztjn    0/1     Terminating   0          4h4m
newpod                    2/2     Running       0          6m25s

root@master:~/app/scheduling# kk delete pod --force mydep-2-54f8467d9-z4gwn mydep-66c55fb688-8psfk mydep-66c55fb688-9x2h9 mydep-66c55fb688-hwwwt mydep-66c55fb688-jztjn
Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "mydep-2-54f8467d9-z4gwn" force deleted
pod "mydep-66c55fb688-8psfk" force deleted
pod "mydep-66c55fb688-9x2h9" force deleted
pod "mydep-66c55fb688-hwwwt" force deleted
pod "mydep-66c55fb688-jztjn" force deleted
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkgp
NAME     READY   STATUS    RESTARTS   AGE
newpod   2/2     Running   0          15m
root@master:~/app/scheduling#


root@master:~/app/scheduling# cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/scheduling# kk create -f pod.yaml
pod/newpod created
root@master:~/app/scheduling# kkgp
NAME     READY   STATUS    RESTARTS   AGE
newpod   2/2     Running   0          4s
root@master:~/app/scheduling#

### if you see there is no resource limit (minimum or maximum) are configured.  
### This is not a good practice to create a pod/deployment in a kubernetes cluster
# assume that your container is resource hungry, it will consume all the resources from the particular node wherever it gets deployed. Which will cause issues with other pod; where other pods will also need resroucs.
  con2:
    Container ID:   cri-o://7856d6a6ebcf9c4d0565ece7f60e0d5acd1479cc8a85c973ff050f9b18c35927
    Image:          tomcat:latest
    Image ID:       docker.io/library/tomcat@sha256:59117505ccf7033bd67a595cf02dbaac3a032468385385acbb6246a4531a2e80
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 26 Dec 2024 14:42:04 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5rfl8 (ro)
Conditions:

### DEFINE THE RESROUCE ###
root@master:~/app/scheduling# nano resroucerestriction.yaml

apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/scheduling# kubectl create -f resourcelimitation.yaml
pod/newpod created

root@master:~/app/scheduling# kkgp
NAME     READY   STATUS    RESTARTS   AGE
newpod   2/2     Running   0          2m26s
root@master:~/app/scheduling#

### check the con1 has a limitation set for resource accessibility
root@master:~/app/scheduling# kkdp newpod

...
Containers:
  con1:
    Container ID:   cri-o://7acc4b394fb4d123eb489c83d650051dbf070d470ec233ff751f3e3edf969268
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:42e917aaa1b5bb40dd0f6f7f4f857490ac7747d7ef73b391c774a41a8b994f15
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 07:11:14 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lkqj6 (ro)
  con2:
    Container ID:   cri-o://03dcf3038b7f497f73b70b3f2f11bc27eb6d7a8c7876d20c59e84e488eee9e28
    Image:          tomcat:latest
    Image ID:       docker.io/library/tomcat@sha256:59117505ccf7033bd67a595cf02dbaac3a032468385385acbb6246a4531a2e80
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 07:11:14 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
...

############## RESOURCE LIMITATION ENDS ##############

############## HEALTH CHECK ##############
### health check of your containers
### two types of probes are available
=== healthcheck probs ===
1. LivenessProbe - if liveness probe is getting failed, then your container will be killed and recreated.
      #during the killing and recreating process, if some user is requesting the data. Surely, it will be routing/forwarding the request to container which is currently not available.
      # the drawback of this is that the user/application users will be able to see the failed errors on the screen.
 LivenessProbe CheckTypes: - e.g http, exec, tcpSocket...
      # http: when to use http probe? when there is a web based server based application. Which could be nginx, apache, and etc 
      # tcpSocket: if you are using non-web based server application, you can then check the health of the container by checking the container port number through tcpSocket e.g for mysql you can check 3306 port number
      # exec: if you are not using web application or any port number. Like you doing a backof the cluster or performing a scheduled job, at the time no http or tcpSocket port will be availabe. You can execute the command. If the command is success the probe is passed and if it is not success then it will fail.
              the container will check 3 times, only then it will recreating the container. 
2. ReadinessProbe - 

#### HTTP LIVENESS PROBE ####
root@master:~/app/scheduling# nano healthcheck-LivenessProbe.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
    livenessProbe:
      httpGet: # because nginx is webserver, so use the http probe
        path: /index.html
        port: 80
      initialDelaySeconds: 5 # before performing this probe, how much time do we have to wait. E.g how much time it will take nginx to be first deployed and be ready and then perform any probing. if the application is not ready and you are performing a probe, your container will be stuck in loop of killing and recreating. You have to know the nature of the application and how long it takes to be ready.
      periodSeconds: 5 # every 5 second perform the probeCheck
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/scheduling# kk create -f healthcheck-LivenessProbe.yaml
pod/newpod created
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkgp
NAME     READY   STATUS    RESTARTS   AGE
newpod   2/2     Running   0          3m35s
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkdp newpod

### Additional line (Livness) has been added on the containe1 which is not present on container2

... 
Containers:
  con1:
    Container ID:   cri-o://11de04bd701c6f0ef64584ce374be70e18562f1395d0865bddebd2fe55af8209
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:42e917aaa1b5bb40dd0f6f7f4f857490ac7747d7ef73b391c774a41a8b994f15
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 08:11:34 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     64Mi
    Liveness:     http-get http://:80/index.html delay=5s timeout=1s period=5s #success=1 #failure=3    <----------------- since the port 80 is showing the http file called index.html if there will be any issues, it will fail.
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4zznw (ro)
  con2:
    Container ID:   cri-o://80c03d126365c5a2b3c96c4d72ada2fcf431199e950ef1db2f110ab0156d1927
    Image:          tomcat:latest
    Image ID:       docker.io/library/tomcat@sha256:59117505ccf7033bd67a595cf02dbaac3a032468385385acbb6246a4531a2e80
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 08:11:35 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4zznw (ro)
...

### lets create a service ###
root@master:~/app/scheduling# kkgs
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   39d
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkgp
NAME     READY   STATUS    RESTARTS   AGE
newpod   2/2     Running   0          7m3s
root@master:~/app/scheduling#

### ClusterIp has been added on the newpod. newpod has a virtual ip now which is not accessible from anywhere, it is for k8s usage.
root@master:~/app/scheduling# kk expose pod newpod
service/newpod exposed
root@master:~/app/scheduling# kkgs
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP           39d
newpod       ClusterIP   10.96.246.192   <none>        80/TCP,8080/TCP   3s
root@master:~/app/scheduling#

### pod ip
root@master:~/app/scheduling# kkgp -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
newpod   2/2     Running   0          8m53s   172.17.166.153   node1   <none>           <none>
root@master:~/app/scheduling#

#whenever you are creating a service, your services are mapping your pod to it.
root@master:~/app/scheduling# kk describe svc newpod
Name:              newpod
Namespace:         default
Labels:            app=myapp
Annotations:       <none>
Selector:          app=myapp
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.246.192
IPs:               10.96.246.192
Port:              port-1  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.166.153:80
Port:              port-2  8080/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.166.153:8080  <--------------- check this it is going to randomly assign a port 8080 because we didnt define it. And it is a faulty configuration
Session Affinity:  None
Events:            <none>
root@master:~/app/scheduling#

### exposing the pod to service with the port
root@master:~/app/scheduling# kk delete svc newpod
service "newpod" deleted

root@master:~/app/scheduling# kk expose pod newpod --port 80
service/newpod exposed
root@master:~/app/scheduling#

### if you check now the Endpoint port is mapped correctly. Traffic is going to correct port number now.
root@master:~/app/scheduling# kkds newpod
Name:              newpod
Namespace:         default
Labels:            app=myapp
Annotations:       <none>
Selector:          app=myapp
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.102.85.76
IPs:               10.102.85.76
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.166.153:80
Session Affinity:  None
Events:            <none>
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkgp -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
newpod   2/2     Running   0          14m   172.17.166.153   node1   <none>           <none>
root@master:~/app/scheduling#

###if you will access this service from outside, it will not be available. If you try to access from outside while probe is getting failed or containers recreating. You will be seeing errors
### liveness probe will allow you to send the request traffic towards your container, whether it is running or notice any trouble. This is one issue with liveness probe but good thing is that it will recreate the container. 
### if the your check is failed in pod describe, then it will show the event log

### CREATING A ERROR WITHIN FILE TO SHOW THE PROBE IN ACTION ###
apiVersion: v1
kind: Pod
metadata:
  name: newpod
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
    livenessProbe:
      httpGet: # because nginx is webserver, so use the http probe
        path: /test.html #ERROR because the /test.html doesnt exist
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5 # every 5 second perform the probeCheck
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/scheduling# kk delete -f healthcheck-LivenessProbe.yaml
pod "newpod" deleted
root@master:~/app/scheduling# nano healthcheck-LivenessProbe.yaml
root@master:~/app/scheduling#
root@master:~/app/scheduling# nano healthcheck-LivenessProbe.yaml
root@master:~/app/scheduling# kk create -f healthcheck-LivenessProbe.yaml

root@master:~/app/scheduling# kkdp newpod

### KUBELET IS DOING THE HEALTHCHECK 
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  91s                default-scheduler  Successfully assigned default/newpod to node1
  Normal   Created    90s                kubelet            Created container con2
  Normal   Pulled     90s                kubelet            Successfully pulled image "nginx:latest" in 610ms (610ms including waiting)
  Normal   Started    90s                kubelet            Started container con2
  Normal   Pulling    90s                kubelet            Pulling image "tomcat:latest"
  Normal   Pulled     90s                kubelet            Successfully pulled image "tomcat:latest" in 489ms (489ms including waiting)
  Normal   Pulled     71s                kubelet            Successfully pulled image "nginx:latest" in 527ms (527ms including waiting)
  Normal   Created    56s (x3 over 90s)  kubelet            Created container con1
  Normal   Started    56s (x3 over 90s)  kubelet            Started container con1
  Normal   Killing    56s (x2 over 71s)  kubelet            Container con1 failed liveness probe, will be restarted
  Normal   Pulling    56s (x3 over 91s)  kubelet            Pulling image "nginx:latest"
  Normal   Pulled     56s                kubelet            Successfully pulled image "nginx:latest" in 526ms (526ms including waiting)
  Warning  Unhealthy  51s (x7 over 81s)  kubelet            Liveness probe failed: HTTP probe failed with statuscode: 404      <--------------------------

### probe will try to restart/recreate the container 5 
root@master:~/app/scheduling# kkgp
NAME     READY   STATUS             RESTARTS      AGE
newpod   1/2     CrashLoopBackOff   5 (19s ago)   2m25s
root@master:~/app/scheduling#

#### EXEC LIVENESS PROBE ####
root@master:~/app/scheduling# root@master:~/app/scheduling# nano healthcheck-LivenessProbe-exec.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod-exec
  labels:
    app: myapp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
    livenessProbe:
      exec:
        command:
        - cat
        - /usr/share/nginx/html/index.html
      initialDelaySeconds: 5
      periodSeconds: 5 # every 5 second perform the probeCheck
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080


root@master:~/app/scheduling# kk create -f healthcheck-LivenessProbe-exec.yaml
pod/newpod-exec created
root@master:~/app/scheduling# kkgp
NAME          READY   STATUS              RESTARTS       AGE
newpod        2/2     Running             8 (5m8s ago)   11m
newpod-exec   0/2     ContainerCreating   0              7s

Containers:
  con1:
    Container ID:
    Image:          nginx:latest
    Image ID:
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     64Mi
    Liveness:     exec [cat /usr/share/nginx/html/index.html] delay=5s timeout=1s period=5s #success=1 #failure=3    <----------------------- command is executed successfully INTERVIEW QUESTION: WHEN my pod will go in state of CrashLoopBackOff: there could be few reason of healthcheck of your container. If you container is continously failed 5th time, it will be going in the CrashLoopBackOff.
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kwrxx (ro)
  con2:
    Container ID:
    Image:          tomcat:latest
    Image ID:
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:


INTERVIEW QUESTION: 
Q WHEN my pod will go in state of CrashLoopBackOff: there could be few reason of healthcheck of your container. If you container is continously failed 5th time, it will be going in the CrashLoopBackOff.
Q what can be the issue instead of healthcheck my container is not starting? or I didnt define health check but my still container is going in CrashLoopBackOff? that can be because of misconfiguration in image or entrypoint inspection which is not able to make your application continously running. 

#### TCPSOCKET LIVENESS PROBE ####
root@master:~/app/scheduling# nano healthcheck-LivenessProbe-tcpSocket.yaml
apiVersion: v1
kind: Pod
metadata:
  name: newpod-tcp
  labels:
    app: myapp-tcp
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests: #e.g 1GB memory    # minimum request to the resources on a node (if my node doesnt have 1gb, then it will not be deployed on the node)
        memory: "64Mi"
        cpu: "100m"
      limits: #e.g 2GB memory    # maximum limit to the resources pod can have access from a node
        memory: "1024Mi"
        cpu: "500m" #1000 mili cpu is equals to 1 cpu
    livenessProbe:
      tcpSocket:
        port: 80 # Our container port is 80
      initialDelaySeconds: 5
      periodSeconds: 5 # every 5 second perform the probeCheck
  - name: con2
    image: tomcat:latest
    ports:
    - containerPort: 8080

root@master:~/app/scheduling# kk create -f healthcheck-LivenessProbe-tcpSocket.yaml
pod/newpod-tcp created
root@master:~/app/scheduling# kkgp
NAME          READY   STATUS             RESTARTS       AGE
newpod        1/2     CrashLoopBackOff   13 (73s ago)   24m
newpod-exec   2/2     Running            0              13m
newpod-tcp    2/2     Running            0              3s
root@master:~/app/scheduling#

root@master:~/app/scheduling# kkdp newpod-tcp
Containers:
  con1:
    Container ID:   cri-o://7f32920acc30820b42f090594413087160b0ec8cd7111a32c7ff508a1c5751df
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:42e917aaa1b5bb40dd0f6f7f4f857490ac7747d7ef73b391c774a41a8b994f15
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 10:46:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     64Mi
    Liveness:     tcp-socket :80 delay=5s timeout=1s period=5s #success=1 #failure=3    <------------------------ tcpSocket probe if the port number is not available, it will fail.
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-q6ksh (ro)
  con2:
    Container ID:   cri-o://4d5435b34e7099e0b42e0ea7d283b06b15cd68d8ceb51f000c1c6d6ed1b4ed10
    Image:          tomcat:latest
    Image ID:       docker.io/library/tomcat@sha256:59117505ccf7033bd67a595cf02dbaac3a032468385385acbb6246a4531a2e80
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 27 Dec 2024 10:46:20 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:



############## HEALTH CHECK ENDS ##############
