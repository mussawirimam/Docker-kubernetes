check if the 
ufw is allowing the ports
swap is off
make sure if you are using containerd or crio define it

in my case when I parameterized the crio in /etc/default/kubelet, for some reason it wasnt working
so disabled the containerd and deleted the related files for the containerd

https://chatgpt.com/share/67304908-71f8-8001-811f-cd6bce7ac3aa

to fix it I followed these commands to remove containerd, so that there isnt conflict of CRI
root@node1:~# history
    1  poweroff
    2  vi /etc/hostname
    3  reboot
    4  sudo su -
    5  apt remove docker
    6  apt-get remove docker
    7  apt-get remove containerd
    8  system disable containerd
    9  systemctl disable containerd
   10  systemctl stop containerd
   11  systemctl remove containerd
   12  apt-get purge docker-engine
   13  apt-get autoremove --purge docker-engine
   14  rm -rf /var/run/docker.sock
   15  rm -rf /var/run/containerd
   16  systemctl status containerd
   17  cd /usr/lib/systemd/system/
root@master:~# cd /usr/lib/systemd/system/
root@master:/usr/lib/systemd/system# rm -rf containerd.service

root@master:/usr/lib/systemd/system# cd /var/run/
root@master:/var/run# ls
alsa           containers    dbus             gdm3.pid   lxd-installer.socket  network          prlrosettad.sock  snapd-snap.socket  systemd           utmp
apport.lock    credentials   dmeventd-client  initctl    motd.dynamic          NetworkManager   prltoolsd.lock    snapd.socket       tmpfiles.d        utsns
avahi-daemon   crio          dmeventd-server  initramfs  mount                 openvpn          prltoolsd.pid     speech-dispatcher  ubuntu-advantage  uuidd
blkid          crond.pid     docker           ipcns      multipath             openvpn-client   screen            spice-vdagentd     udev              wpa_supplicant
cloud-init     crond.reboot  docker.pid       lock       multipathd.pid        openvpn-server   sendsigs.omit.d   sshd               udisks2
console-setup  cryptsetup    docker.sock      log        needrestart           pidns            setrans           sshd.pid           user
containerd     cups          gdm3             lvm        netns                 prlrosettad.pid  shm               sudo               userns
root@master:/var/run# ls container
ls: cannot access 'container': No such file or directory
root@master:/var/run# cd containerd/
root@master:/var/run/containerd# ls
containerd.sock  containerd.sock.ttrpc  io.containerd.runtime.v1.linux  io.containerd.runtime.v2.task
root@master:/var/run/containerd# cd ..
root@master:/var/run# rm -rf containerd
   18  systemctl status containerd
   19  cd ~
   20  vi installk8s.sh
   21  chmod +x installk8s.sh
   22  head installk8s.sh
   23  ./installk8s.sh
   24  kubeadm join 10.211.55.9:6443 --token xxxxxxxx --discovery-token-ca-cert-hash sha256:xxxxxxxx
   25  history
root@node1:~#








root@node1:~# kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
root@node1:~# kubeadm reset
W1113 16:39:49.227283    5217 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W1113 16:39:53.897273    5217 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
root@node1:~# /etc/kubernetes/kubelet.conf
-bash: /etc/kubernetes/kubelet.conf: No such file or directory
root@node1:~# kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase kubelet-start: a Node with name "node1" and status "Ready" already exists in the cluster. You must delete the existing Node or change the name of this new joining Node
To see the stack trace of this error execute with --v=5 or higher
root@node1:~# kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
root@node1:~# rm -rf /etc/kubernetes/pki/ca.crt
root@node1:~# kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@node1:~# 







-----------
root@node1:~# history
    1  sudo poweroff
    2  echo "node1" > /etc/hostname 
    3  install.sh
    4  vi install.sh
    5  chmod +x install.sh 
    6  ./install.sh 
    7  ifconfig
    8  swapoff -a
    9  vi /etc/fstab 
   10  cat /etc/fstab 
   11  ifconfig
   12  systemctl status crio
   13  clear
   14  reboot
   15  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   16  systemctl start kubelet.service
   17  swapoff -a
   18  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   19  rm -rf /etc/kubernetes/pki/ca.crt
   20  rm -rf etc/kubernetes/kubelet.conf
   21  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   22  rm -rf
   23  rm -rf /etc/kubernetes/bootstrap-kubelet.conf 
   24  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   25  rm -rf /etc/kubernetes/kubelet.conf
   26  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   27  clear
   28  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   29  kubeadm reset
   30  /etc/kubernetes/kubelet.conf
   31  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   32  rm -rf /etc/kubernetes/pki/ca.crt
   33  kubeadm join 192.168.1.236:6443 --token o76imh.73jyoyo3ph3sxyj0 --discovery-token-ca-cert-hash sha256:536a69f7241fcb99587d2d2114568f19fa7dc55d3649c014b88b8fe04df84fba 
   34  history

